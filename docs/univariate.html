<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>riskmodels.univariate API documentation</title>
<meta name="description" content="This module contains univariate models for risk analysis, namely, empirical (discrete) distributions and semiparametric distributions with Generalised â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>riskmodels.univariate</code></h1>
</header>
<section id="section-intro">
<p>This module contains univariate models for risk analysis, namely, empirical (discrete) distributions and semiparametric distributions with Generalised Pareto tail models. Both MLE-based and Bayesian estimation are supported for the GP models; useful diagnostic plots are available for fitted models, and exceedance conditional distributions of the type X | X &gt; u for a fitted model X are implemented through the &gt;= and &gt; operators for all model classes, as well as scalar addition and (positive) rescaling, Finally, Binned distributions with integer support are available and can be convolved to get the distribution of a sum of independent integer random variables. This is useful for risk models in energy procurement.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
This module contains univariate models for risk analysis, namely, empirical (discrete) distributions and semiparametric distributions with Generalised Pareto tail models. Both MLE-based and Bayesian estimation are supported for the GP models; useful diagnostic plots are available for fitted models, and exceedance conditional distributions of the type X | X &gt; u for a fitted model X are implemented through the &gt;= and &gt; operators for all model classes, as well as scalar addition and (positive) rescaling, Finally, Binned distributions with integer support are available and can be convolved to get the distribution of a sum of independent integer random variables. This is useful for risk models in energy procurement.
&#34;&#34;&#34;
from __future__ import annotations

import logging
import time
import typing as t
import traceback
import warnings
from argparse import Namespace
from abc import ABC, abstractmethod
from multiprocessing import Pool
import copy

import pandas as pd
import scipy as sp

import matplotlib.pyplot as plt
from matplotlib.colors import Normalize
from matplotlib import cm
import matplotlib

import numpy as np
import emcee

from scipy.stats import genpareto as gpdist
from scipy.optimize import LinearConstraint, minimize, root_scalar
from scipy.stats import gaussian_kde as kde
from scipy.signal import fftconvolve


from pydantic import BaseModel, ValidationError, validator, PositiveFloat
from functools import reduce

import statsmodels.distributions.empirical_distribution as ed
# class BaseWrapper(BaseModel):

#   class Config:
#     arbitrary_types_allowed = True


class BaseDistribution(BaseModel):

  &#34;&#34;&#34;Base interface for available data model types
  &#34;&#34;&#34;
  _allowed_scalar_types = (int, float, np.int64, np.int32, np.float32, np.float64)
  _figure_color_palette = [&#34;tab:cyan&#34;, &#34;deeppink&#34;]
  _error_tol = 1e-6

  def __repr__(self):
    return &#34;Base distribution object&#34;

  def __str__(self):
    return self.__repr__()

  class Config:
    arbitrary_types_allowed = True

  @abstractmethod
  def simulate(self, size: int) -&gt; np.ndarray:
    &#34;&#34;&#34;Produces simulated values from model
    
    Args:
        n (int): Number of samples
    &#34;&#34;&#34;
    pass

  @abstractmethod
  def moment(self, n: int) -&gt; float:
    &#34;&#34;&#34;Calculates non-centered moments
    
    Args:
        n (int): moment order
    &#34;&#34;&#34;
    pass

  @abstractmethod
  def ppf(self, q: float) -&gt; float:
    &#34;&#34;&#34;Calculates the corresponding quantile for a given probability value
    
    Args:
        q (float): probability level
    &#34;&#34;&#34;
    pass

  def mean(self, **kwargs) -&gt; float:
    &#34;&#34;&#34;Calculates the expected value
    
    Args:
        **kwargs: Additional arguments passed to `moments`. This is needed for Bayesian model instances in which a `return_all` parameter can be passed.
    
    Returns:
        float: mean value
    
    &#34;&#34;&#34;
    return self.moment(1, **kwargs)

  def std(self, **kwargs) -&gt; float:
    &#34;&#34;&#34;Calculates the standard deviation
    
    Args:
        **kwargs: Additional arguments passed to `moments`. This is needed for Bayesian model instances in which a `return_all` parameter can be passed.
    
    Returns:
        float: standard deviation value
    
    &#34;&#34;&#34;
    return np.sqrt(self.moment(2, **kwargs) - self.mean(**kwargs)**2)

  @abstractmethod
  def cdf(self, x:float) -&gt; float:
    &#34;&#34;&#34;Evaluates the cumulative probability function
    
    Args:
        x (float): a point in the support
    &#34;&#34;&#34;
    pass

  @abstractmethod
  def pdf(self, x:float) -&gt; float:
    &#34;&#34;&#34;Calculates the probability mass or probability density function
    
    Args:
        x (float): a point in the support
    &#34;&#34;&#34;
    pass

  def histogram(self, size: int = 1000) -&gt; None:
    &#34;&#34;&#34;Plots a histogram of a simulated sample
    
    Args:
        size (int, optional): sample size
    
    &#34;&#34;&#34;

    #show histogram from 1k samples
    samples = self.simulate(size=size)
    plt.hist(samples, bins=25, edgecolor=&#34;white&#34;, color=self._figure_color_palette[0])
    plt.title(f&#34;Histogram from {np.round(size/1000,1)}K simulated samples&#34;)
    plt.show()

  def plot(self, size: int = 1000) -&gt; None:
    &#34;&#34;&#34;Plots a histogram of a simulated sample
    
    Args:
        size (int, optional): sample size
    
    &#34;&#34;&#34;
    self.histogram(size)

  def cvar(self, p: float, **kwargs) -&gt; float:
    &#34;&#34;&#34;Calculates conditional value at risk for a probability level p, defined as the mean conditioned to an exceedance above the p-quantile.
    
    Args:
        p (float): Description
        **kwargs: Additional arguments passed to `moments`. This is needed for Bayesian model instances in which a `return_all` parameter can be passed.
    
    Returns:
        float: conditional value at risk
    
    Raises:
        ValueError: Description
    
    &#34;&#34;&#34;
    if not isinstance(p, float) or p &lt; 0 or p &gt;= 1:
      raise ValueError(&#34;p must be in the open interval (0,1)&#34;)

    return (self &gt;= self.ppf(p)).mean(**kwargs)

  @abstractmethod
  def __gt__(self, other: float) -&gt; BaseDistribution:
    pass

  @abstractmethod
  def __ge__(self, other: float) -&gt; BaseDistribution:
    pass

  @abstractmethod
  def __add__(self, other: self._allowed_scalar_types): 
    pass

  @abstractmethod
  def __mul__(self, other: self._allowed_scalar_types):
    pass

  def __sub__(self, other: float):

    if not isinstance(other, self._allowed_scalar_types):
      raise TypeError(f&#34;- is implemented for instances of types: {self._allowed_scalar_types}&#34;)

    return self.__add__(-other)

  def __rmul__(self, other):
    return self.__mul__(other)

  def __radd__(self, other):
    return self.__add__(other)

  def __rsub__(self, other):
    return self.__sub__(other)

  #__radd__ = __add__

  #__rmul__ = __mul__

  #__rsub__ = __sub__


class Mixture(BaseDistribution):

  &#34;&#34;&#34;This class represents a probability distribution given by a mixture of weighted continuous and empirical densities; as the base continuous densities can only be of class GPTail, this class is intended to represent either a semiparametric model with a Generalised Pareto tail, or the convolution of such a model with an integer distribution, as is the case for the power surplus distribution in power system reliability modeling.

  Args:
      distributions (t.List[BaseDistribution]): list of distributions that make up the mixture
      weights (np.ndarray): weights for each of the distribution. The weights must be a distribution themselves
  &#34;&#34;&#34;
  
  distributions: t.List[BaseDistribution]
  weights: np.ndarray

  def __repr__(self):
    return f&#34;Mixture with {len(self.weights)} components&#34;

  @validator(&#34;weights&#34;, allow_reuse=True)
  def check_weigths(cls, weights):
    if not np.isclose(np.sum(weights),1, atol=cls._error_tol):
      raise ValidationError(f&#34;Weights don&#39;t sum 1 (sum = {np.sum(weights)})&#34;)
    elif np.any(weights &lt;= 0):
      raise ValidationError(&#34;Negative or null weights are present&#34;)
    else:
      return weights


  def __mul__(self, factor: float):

    return Mixture(
      weights=self.weights, 
      distributions = [factor*dist for dist in self.distributions])

  def __add__(self, factor: float):

    return Mixture(
      weights=self.weights, 
      distributions = [factor + dist for dist in self.distributions])

  def __ge__(self, other:float):

    if not isinstance(other, self._allowed_scalar_types):
      raise TypeError(f&#34;&gt;= is implemented for instances of types : {self._allowed_scalar_types}&#34;)

    if self.cdf(other) == 1:
      raise ValueError(&#34;There is no probability mass above provided threshold&#34;)

    cond_weights = np.array([1 - dist.cdf(other) + (isinstance(dist,Empirical))*dist.pdf(other) for dist in self.distributions])
    new_weights = cond_weights*self.weights

    indices = (new_weights &gt; 0).nonzero()[0]

    nz_weights = new_weights[indices]
    nz_dists = [self.distributions[i] for i in indices]

    return Mixture(
      weights = nz_weights/np.sum(nz_weights), 
      distributions = [dist &gt;= other for dist in nz_dists])

  def __gt__(self, other:float):

    if not isinstance(other, self._allowed_scalar_types):
      raise TypeError(f&#34;&gt; is implemented for instances of types : {self._allowed_scalar_types}&#34;)

    if self.cdf(other) == 1:
      raise ValueError(&#34;There is no probability mass above provided threshold&#34;)

    cond_weights = np.array([1 - dist.cdf(other) + (isinstance(dist,Empirical))*dist.pdf(other) for dist in self.distributions])

    new_weights = cond_weights*self.weights

    indices = (new_weights &gt; 0).nonzero()[0]

    nz_weights = new_weights[indices]
    nz_dists = [self.distributions[i] for i in indices]

    return Mixture(
      weights = nz_weights/np.sum(nz_weights), 
      distributions = [dist &gt; other for dist in nz_dists])

    # index = self.support &gt; other

    # return type(self)(
    #   self.support[index],
    #   self.pdf_values[index]/np.sum(self.pdf_values[index]), 
    #   self.data[self.data &gt; other])


  def simulate(self, size: int) -&gt; np.ndarray:
    &#34;&#34;&#34;Simulate values from mixture distribution
    
    Args:
        size (int): Sample size
    
    Returns:
        np.ndarray: simulated sample
    &#34;&#34;&#34;
    n_samples = np.random.multinomial(n=size, pvals = self.weights, size=1)[0]
    indices = (n_samples &gt; 0).nonzero()[0]
    samples = [dist.simulate(size=k) for dist, k in zip([self.distributions[k] for k in indices], n_samples[indices])]
    return np.concatenate(samples, axis=0)

  def cdf(self, x: t.Union[float, np.ndarray], **kwargs) -&gt; t.Union[float, np.ndarray]:
    &#34;&#34;&#34;Evaluate Mixture&#39;s CDF function
    
    Args:
        x (t.Union[float, np.ndarray]): point at which to evaluate CDF
        **kwargs: Additional arguments passed to individual mixture component&#39;s CDF function
    
    Returns:
        t.Union[float, np.ndarray]: CDF value
    &#34;&#34;&#34;

    # the use of a list and a reduce is needed because mixture components might return scalars or vectors depending on their class and on the passed kwargs.
    vals = [w*dist.cdf(x,**kwargs) for w, dist in zip(self.weights, self.distributions)]
    return reduce(lambda x,y: x + y, vals)

  def pdf(self, x: t.Union[float, np.ndarray], **kwargs) -&gt; t.Union[float, np.ndarray]:
    &#34;&#34;&#34;Evaluate Mixture&#39;s pdf function
    
    Args:
        x (t.Union[float, np.ndarray]): point at which to evaluate CDF
        **kwargs: Additional arguments passed to individual mixture component&#39;s pdf function
    
    Returns:
        t.Union[float, np.ndarray]: pdf value
    &#34;&#34;&#34;

    # the use of a list and a reduce is needed because mixture components might return scalars or vectors depending on their class and on the passed kwargs.
    vals = [w*dist.pdf(x,**kwargs) for w, dist in zip(self.weights, self.distributions)]
    return reduce(lambda x,y: x + y, vals)

  def ppf(self, q: t.Union[float, np.ndarray], **kwargs) -&gt; t.Union[float, np.ndarray]:
    &#34;&#34;&#34;Evaluate Mixture&#39;s quantile function function
    
    Args:
        x (t.Union[float, np.ndarray]): point at which to evaluate quantile function
        **kwargs: Additional arguments passed to individual mixture component&#39;s quantile function
    
    Returns:
        t.Union[float, np.ndarray]: quantile value
    &#34;&#34;&#34;

    if isinstance(q, np.ndarray):
      return np.array([self.ppf(elem, **kwargs) for elem in q])

    def target_function(x):
      return self.cdf(x) - q

    vals =[w*dist.ppf(q, **kwargs) for w, dist in zip(self.weights, self.distributions)]
    x0 = reduce(lambda x,y: x + y, vals)

    # the use of a list and a reduce is needed because mixture components might return scalars or vectors depending on their class and on the passed kwargs.
    vals =[w*dist.ppf(0.5 + q/2, **kwargs) for w, dist in zip(self.weights, self.distributions)] 
    x1 = reduce(lambda x,y: x + y, vals)

    return root_scalar(target_function, x0 = x0, x1 = x1, method=&#34;secant&#34;).root

  def moment(self, n: int, **kwargs) -&gt; float:
    &#34;&#34;&#34;Evaluate Mixture&#39;s n-th moment
    
    Args:
        x (t.Union[float, np.ndarray]): Moment order
        **kwargs: Additional arguments passed to individual mixture components&#39; moment function
    
    Returns:
        t.Union[float, np.ndarray]: moment value
    &#34;&#34;&#34;
    
    # the use of a list and a reduce is needed because mixture components might return scalars or vectors depending on their class and on the passed kwargs.
    vals = [w*dist.moment(n, **kwargs) for w, dist in zip(self.weights, self.distributions)]
    return reduce(lambda x,y: x + y, vals)

  def mean(self, **kwargs) -&gt; float:
    &#34;&#34;&#34;Evaluate Mixture&#39;s mean
    
    Args:
        **kwargs: Additional arguments passed to individual mixture components&#39; mean function
    
    Returns:
        float: mean value
    &#34;&#34;&#34;

    # the use of a list and a reduce is needed because mixture components might return scalars or vectors depending on their class and on the passed kwargs.
    vals = [w*dist.mean(**kwargs) for w, dist in zip(self.weights, self.distributions)]
    return reduce(lambda x,y: x + y, vals)

  def std(self, **kwargs) -&gt; float:
    &#34;&#34;&#34;Evaluate Mixture&#39;s standard deviation
    
    Args:
        **kwargs: Additional arguments passed to individual mixture components&#39; standard deviation function
    
    Returns:
        float: standard deviation value
    &#34;&#34;&#34;

    # the use of a list and a reduce is needed because mixture components might return scalars or vectors depending on their class and on the passed kwargs.
    vals = [w*(dist.std(**kwargs)**2 + dist.mean(**kwargs)**2) for w, dist in zip(self.weights, self.distributions)]
    return np.sqrt(reduce(lambda x,y: x + y, vals) - self.mean()**2)




class GPTail(BaseDistribution):
  &#34;&#34;&#34;Representation of a fitted Generalized Pareto distribution as an exceedance model. It&#39;s density is given by

  $$ f(x) = \\frac{1}{\\sigma} \\left( 1 + \\xi \\left( \\frac{x - \\mu}{\\sigma} \\right) \\right)_{+}^{-(1 + 1/\\xi)} $$
  
  where \\( \\mu, \\sigma, \\xi \\) are the location, scale and shape parameters, and \\( (\\cdot)_{+} = \\max(\\cdot, 0)\\). The location parameter is also the lower endpoint (or threshold) of the distribution.

  Args:
      threshold (float): modeling threshold
      shape (float): fitted shape parameter
      scale (float): fitted scale parameter
      data (np.array, optional): exceedance data
  &#34;&#34;&#34;

  threshold: float
  shape: float
  scale: PositiveFloat
  data: t.Optional[np.ndarray]

  def __repr__(self):
    return f&#34;Generalised Pareto tail model with (mu, scale, shape) = ({self.threshold},{self.scale},{self.shape}) components&#34;

  @property
  def endpoint(self):
    return self.threshold - self.scale/self.shape if self.shape &lt; 0 else np.Inf
  

  @property
  def model(self):
    return gpdist(loc=self.threshold, c=self.shape, scale=self.scale)

  # @classmethod
  # def fit(cls, data: np.array, threshold: float) -&gt; GPTail:
  #   exceedances = data[data &gt; threshold]
  #   shape, _, scale = gpdist.fit(exceedances, floc=threshold)
  #   return cls(
  #     threshold = threshold,
  #     shape = shape,
  #     scale = scale,
  #     data = exceedances)

  def __add__(self, other: float):

    if not isinstance(other, self._allowed_scalar_types):
      raise TypeError(f&#34;+ is implemented for instances of types: {self._allowed_scalar_types}&#34;)

    return GPTail(
      threshold = self.threshold + other,
      shape = self.shape,
      scale = self.scale,
      data = self.data + other if self.data is not None else None)

  def __ge__(self, other: float):

    if not isinstance(other, self._allowed_scalar_types):
      raise TypeError(f&#34;&gt;= and &gt; are implemented for instances of types: {self._allowed_scalar_types}&#34;)

    if other &gt;= self.endpoint:
      raise ValueError(f&#34;No probability mass above endpoint ({self.endpoint}); conditional distribution X | X &gt;= {other} does not exist&#34;)

    if other &lt; self.threshold:
      return self &gt;= self.threshold
    else:
      # condition on empirical data if applicable
      if self.data is None or max(self.data) &lt; other:
        new_data = None
        warnings.warn(f&#34;No observed data above {other}; setting data to None in conditional model.&#34;, stacklevel=2)
      else:
        new_data = self.data[self.data &gt;= other]
      
      return GPTail(
        threshold=other,
        shape = self.shape,
        scale = self.scale + self.shape*(other - self.threshold),
        data = new_data)

  def __gt__(self, other: float) -&gt; GPTail:

    return self.__ge__(other)

  def __mul__(self, other: float) -&gt; GPTail:

    if not isinstance(other, self._allowed_scalar_types) or other &lt;= 0:
      raise TypeError(f&#34;* is implemented for positive instances of: {self._allowed_scalar_types}&#34;)

    new_data = other*self.data if self.data is not None else None

    return GPTail(
      threshold= other*self.threshold,
      shape = self.shape,
      scale = other*self.scale,
      data = new_data)

  def simulate(self, size: int) -&gt; np.ndarray:
    return self.model.rvs(size=size)

  def moment(self, n: int, **kwargs) -&gt; float:
    return self.model.moment(n)

  def ppf(self, q: t.Union[float, np.ndarray], **kwargs) -&gt; t.Union[float, np.ndarray]:
    return self.model.ppf(q)

  def cdf(self, x:t.Union[float, np.ndarray], **kwargs) -&gt; t.Union[float, np.ndarray]:
    return self.model.cdf(x)

  def pdf(self, x:t.Union[float, np.ndarray], **kwargs) -&gt; t.Union[float, np.ndarray]:
    return self.model.pdf(x)

  def std(self, **kwargs):
    return self.model.std()

  def mle_cov(self) -&gt; np.ndarray:
    &#34;&#34;&#34;Returns the estimated parameter covariance matrix evaluated at the fitted parameters
    
    Returns:
        np.ndarray: Covariance matrix
    &#34;&#34;&#34;

    if self.data is None:
      raise ValueError(&#34;exceedance data not provided for this instance of GPTail; covariance matrix can&#39;t be estimated&#34;)
    else:
      hess = self.loglik_hessian([self.scale,self.shape], threshold=self.threshold, data=self.data)
      return np.linalg.inv(-hess)

  @classmethod
  def loglik(cls, params: t.List[float], threshold: float, data: np.ndarray) -&gt; float:
    &#34;&#34;&#34;Returns the negative log-likelihood for a Generalised Pareto model
    
    Args:
        *params (t.List[float]): Vector parameter with scale and shape values, in that order
        threshold (float): model threshold
        data (np.ndarray): exceedance data
    
    Returns:
        float
    &#34;&#34;&#34;
    scale, shape = params
    return np.sum(gpdist.logpdf(data, loc=threshold, c=shape, scale=scale))

  @classmethod
  def loglik_grad(cls, params: t.List[float], threshold: float, data: np.ndarray) -&gt; np.ndarray:
    &#34;&#34;&#34;Returns the gradient of the negative log-likelihood for a Generalised Pareto model
    
    Args:
        *params (t.List[float]): Vector parameter with scale and shape values, in that order
        threshold (float): model threshold
        data (np.ndarray): exceedance data
    
    Returns:
        np.ndarray: gradient array
    &#34;&#34;&#34;
    scale, shape = params
    y = data - threshold

    if not np.isclose(shape, 0, atol=cls._error_tol):
      grad_scale = np.sum((y - scale)/(scale*(y*shape + scale)))
      grad_shape = np.sum(-((y*(1 + shape))/(shape*(y*shape + scale))) + np.log(1 + (y*shape)/scale)/shape**2)
    else:
      grad_scale = np.sum((y-scale)/scale**2)
      grad_shape = np.sum(y*(y-2*scale)/(2*scale**2))

    return np.array([grad_scale,grad_shape])


  @classmethod
  def loglik_hessian(cls, params: t.List[float], threshold: float, data: np.ndarray) -&gt; np.ndarray:
    &#34;&#34;&#34;Returns the Hessian matrix of the negative log-likelihood for a Generalised Pareto model
    
    Args:
        *params (t.List[float]): Vector parameter with scale and shape values, in that order
        threshold (float): model threshold
        data (np.ndarray): exceedance data above the threshold
    
    Returns:
        np.ndarray: hessian matrix array
    &#34;&#34;&#34;
    scale, shape = params
    y = data - threshold

    if not np.isclose(shape, 0, atol=cls._error_tol):
      d2scale = np.sum(-(1/(shape*scale**2)) + (1 + shape)/(shape*(y*shape + scale)**2))

      #d2shape = (y (3 y Î¾ + y Î¾^2 + 2 Ïƒ))/(Î¾^2 (y Î¾ + Ïƒ)^2) - (2 Log[1 + (y Î¾)/Ïƒ])/Î¾^3
      d2shape = np.sum((y*(3*y*shape + y*shape**2 + 2*scale))/(shape**2*(y*shape + scale)**2) - (2*np.log(1 + (y*shape)/scale))/shape**3)

      #dscale_dshape = (y (-y + Ïƒ))/(Ïƒ (y Î¾ + Ïƒ)^2)
      dscale_dshape = np.sum((y*(-y + scale))/(scale*(y*shape + scale)**2))
    else:
      d2scale = np.sum((scale-2*y)/scale**3)
      dscale_dshape = np.sum(-y*(y-scale)/scale**3)
      d2shape = np.sum(y**2*(3*scale-2*y)/(3*scale**3))

    hessian = np.array([[d2scale,dscale_dshape],[dscale_dshape,d2shape]])

    return hessian

  @classmethod
  def logreg(cls, params: t.List[float]) -&gt; float:
    &#34;&#34;&#34;The regularisation terms are inspired in uninformative and zero-mean Gaussian priors for the scale and shape respectively, thus it is given by

    $$r(\\sigma, \\xi) = 0.5 \\cdot \\log(\\sigma) - 0.5 \\xi^2$$
    
    Args:
        params (t.List[float]): scale and shape parameters in that order
    
    Returns:
        float: Description
    &#34;&#34;&#34;
    scale, shape = params

    return 0.5*(np.log(scale) + shape**2)

  @classmethod
  def logreg_grad(cls, params: t.List[float]) -&gt; float:
    &#34;&#34;&#34;Returns the gradient of the regularisation term
    
    Args:
        params (t.List[float]): scale and shape parameters in that order

    &#34;&#34;&#34;
    scale, shape = params
    return 0.5*np.array([1.0/scale, 2*shape])

  @classmethod
  def logreg_hessian(cls, params: t.List[float]) -&gt; float:
    &#34;&#34;&#34;Returns the Hessian of the regularisation term
    
    Args:
        params (t.List[float]): scale and shape parameters in that order

    &#34;&#34;&#34;
    scale, shape = params
    return 0.5*np.array([-1.0/scale**2,0,0,2]).reshape((2,2))

  @classmethod
  def loss(cls, params: t.List[float], threshold: float, data: np.ndarray) -&gt; np.ndarray:
    &#34;&#34;&#34;Calculate the loss function on the provided parameters; this is the sum of the (negative) data log-likelihood and (negative) regularisation terms for the scale and shape. Everything is divided by the number of data points.
    
    Args:
        params (t.List[float]): Description
        threshold (float): Model threshold; eequivalently, location parameter
        data (np.ndarray): exceedance data above threshold
    
    &#34;&#34;&#34;
    scale, shape = params
    n = len(data)
    unnorm = cls.loglik(params, threshold, data) + cls.logreg(params)
    return -unnorm/n

  @classmethod
  def loss_grad(cls, params: t.List[float], threshold: float, data: np.ndarray) -&gt; np.ndarray:
    &#34;&#34;&#34;Calculate the loss function&#39;s gradient on the provided parameters
    
    Args:
        params (t.List[float]): Description
        threshold (float): Model threshold; eequivalently, location parameter
        data (np.ndarray): exceedance data above threshold
    
    &#34;&#34;&#34;
    n = len(data)
    return -(cls.loglik_grad(params, threshold, data) + cls.logreg_grad(params))/n

  @classmethod
  def loss_hessian(cls, params: t.List[float], threshold: float, data: np.ndarray) -&gt; np.ndarray:
    &#34;&#34;&#34;Calculate the loss function&#39;s Hessian on the provided parameters
    
    Args:
        params (t.List[float]): Description
        threshold (float): Model threshold; eequivalently, location parameter
        data (np.ndarray): exceedance data above threshold
    
    &#34;&#34;&#34;
    n = len(data)
    return -(cls.loglik_hessian(params, threshold, data) + cls.logreg_hessian(params))/n

  @classmethod
  def fit(
    cls, 
    data: np.ndarray, 
    threshold: float, 
    x0: np.ndarray = None,
    return_opt_results=False) -&gt; t.Union[GPTail, sp.optimize.OptimizeResult]:
    &#34;&#34;&#34;Fits a eneralised Pareto tail model using a constrained trust region method
    
    Args:
        data (np.ndarray): exceedance data above threshold
        threshold (float): Model threshold
        x0 (np.ndarray, optional): Initial guess for optimization; if None, the result of scipy.stats.genpareto.fit is used as a starting point.
        return_opt_results (bool, optional): If True, return the OptimizeResult object; otherwise return fitted instance of GPTail
    
    Returns:
        t.Union[GPTail, sp.optimize.OptimizeResult]: Description
    &#34;&#34;&#34;
    exceedances = data[data &gt; threshold]

    #rescale exceedances and threshold so that both parameters are in roughly the same scale, improving numerical conditioning
    sdev = np.std(exceedances)
    
    # rescaling the data rescales the location and scale parameter, and leaves the shape parameter unchanged
    norm_exceedances = exceedances/sdev
    norm_threshold = threshold/sdev

    norm_max = max(norm_exceedances)

    constraints = LinearConstraint(
      A = np.array([[1/(norm_max-norm_threshold),1], [1, 0]]), 
      lb=np.zeros((2,)), 
      ub=np.Inf)

    if x0 is None:
      # use default scipy fitter to get initial estimate
      # this is almost always good enough
      shape, _, scale = gpdist.fit(norm_exceedances, floc=norm_threshold)
      x0 = np.array([scale, shape])

    loss_func = lambda params: GPTail.loss(params, norm_threshold, norm_exceedances)
    loss_grad = lambda params: GPTail. loss_grad(params, norm_threshold, norm_exceedances)
    loss_hessian = lambda params: GPTail.loss_hessian(params, norm_threshold, norm_exceedances)

    res = minimize(
      fun=loss_func, 
      x0 = x0,
      method = &#34;trust-constr&#34;,
      jac = loss_grad,
      hess = loss_hessian,
      constraints = [constraints])

    #print(res)
    if return_opt_results:
      warnings.warn(&#34;Returning raw results for rescaled exceedance data (sdev ~ 1).&#34;)
      return res
    else:
      scale, shape = list(res.x)

      return cls(
        threshold = sdev*norm_threshold,
        scale = sdev*scale,
        shape = shape,
        data = sdev*norm_exceedances)

  def plot_diagnostics(self) -&gt; None:
    &#34;&#34;&#34;Returns a figure with fit diagnostic for the GP model
    
    &#34;&#34;&#34;
    if self.data is None:
      raise ValueError(&#34;Exceedance data was not provided for this model.&#34;)

    fig, axs = plt.subplots(3, 2)


    #################### Profile log-likelihood

    # set profile intervals based on MLE variance
    mle_cov = self.mle_cov()
    scale_bounds, shape_bounds = (self.scale - np.sqrt(mle_cov[0,0]), self.scale + np.sqrt(mle_cov[0,0])), (self.shape - np.sqrt(mle_cov[1,1]), self.shape + np.sqrt(mle_cov[1,1]))

    #set profile grids
    scale_grid = np.linspace(scale_bounds[0], scale_bounds[1], 50)
    shape_grid = np.linspace(shape_bounds[0], shape_bounds[1], 50)


    #declare profile functions
    scale_profile_func = lambda x: self.loglik([x, self.shape], self.threshold, self.data)
    shape_profile_func = lambda x: self.loglik([self.scale, x], self.threshold, self.data)

    loss_value = scale_profile_func(self.scale)

    scale_profile = np.array([scale_profile_func(x) for x in scale_grid])
    shape_profile = np.array([shape_profile_func(x) for x in shape_grid])

    alpha = 2 if loss_value &gt; 0 else 0.5

    # filter to almost-optimal values

    def filter_grid(grid, optimum):
      radius = 2*np.abs(optimum)
      return np.logical_and(np.logical_not(np.isnan(grid)), np.isfinite(grid), np.abs(grid - optimum) &lt; radius)

    scale_filter = filter_grid(scale_profile, loss_value)
    shape_filter = filter_grid(shape_profile, loss_value)


    valid_scales = scale_grid[scale_filter]
    valid_scale_profile = scale_profile[scale_filter]

    axs[0,0].plot(valid_scales, valid_scale_profile, color=self._figure_color_palette[0])
    axs[0,0].vlines(x=self.scale, ymin=min(valid_scale_profile), ymax = max(valid_scale_profile), linestyle=&#34;dashed&#34;, colors = self._figure_color_palette[1])
    axs[0,0].title.set_text(&#39;Profile scale log-likelihood&#39;)
    axs[0,0].set_xlabel(&#39;Scale&#39;)
    axs[0,0].set_ylabel(&#39;log-likelihood&#39;)

    valid_shapes = shape_grid[shape_filter]
    valid_shape_profile = shape_profile[shape_filter]

    axs[0,1].plot(valid_shapes, valid_shape_profile, color=self._figure_color_palette[0])
    axs[0,1].vlines(x=self.shape, ymin=min(valid_shape_profile), ymax = max(valid_shape_profile), linestyle=&#34;dashed&#34;, colors = self._figure_color_palette[1])
    axs[0,1].title.set_text(&#39;Profile shape log-likelihood&#39;)
    axs[0,1].set_xlabel(&#39;Shape&#39;)
    axs[0,1].set_ylabel(&#39;log-likelihood&#39;)

    ######################## Log-likelihood surface ###############
    scale_grid, shape_grid = np.mgrid[
    min(valid_scales):max(valid_scales):2*np.sqrt(mle_cov[0,0])/50,
    shape_bounds[0]:shape_bounds[1]:2*np.sqrt(mle_cov[0,0])/50]

    scale_mesh, shape_mesh = np.meshgrid(
      np.linspace(min(valid_scales), max(valid_scales), 50), 
      np.linspace(min(valid_shapes), max(valid_shapes), 50))

    max_x = max(self.data)

    z = np.empty(scale_mesh.shape)
    for i in range(scale_mesh.shape[0]):
      for j in range(scale_mesh.shape[1]):
        shape = shape_mesh[i,j]
        scale = scale_mesh[i,j]
        if shape &lt; 0 and self.threshold - scale/shape &lt; max_x:
          z[i,j] = np.nan
        else:
          z[i,j] = self.loglik([scale, shape], self.threshold, self.data)

    # negate z to recover true loglikelihood
    axs[1,0].contourf(scale_mesh, shape_mesh, z, levels = 15)
    axs[1,0].scatter([self.scale], [self.shape], color=&#34;darkorange&#34;, s=2)
    axs[1,0].annotate(text=&#34;MLE&#34;, xy = (self.scale, self.shape), color=&#34;darkorange&#34;)
    axs[1,0].title.set_text(&#39;Log-likelihood surface&#39;)
    axs[1,0].set_xlabel(&#39;Scale&#39;)
    axs[1,0].set_ylabel(&#39;Shape&#39;)



    ############## histogram vs density ################
    hist_data = axs[1,1].hist(self.data, bins=25, edgecolor=&#34;white&#34;, color=self._figure_color_palette[0])

    range_min, range_max = min(self.data), max(self.data)
    x_axis = np.linspace(range_min, range_max, 100)
    pdf_vals = self.pdf(x_axis)
    y_axis = hist_data[0][0] / pdf_vals[0] * pdf_vals

    axs[1,1].plot(x_axis, y_axis, color=self._figure_color_palette[1])
    axs[1,1].title.set_text(&#34;Data vs fitted density&#34;)
    axs[1,1].set_xlabel(&#39;Exceedance data&#39;)
    axs[1,1].yaxis.set_visible(False) # Hide only x axis
    #axs[0, 0].set_aspect(&#39;equal&#39;, &#39;box&#39;)


    ############# Q-Q plot ################
    probability_range = np.linspace(0.01,0.99, 99)
    empirical_quantiles = np.quantile(self.data, probability_range)
    tail_quantiles = self.ppf(probability_range)

    axs[2,0].scatter(tail_quantiles, empirical_quantiles, color = self._figure_color_palette[0])
    min_x, max_x = min(tail_quantiles), max(tail_quantiles)
    #axs[0,1].set_aspect(&#39;equal&#39;, &#39;box&#39;)
    axs[2,0].title.set_text(&#39;Q-Q plot&#39;)
    axs[2,0].set_xlabel(&#39;model quantiles&#39;)
    axs[2,0].set_ylabel(&#39;Data quantiles&#39;)
    axs[2,0].grid()
    axs[2,0].plot([min_x,max_x],[min_x,max_x], linestyle=&#34;--&#34;, color=&#34;black&#34;)

    ############ Mean return plot ###############
    # scale, shape = self.scale, self.shape

    # n_obs = len(self.data)
    # exceedance_frequency = 1/np.logspace(1,4,20)
    # return_levels = self.ppf(1 - exceedance_frequency)

    # axs[2,1].plot(1.0/exceedance_frequency,return_levels,color=self._figure_color_palette[0])
    # axs[2,1].set_xscale(&#34;log&#34;)
    # axs[2,1].title.set_text(&#34;Exceedance model&#39;s return levels&#34;)
    # axs[2,1].set_xlabel(&#39;1/frequency&#39;)
    # axs[2,1].set_ylabel(&#39;Return level&#39;)
    # axs[2,1].grid()




    # exs_prob = 1 #carried over from older code

    # m = 10**np.linspace(np.log(1/exs_prob + 1)/np.log(10), 3,20)
    # return_levels = self.ppf(1 - 1/(exs_prob*m))

    # axs[2,1].plot(m,return_levels,color=self._figure_color_palette[0])
    # axs[2,1].set_xscale(&#34;log&#34;)
    # axs[2,1].title.set_text(&#39;Exceedance return levels&#39;)
    # axs[2,1].set_xlabel(&#39;1/frequency&#39;)
    # axs[2,1].set_ylabel(&#39;Return level&#39;)
    # axs[2,1].grid()

    # try:
    #   #for this bit, look at An Introduction to Statistical selfing of Extreme Values, p.82
    #   mle_cov = self.mle_cov()
    #   eigenvals, eigenvecs = np.linalg.eig(mle_cov)
    #   if np.all(eigenvals &gt; 0):
    #     covariance = np.eye(3)
    #     covariance[1::,1::] = mle_cov
    #     covariance[0,0] = exs_prob*(1-exs_prob)/n_obs
    #     #
    #     return_stdevs = []
    #     for m_ in m:
    #       quantile_grad = np.array([
    #         scale*m_**(shape)*exs_prob**(shape-1),
    #         shape**(-1)*((exs_prob*m_)**shape-1),
    #         -scale*shape**(-2)*((exs_prob*m_)**shape-1)+scale*shape**(-1)*(exs_prob*m_)**shape*np.log(exs_prob*m_)
    #         ])
    #       #
    #       sdev = np.sqrt(quantile_grad.T.dot(covariance).dot(quantile_grad))
    #       return_stdevs.append(sdev)
    #     #
    #     axs[2,1].fill_between(m, return_levels - return_stdevs, return_levels + return_stdevs, alpha=0.2, color=self._figure_color_palette[1])
    #   else:
    #     warnings.warn(&#34;Covariance MLE matrix is not positive definite; it might be ill-conditioned&#34;, stacklevel=2)
    # except Exception as e:
    #   warnings.warn(f&#34;Confidence bands for return level could not be calculated; covariance matrix might be ill-conditioned; full trace: {traceback.format_exc()}&#34;, stacklevel=2)

    plt.tight_layout()
    return fig






class GPTailMixture(BaseDistribution):

  &#34;&#34;&#34;Mixture distribution with generalised Pareto components. This is a base class for  Bayesian generalised Pareto tail models, which can be seen as an uniformly weighted mixture of the posterior samples; the convolution of a discrete (or empirical) distribution with a generalised Pareto distribution also results in a mixture of this kind. Most methods inherited from `BaseDistribution` have an extra argument `return_all`. When it is True, the full posterior sample of the method evaluation is returned.

  Args:
      data(np.ndarray, optional): Data that induced the model, if applicable
      weights (np.ndarray): component weights
      thresholds (np.ndarray): vector of threshold parameters, one for each component
      scales (np.ndarray): vector of scale parameters, one for each component
      shapes (np.ndarray): vector of shape parameters, one for each component
  &#34;&#34;&#34;

  data: t.Optional[np.ndarray]
  weights: np.ndarray
  thresholds: np.ndarray
  scales: np.ndarray
  shapes: np.ndarray

  def __repr__(self):
    return f&#34;Mixture of generalised Pareto distributions with {len(self.weights)} components&#34;

  @validator(&#34;weights&#34;, allow_reuse=True)
  def check_weigths(cls, weights):
    if not np.isclose(np.sum(weights),1, atol=cls._error_tol):
      raise ValueError(f&#34;Weights don&#39;t sum 1 (sum = {np.sum(weights)})&#34;)
    elif np.any(weights &lt;= 0):
      raise ValueError(&#34;Negative or null weights are present&#34;)
    else:
      return weights

  def moment(self, n: int, return_all: bool = False) -&gt; float:
    &#34;&#34;&#34;Returns the n-th order moment
    
    Args:
        n (int): Moment&#39;s order
        return_all (bool, optional): If `True`, all posterior samples of the value are returned; otherwise  the moment value of the posterior predictive distribution is returned. The posterior distribution is taken to be the empirical distribution of posterior samples.
    
    Returns:
        float: moment value
    &#34;&#34;&#34;
    vals = np.array([gpdist.moment(
      n,
      loc=threshold, 
      c=shape, 
      scale=scale) for threshold, shape, scale in zip(self.thresholds, self.shapes, self.scales)])

    if return_all:
      return vals
    else:
      return np.dot(self.weights, vals)

  def mean(self, return_all: bool = False) -&gt; float:
    &#34;&#34;&#34;Returns the mean value
    
    Args:
        return_all (bool, optional): If `True`, all posterior samples of the value are returned; the mean of the posterior predictive distribution is evaluated. The posterior distribution is taken to be the empirical distribution of posterior samples.
    
    Returns:
        float: mean value
    &#34;&#34;&#34;
    vals = gpdist.mean(
      loc=self.thresholds, 
      c=self.shapes, 
      scale=self.scales)

    if return_all:
      return vals
    else:
      return np.dot(self.weights, vals)

  def std(self, return_all: bool = False) -&gt; float:
    &#34;&#34;&#34;Standard deviation value
    
    Args:
        return_all (bool, optional): If `True`, all posterior samples of the value are returned; otherwise the standard deviation of the posterior predictive distribution is returned. The posterior distribution is taken to be the empirical distribution of posterior samples.
    
    Returns:
        float: standard deviation value
    &#34;&#34;&#34;
    var = gpdist.var(
      loc=self.thresholds, 
      c=self.shapes, 
      scale=self.scales)

    mean = gpdist.mean(
      loc=self.thresholds, 
      c=self.shapes, 
      scale=self.scales)

    if return_all:
      return np.sqrt(var)
    else:
      return np.sqrt(np.dot(self.weights, var + mean**2) - self.mean()**2)

  def cdf(self, x: t.Union[float, np.ndarray], return_all: bool = False) -&gt; t.Union[float, np.ndarray]:
    &#34;&#34;&#34;Evaluates the CDF function
    
    Args:
        x (t.Union[float, np.ndarray]): Point at which to evaluate it
        return_all (bool, optional): If `True`, all posterior samples of the value are returned; the CDF of the posterior predictive distribution is evaluated. The posterior distribution is taken to be the empirical distribution of posterior samples.
    
    Returns:
        t.Union[float, np.ndarray]: CDF value
    &#34;&#34;&#34;

    if isinstance(x, np.ndarray):
      return np.array([self.cdf(elem, return_all) for elem in x])

    vals = gpdist.cdf(
      x,
      loc=self.thresholds, 
      c=self.shapes, 
      scale=self.scales)

    if return_all:
      return vals
    else:
      return np.dot(self.weights, vals)

  def pdf(self, x: t.Union[float, np.ndarray], return_all: bool = False) -&gt; t.Union[float, np.ndarray]:
    &#34;&#34;&#34;Evaluates the pdf function
    
    Args:
        x (t.Union[float, np.ndarray]): Point at which to evaluate it
        return_all (bool, optional): If `True`, all posterior samples of the value are returned; otherwise the pdf of the posterior predictive distribution is evaluated. The posterior distribution is taken to be the empirical distribution of posterior samples.
    
    Returns:
        t.Union[float, np.ndarray]: pdf value
    &#34;&#34;&#34;
    
    if isinstance(x, np.ndarray):
      return np.array([self.pdf(elem, return_all) for elem in x])

    vals = gpdist.pdf(
      x,
      loc=self.thresholds, 
      c=self.shapes, 
      scale=self.scales)

    if return_all:
      return vals
    else:
      return np.dot(self.weights, vals)

  def ppf(self, q: t.Union[float, np.ndarray], return_all: bool = False) -&gt; t.Union[float, np.ndarray]:
    &#34;&#34;&#34;Evaluates the quantile function
    
    Args:
        x (t.Union[float, np.ndarray]): probability level
        return_all (bool, optional): If `True`, all posterior samples of the value are returned; otherwise  the quantile function of the posterior predictive distribution is evaluated. The posterior distribution is taken to be the empirical distribution of posterior samples.
    
    Returns:
        t.Union[float, np.ndarray]: quantile function value value
    &#34;&#34;&#34;
    if isinstance(q, np.ndarray):
      return np.array([self.ppf(elem, return_all) for elem in q])

    vals = gpdist.ppf(
      q,
      loc=self.thresholds, 
      c=self.shapes, 
      scale=self.scales)


    if return_all:
      return vals
    else:
      def target_function(x):
        return self.cdf(x) - q
      x0 = np.dot(self.weights, vals)
      return root_scalar(target_function, x0 = x0, x1 = x0 + 1, method=&#34;secant&#34;).root

  def cvar(self, p:float, return_all: bool = False) -&gt; float:
    &#34;&#34;&#34;Returns the conditional value at risk for a given probability level
    
    Args:
        x (t.Union[float, np.ndarray]): probability level
        return_all (bool, optional): If `True`, all posterior samples of the value are returned; otherwise  the cvar of the posterior predictive distribution is evaluated. The posterior distribution is taken to be the empirical distribution of posterior samples.
    
    Returns:
        t.Union[float, np.ndarray]: conditional value at risk
    &#34;&#34;&#34;
    if p &lt; 0 or p &gt;= 1:
      raise ValueError(&#34;p must be in the open interval (0,1)&#34;)

    return (self &gt;= self.ppf(p)).mean(return_all)

  def simulate(self, size: int) -&gt; np.ndarray:
    n_samples = np.random.multinomial(n=size, pvals = self.weights, size=1)[0]
    indices = (n_samples &gt; 0).nonzero()[0]
    samples = [gpdist.rvs(size=n_samples[i], c=self.shapes[i], scale=self.scales[i], loc=self.thresholds[i]) for i in indices]
    return np.concatenate(samples, axis=0)


  def __gt__(self, other: float) -&gt; GPTailMixture:
    exceedance_prob = 1 - self.cdf(other)
    #prob_exceedance = np.dot(self.weights, prob_cond_exceedance)
    if exceedance_prob == 0:
      raise ValueError(f&#34;There is no probability mass above {other}; conditional distribution does not exist.&#34;)

    conditional_weights = self.weights*gpdist.sf(other, c=self.shapes, scale=self.scales, loc=self.thresholds)/exceedance_prob

    indices = (conditional_weights &gt; 0).nonzero()[0] #indices of mixture components with nonzero exceedance probability

    new_weights = conditional_weights[indices]

    # disable warnings temporarily
    # with warnings.catch_warnings():
    #   warnings.simplefilter(&#34;ignore&#34;)
    #   new_thresholds = np.array([(GPTail(threshold = mu, shape = xi, scale = sigma) &gt;= other).threshold for mu, sigma, xi in zip(self.thresholds[indices], self.scales[indices], self.shapes[indices])])

    new_thresholds = np.array([max(other, threshold) for threshold in self.thresholds[indices]])
    new_shapes = self.shapes[indices]
    new_scales = self.scales[indices] + new_shapes*(new_thresholds - self.thresholds[indices])
    # new_thresholds = np.clip(self.thresholds[indices], a_min=other, a_max = np.Inf)
    # new_shapes = self.shapes[indices]
    # new_scales = self.scales[indices] + new_shapes*np.clip(other - new_thresholds, a_min = 0.0, a_max=np.Inf)

    if self.data is not None and np.all(self.data &lt; other):
      warnings.warn(f&#34;No observed data above {other}; setting data to None in conditioned model&#34;, stacklevel=2)
      new_data = None
    elif self.data is None:
      new_data = None
    else:
      new_data = self.data[self.data &gt; other]

    return type(self)(
      weights = new_weights,
      thresholds = new_thresholds,
      shapes = new_shapes,
      scales = new_scales,
      data = new_data)

  def __ge__(self, other: float) -&gt; BaseDistribution:
    return self.__gt__(other)

  def __add__(self, other: self._allowed_scalar_types):

    if type(other) not in self._allowed_scalar_types:
      raise TypeError(f&#34;+ is implemented for types {self._allowed_scalar_types}&#34;)

    new_data = None if self.data is None else self.data + other

    return type(self)(
      weights = self.weights,
      thresholds = self.thresholds + other,
      shapes = self.shapes,
      scales = self.scales,
      data = new_data)

  def __mul__(self, other: self._allowed_scalar_types):

    if type(other) not in self._allowed_scalar_types:
      raise TypeError(f&#34;* is implemented for types {self._allowed_scalar_types}&#34;)

    if other &lt;= 0:
      raise ValueError(f&#34;product supported for positive scalars only&#34;)

    new_data = None if self.data is None else other*self.data

    return type(self)(
      weights = self.weights,
      thresholds = other*self.thresholds,
      shapes = self.shapes,
      scales = other*self.scales,
      data = new_data)






class Empirical(BaseDistribution):

  &#34;&#34;&#34;Model for an empirical probability distribution, induced by a sample of data.

  Args:
      support (np.ndarray): distribution support
      pdf_values (np.ndarray): pdf array
      data (np.array, optional): data
  &#34;&#34;&#34;
  _sum_compatible = (GPTail, Mixture, GPTailMixture)

  support: np.ndarray
  pdf_values: np.ndarray
  data: t.Optional[np.ndarray]

  def __repr__(self):
    if self.data is not None:
      return f&#34;Empirical distribution with {len(self.data)} points&#34;
    else:
      return f&#34;Discrete distribution with support of size {len(self.pdf_values)}&#34;

  @validator(&#34;pdf_values&#34;, allow_reuse=True)
  def check_pdf_values(cls, pdf_values):
    if np.any(pdf_values &lt; -cls._error_tol):
      raise ValueError(&#34;There are negative pdf values&#34;)
    if not np.isclose(np.sum(pdf_values), 1, atol=cls._error_tol):
      print(f&#34;sum: {np.sum(pdf_values)}, pdf vals: {pdf_values}&#34;)
      raise ValueError(&#34;pdf values don&#39;t sum 1&#34;)
    # pdf_values = np.clip(pdf_values, a_min = 0.0, a_max = 1.0)
    # # normalise
    # pdf_values = pdf_values/np.sum(pdf_values)

    return pdf_values

  @property
  def is_valid(self):
    n = len(self.support)
    m = len(self.pdf_values)

    if n != m:
      raise ValueError(&#34;Lengths of support and pdf arrays don&#39;t match&#34;)

    if not np.all(np.diff(self.support) &gt; 0):
      raise ValueError(&#34;Support array must be in increasing order&#34;)

    return True 
  
  @property
  def pdf_lookup(self):
    &#34;&#34;&#34;Mapping from values in the support to their probability mass
    
    &#34;&#34;&#34;
    return {key: val for key, val in zip(self.support, self.pdf_values)}

  @property
  def min(self):
    &#34;&#34;&#34;Minimum value in the support
    &#34;&#34;&#34;
    return self.support[0]

  @property
  def max(self):
    &#34;&#34;&#34;Maximum value in the support
    
    &#34;&#34;&#34;
    return self.support[-1]

  @property
  def cdf_values(self):
    &#34;&#34;&#34;Mapping from values in the support to their cumulative probability
    
    &#34;&#34;&#34;
    x = np.cumsum(self.pdf_values)
    # make sure cdf reaches 1
    x[-1] = 1.0
    return x

  @property
  def ecdf(self):
    &#34;&#34;&#34;Mapping from values in the support to their cumulative probability
    
    &#34;&#34;&#34;
    cdf_vals = np.cumsum(self.pdf_values)
    # make sure cdf reaches 1
    cdf_vals[-1] = 1.0

    return ed.StepFunction(x=self.support, y=cdf_vals, side=&#34;right&#34;)

  @property
  def ecdf_inv(self):
    &#34;&#34;&#34;Linearly interpolated mapping from probability values to their quantiles
    
    &#34;&#34;&#34;
    return ed.monotone_fn_inverter(self.ecdf, self.support)
  
  def __mul__(self, factor: float):

    if not isinstance(factor, self._allowed_scalar_types) or factor == 0:
      raise TypeError(f&#34;multiplication is supported only for nonzero instances of type:{self._allowed_scalar_types}&#34;)

    new_data = None if self.data is None else self.data*factor
    return Empirical(support = factor*self.support, pdf_values = np.copy(self.pdf_values, order=&#34;C&#34;), data = new_data)

  def __add__(self, other: t.Union[int, float, GPTail, Mixture, GPTailMixture]):
    
    if isinstance(other, self._allowed_scalar_types):
      new_data = None if self.data is None else self.data + other
      return Empirical(support = self.support + other, pdf_values = np.copy(self.pdf_values, order=&#34;C&#34;), data = new_data)

    elif isinstance(other, GPTail):

      indices = (self.pdf_values &gt; 0).nonzero()[0]
      nz_pdf = self.pdf_values[indices]
      nz_support = self.support[indices]
      # mixed GPTails don&#39;t carry over exceedance data for efficient memory use
      #dists = [GPTail(threshold=other.threshold + x, scale=other.scale, shape=other.shape) for x in nz_support]

      return GPTailMixture(
        data = other.data,
        weights = nz_pdf,
        thresholds = other.threshold + nz_support,
        scales = np.array([other.scale for w in nz_support]),
        shapes = np.array([other.shape for w in nz_support])
        )

    elif isinstance(other, Mixture):
      return Mixture(weights=other.weights, distributions = [self + dist for dist in other.distributions])

    elif isinstance(other, GPTailMixture):
      # Return a new mixture where the old mixture is replicated for each point in the discrete support
      new_weights = np.concatenate([w*other.weights for w in self.pdf_values], axis = 0)
      new_th = np.concatenate([other.thresholds + t for t in self.support], axis = 0)
      new_scales = np.tile(other.scales, len(self.support))
      new_shapes = np.tile(other.shapes, len(self.support))
      return GPTailMixture(
        data = other.data,
        weights = new_weights[new_weights &gt; 0],
        thresholds = new_th[new_weights &gt; 0],
        scales = new_scales[new_weights &gt; 0],
        shapes = new_shapes[new_weights &gt; 0])

    else:
      raise TypeError(f&#34;+ is supported only for types: {self._sum_compatible}&#34;)

  def __neg__(self):

    return self.map(lambda x: -x)

  def __sub__(self, other: Empirical):

    if isinstance(other, (Empirical,)+ self._allowed_scalar_types):

      return self + (-other)

    else:
      raise TypeError(&#34;Subtraction is only defined for instances of Empirical or float &#34;)

  def __ge__(self, other:float):

    if not isinstance(other, self._allowed_scalar_types):
      raise TypeError(f&#34;&gt;= is implemented for instances of types : {self._allowed_scalar_types}&#34;)

    if 1 - self.cdf(other) + self.pdf(other) == 0.0:
      raise ValueError(f&#34;No probability mass above conditional threshold ({other}).&#34;)

    index = self.support &gt;= other

    new_data = None if self.data is None else self.data[self.data &gt;= other]

    pdf_vals = self.pdf_values[index]/np.sum(self.pdf_values[index])

    return type(self)(
      support = self.support[index],
      pdf_values = pdf_vals, 
      data = new_data)

  def __gt__(self, other:float):

    if not isinstance(other, self._allowed_scalar_types):
      raise TypeError(f&#34;&gt; is implemented for instances of types: {self._allowed_scalar_types}&#34;)

    if 1 - self.cdf(other) == 0:
      raise ValueError(f&#34;No probability mass above conditional threshold ({other}).&#34;)

    index = self.support &gt; other

    new_data = None if self.data is None else self.data[self.data &gt; other]

    return type(self)(
      support = self.support[index],
      pdf_values = self.pdf_values[index]/np.sum(self.pdf_values[index]), 
      data = new_data)


  def simulate(self, size: int) -&gt; np.ndarray:
    &#34;&#34;&#34;Draws simulated values from the distribution
    
    Args:
        size (int): Sample size
    
    Returns:
        np.ndarray: simulated sample
    &#34;&#34;&#34;
    return np.random.choice(self.support, size=size, p=self.pdf_values)

  def moment(self, n: int, **kwargs) -&gt; float:
    &#34;&#34;&#34;Evaluates the n-th moment of the distribution
    
    Args:
        n (int): moment order
        **kwargs: dummy additional arguments (not used)
    
    Returns:
        float: n-th moment value
    &#34;&#34;&#34;
    return np.sum(self.pdf_values * self.support**n)

  def ppf(self, q: t.Union[float, np.ndarray], **kwargs) -&gt; t.Union[float, np.ndarray]:
    &#34;&#34;&#34;Inverse CDF function; it uses linear interpolation.
    
    Args:
        q (t.Union[float, np.ndarray]): probability level
    
    Returns:
        t.Union[float, np.ndarray]: Linearly interpolated quantile function
    
    &#34;&#34;&#34;
    is_scalar = isinstance(q, self._allowed_scalar_types)

    if is_scalar:
      q = np.array([q])

    if np.any(q &lt; 0) or np.any(q &gt;1):
      raise ValueError(f&#34;q needs to be in the interval [0,1]&#34;)

    ppf_values = np.empty((len(q),))

    left_vals_idx = q &lt;= self.ecdf_inv.x[0]
    right_vals_idx = q &gt;= self.ecdf_inv.x[-1]
    inside_vals_idx = np.logical_and(np.logical_not(left_vals_idx), np.logical_not(right_vals_idx))

    ppf_values[left_vals_idx] = self.ecdf_inv.y[0]
    ppf_values[right_vals_idx] = self.ecdf_inv.y[-1]
    ppf_values[inside_vals_idx] = self.ecdf_inv(q[inside_vals_idx])

    if is_scalar:
      return ppf_values[0]
    else:
      return ppf_values

  def cdf(self, x: t.Union[float, np.ndarray], **kwargs) -&gt; t.Union[float, np.ndarray]:
    return self.ecdf(x)

  def pdf(self, x: t.Union[float, np.ndarray], **kwargs):

    if isinstance(x, np.ndarray):
      return np.array([self.pdf(elem) for elem in x])

    try:
      pdf_val = self.pdf_lookup[x]
      return pdf_val
    except KeyError as e:
      return 0.0

  def std(self, **kwargs):
    return np.sqrt(self.map(lambda x: x - self.mean()).moment(2))

  @classmethod
  def from_data(cls, data: np.array):

    support, unnorm_pdf = np.unique(data, return_counts=True)
    n = np.sum(unnorm_pdf)
    return cls(
      support=support, 
      pdf_values=unnorm_pdf/n, 
      data=data)

  def plot_mean_residual_life(self, threshold: float) -&gt; matplotlib.figure.Figure:
    &#34;&#34;&#34;Produces a mean residual life plot for the tail of the distribution above a given threshold
    
    Args:
        threshold (float): threshold value
    
    Returns:
        matplotlib.figure.Figure: figure
    
    
    &#34;&#34;&#34;
    fig = plt.figure()
    fitted = self.fit_tail_model(threshold)
    scale, shape = fitted.tail.scale, fitted.tail.shape

    x_vals = np.linspace(threshold, max(self.data))
    if shape &gt;= 1:
      raise ValueError(f&#34;Expectation is not finite: fitted shape parameter is {shape}&#34;)
    #y_vals = (scale + shape*x_vals)/(1-shape)
    y_vals = np.array([np.mean(fitted.tail.data[fitted.tail.data &gt;= x]) for x in x_vals])
    plt.plot(x_vals,y_vals, color=self._figure_color_palette[0])
    plt.scatter(x_vals, y_vals, color=self._figure_color_palette[0])
    plt.title(&#34;Mean residual life plot&#34;)
    plt.xlabel(&#34;Threshold&#34;)
    plt.ylabel(&#34;Mean exceedance&#34;)
    return fig

  def fit_tail_model(self, threshold: float, bayesian=False, **kwargs) -&gt; t.Union[EmpiricalWithGPTail, EmpiricalWithBayesianGPTail]:
    &#34;&#34;&#34;Fits a tail GP model above a specified threshold and return the fitted semiparametric model
    
    Args:
        threshold (float): Threshold above which a Generalised Pareto distribution will be fitted
        bayesian (bool, optional): If True, fit model through Bayesian inference 
        **kwargs: Additional parameters passed to BayesianGPTail.fit or to GPTail.fit
    
    Returns:
        t.Union[EmpiricalWithGPTail, EmpiricalWithBayesianGPTail]
    
    &#34;&#34;&#34;

    if threshold &gt;= self.max:
      raise ValueError(&#34;Empirical pdf is 0 above the provided threshold. Select a lower threshold for estimation.&#34;)

    if self.data is None:
      raise ValueError(&#34;Data is not set for this distribution, so a tail model cannot be fitted. You can simulate from it and use the sampled data instead&#34;)
    else:
      data = self.data

    if bayesian:
      return EmpiricalWithBayesianGPTail.from_data(data, threshold, bin_empirical=isinstance(self, Binned), **kwargs)
    else:
      return EmpiricalWithGPTail.from_data(data, threshold, bin_empirical=isinstance(self, Binned), **kwargs)

  def map(self, f: t.Callable) -&gt; Empirical:
    &#34;&#34;&#34;Returns the distribution resulting from an arbitrary transformation
    
    Args:
        f (t.Callable): Target transformation; it should take a numpy array as input
    
    &#34;&#34;&#34;
    dist_df = pd.DataFrame({&#34;pdf&#34;: self.pdf_values, &#34;support&#34;: f(self.support)})
    mapped_dist_df = dist_df.groupby(&#34;support&#34;).sum().reset_index().sort_values(&#34;support&#34;)

    return Empirical(
      support = np.array(mapped_dist_df[&#34;support&#34;]),
      pdf_values = np.array(mapped_dist_df[&#34;pdf&#34;]),
      data = f(self.data) if self.data is not None else None)

  def to_integer(self):
    &#34;&#34;&#34;Convert to Binned distribution
    
    &#34;&#34;&#34;
    return Binned.from_empirical(self)





class Binned(Empirical):

  &#34;&#34;&#34;Empirical distribution with an integer support. This allows it to be convolved with other integer distribution to obtain the distribution of a sum of random variables, assuming independence between the summands. 
  &#34;&#34;&#34;
  
  _supported_types = [np.int64, int]

  def __repr__(self):
    return f&#34;Integer distribution with support of size {len(self.pdf_values)} ({np.sum(self.pdf_values&gt; 0)} non-zero)&#34;

  @validator(&#34;support&#34;, allow_reuse=True)
  def integer_support(cls, support):
    n = len(support)
    if not np.all(np.diff(support) == 1):
      raise ValueError(&#34;The support vector must contain every integer between its minimum and maximum value&#34;)
    elif support.dtype not in cls._supported_types:
      raise ValueError(f&#34;Support entry types must be one of {self._supported_types}&#34;)
    else:
      return support

  def __mul__(self, factor: self._allowed_scalar_types):

    if not isinstance(factor, self._allowed_scalar_types) or factor == 0:
      raise TypeError(f&#34;multiplication is supported only for nonzero instances of type:{self._allowed_scalar_types}&#34;)

    if isinstance(factor, int):
      return self.from_empirical(float(factor) * self)
      #new_data = None if self.data is None else self.data*factor
      #return Binned(support = factor*self.support, pdf_values = self.pdf_values, data = new_data)

    else:
      return super().__mul__(factor)

  def __add__(self, other: t.Union[float, int, Binned, GPTail, Mixture]):

    if isinstance(other, int):
      new_support = np.arange(min(self.support) + other, max(self.support) + other + 1)
      new_data = None if self.data is None else self.data + other
      return Binned(
        support = new_support, 
        pdf_values = np.copy(self.pdf_values, order=&#34;C&#34;), 
        data = new_data)

    if isinstance(other, Binned):
      new_support = np.arange(min(self.support) + min(other.support), max(self.support) + max(other.support) + 1)

      pdf_vals = fftconvolve(self.pdf_values, other.pdf_values)
      pdf_vals = np.abs(pdf_vals) #some values are negative due to numerical rounding error, set to positive (they are infinitesimal in any case)
      pdf_vals = pdf_vals/np.sum(pdf_vals)

      # this block removes trailing support elements with zero probability
      while pdf_vals[-1] == 0.0:
        new_support = new_support[0:len(pdf_vals)-1]
        pdf_vals = pdf_vals[0:len(pdf_vals)-1]

      # add any missing probability mass
      error = 1.0 - np.sum(pdf_vals)
      pdf_vals[0] += np.sign(error)*np.abs(error)

      return Binned(
        support = new_support, 
        pdf_values = np.abs(pdf_vals), #some negative values persist
        data = None)

    else:
      return super().__add__(other)

  def __sub__(self, other: float):

    if isinstance(other, (int, Binned)):
      return self + (-other)
    else:
      super().__sub__(other)

  def __neg__(self):

    return super().__neg__().to_integer() 

  @classmethod
  def from_data(cls, data: np.ndarray) -&gt; Binned:
    &#34;&#34;&#34;Instantiates a Binned object from observed data
    
    Args:
        data (np.ndarray): Observed data
    
    Returns:
        Binned: Integer distribution object
    &#34;&#34;&#34;
    data = np.array(data)
    if data.dtype not in self._supported_types:
      warnings.warn(&#34;Casting input data to integer values by rounding&#34;, stacklevel=2)
      data = data.astype(np.int64)

    return super().from_data(data).to_integer()

  @classmethod
  def from_empirical(cls, dist: Empirical) -&gt; Binned:
    &#34;&#34;&#34;Takes an Empirical instance with discrete support and creates a Binned instance by casting the support to integer values and filling the gaps in the support
    
    
    Args:
        empirical_dist (Empirical): Empirical instance
    
    Returns:
        Binned: Binned distribution
    
    
    &#34;&#34;&#34;
    empirical_dist = dist.map(lambda x: np.round(x))
    base_support = empirical_dist.support.astype(Binned._supported_types[0])
    full_support = np.arange(min(base_support), max(base_support) + 1)
    
    full_pdf = np.zeros((len(full_support,)), dtype=np.float64)
    indices = base_support - min(base_support)
    full_pdf[indices] = empirical_dist.pdf_values
    # try:
    #   full_pdf[indices] = empirical_dist.pdf_values
    # except Exception as e:
    #   print(f&#34;base support: {base_support}&#34;)
    #   print(f&#34;full support: {full_support}&#34;)

    data = None if empirical_dist.data is None else empirical_dist.data.astype(Binned._supported_types[0])

    return Binned(
      support = full_support.astype(cls._supported_types[0]), 
      pdf_values = full_pdf,
      data = data)

  def pdf(self, x: t.Union[float, np.ndarray], **kwargs):

    if isinstance(x, np.ndarray):
      return np.array([self.pdf(elem) for elem in x])

    if not isinstance(x, (int, np.int32, np.int64)) or x &gt; self.max or x &lt; self.min:
      return 0.0
    else:
      return self.pdf_values[x - self.min]









class EmpiricalWithGPTail(Mixture):

  &#34;&#34;&#34;Represents a semiparametric extreme value model with a fitted Generalized Pareto distribution above a certain threshold, and an empirical distribution below it

  &#34;&#34;&#34;
  threshold: float

  def __repr__(self):
    return f&#34;Semiparametric model with generalised Pareto tail. Modeling threshold: {self.threshold}, exceedance probability: {self.exs_prob}&#34;

  @property
  def empirical(self) -&gt; Empirical:
    &#34;&#34;&#34;Empirical distribution below the modeling threshold
    
    Returns:
        Empirical: Distribution object
    &#34;&#34;&#34;
    return self.distributions[0]

  @property
  def tail(self) -&gt; GPTail:
    &#34;&#34;&#34;Generalised Pareto tail model above modeling threshold
    
    Returns:
        GPTail: Distribution
    &#34;&#34;&#34;
    return self.distributions[1]
  
  # @property
  # def threshold(self) -&gt; float:
  #   &#34;&#34;&#34;Modeling threshold
    
  #   Returns:
  #       float: threshold value
  #   &#34;&#34;&#34;
  #   return self.distributions[1].thresholds

  @property
  def exs_prob(self) -&gt; float:
    &#34;&#34;&#34;Probability mass above threshold; probability weight of tail model.
    
    Returns:
        float: weight
    &#34;&#34;&#34;
    return self.weights[1]

  def ppf(self, q: t.Union[float, np.ndarray]) -&gt; t.Union[float, np.ndarray]:

    
    is_scalar = isinstance(q, self._allowed_scalar_types)

    if is_scalar:
      q = np.array([q])

    lower_idx = q &lt;= 1 - self.exs_prob
    higher_idx = np.logical_not(lower_idx)

    ppf_values = np.empty((len(q),))
    ppf_values[lower_idx] = self.empirical.ppf(q[lower_idx]/(1-self.exs_prob))
    ppf_values[higher_idx] = self.tail.ppf((q[higher_idx] - (1-self.exs_prob))/self.exs_prob)
    
    if is_scalar:
      return ppf_values[0]
    else:
      return ppf_values


  @classmethod
  def from_data(cls, data: np.ndarray, threshold: float, bin_empirical: bool = False, **kwargs) -&gt; EmpiricalWithGPTail:
    &#34;&#34;&#34;Fits a model from a given data array and threshold value
    
    Args:
        data (np.ndarray): Data 
        threshold (float): Threshold value to use for the tail model
        bin_empirical (bool, optional): Whether to cast empirical mixture component to an integer distribution by rounding
        **kwargs: Additional arguments passed to GPTail.fit
    
    
    Returns:
        EmpiricalWithGPTail: Fitted model
    &#34;&#34;&#34;
    exs_prob = 1 - Empirical.from_data(data).cdf(threshold)

    exceedances = data[data &gt; threshold]
    
    empirical = Empirical.from_data(data[data &lt;= threshold])
    if bin_empirical:
      empirical = empirical.to_integer()

    tail = GPTail.fit(data=exceedances, threshold=threshold, **kwargs)

    return cls(
      distributions = [empirical, tail],
      weights = np.array([1 - exs_prob, exs_prob]),
      threshold = threshold)

  def plot_diagnostics(self) -&gt; matplotlib.figure.Figure:
    return self.tail.plot_diagnostics()

  def plot_return_levels(self) -&gt; matplotlib.figure.Figure:
    &#34;&#34;&#34;Returns a figure with a return level plot using the fitted tail model
    
    Returns:
        matplotlib.figure.Figure: figure
    
    &#34;&#34;&#34;
    fig = plt.figure()
    scale, shape = self.tail.scale, self.tail.shape
    exs_prob = self.exs_prob
    
    if self.tail.data is None:
      n_obs = np.Inf # this only means that threshold estimation variance is ignored in figure confidence bounds
    else:
      n_obs = len(self.tail.data)

    #exceedance_frequency = 1/np.logspace(1,4,20)
    #exceedance_frequency = exceedance_frequency[exceedance_frequency &lt; exs_prob] #plot only levels inside fitted tail model
    # shown return levels go from largest power of 10th below exceedance prob, to 1/10000-th of that.

    x_min = np.floor(np.log(exs_prob)/np.log(10))
    x_max = x_min - 4
    exceedance_frequency = 10**(np.linspace(x_min, x_max, 50))
    return_levels = self.ppf(1 - exceedance_frequency)

    plt.plot(1.0/exceedance_frequency,return_levels,color=self._figure_color_palette[0])
    plt.xscale(&#34;log&#34;)
    plt.title(&#34; Return levels&#34;)
    plt.xlabel(&#39;Return period&#39;)
    plt.ylabel(&#39;Return level&#39;)
    plt.grid()

    try:
      #for this bit, look at An Introduction to Statistical selfing of Extreme Values, p.82
      mle_cov = self.tail.mle_cov()
      eigenvals, eigenvecs = np.linalg.eig(mle_cov)
      if np.all(eigenvals &gt; 0):
        covariance = np.eye(3)
        covariance[1::,1::] = mle_cov
        covariance[0,0] = exs_prob*(1-exs_prob)/n_obs
        #
        return_stdevs = []
        for m in 1.0/exceedance_frequency:
          quantile_grad = np.array([
            scale*m*(m*exs_prob)**(shape-1),
            1/shape*((exs_prob*m)**shape-1),
            -scale/shape**2*((exs_prob*m)**shape-1)+scale/shape*(exs_prob*m)**shape*np.log(exs_prob*m)
            ])
          #
          sdev = np.sqrt(quantile_grad.T.dot(covariance).dot(quantile_grad))
          return_stdevs.append(sdev)
        #
        return_stdevs = np.array(return_stdevs)
        plt.fill_between(1.0/exceedance_frequency, return_levels - 1.96*return_stdevs, return_levels + 1.96*return_stdevs, alpha=0.2, color=self._figure_color_palette[1])
      else:
        warnings.warn(&#34;Covariance MLE matrix is not positive definite; it might be ill-conditioned&#34;, stacklevel=2)
    except Exception as e:
      warnings.warn(f&#34;Confidence bands for return level could not be calculated; covariance matrix might be ill-conditioned; full trace: {traceback.format_exc()}&#34;, stacklevel=2)

    return fig


class BayesianGPTail(GPTailMixture):

  &#34;&#34;&#34;Generalised Pareto tail model which is fitted through Bayesian inference, using uninformative (uniform) priors for the shape and scale parameters

  Args:
      threshold (float): modeling threshold
      data (np.array, optional): exceedance data
      shape (np.ndarray): sample from posterior shape distribution
      scale (np.ndarray): sample from posterior scale distribution
  &#34;&#34;&#34;

  #_self.posterior_trace = None

  @classmethod
  def fit(
    cls, 
    data: np.ndarray, 
    threshold: float, 
    max_posterior_samples: int = 1000,
    chain_length: int = 2000,
    x0: np.ndarray = None,
    plot_diagnostics: bool = False,
    n_walkers: int = 32,
    n_cores: int = 4,
    burn_in: int = 100,
    thinning: int = None,
    log_prior: t.Callable = None) -&gt; BayesianGPTail:
    &#34;&#34;&#34;Fits a Generalised Pareto model through Bayesian inference using exceedance data, starting with flat, uninformative priors for both and sampling from posterior shape and scale parameter distributions.
    
    Args:
        data (np.ndarray): observational data
        threshold (float): modeling threshold; location parameter for Generalised Pareto model
        max_posterior_samples (int, optional): Maximum number of posterior samples to keep
        chain_length (int, optional): timesteps in each chain
        x0 (np.ndarray, optional): Starting point for the chains. If None, MLE estimates are used.
        plot_diagnostics (bool, optional): If True, plots MCMC diagnostics in the background.
        n_walkers (int, optional): Number of concurrent paths to use
        n_cores (int, optional): Number of cores to use
        burn_in (int, optional): Number of initial samples to drop
        thinning (int, optional): Thinning factor to reduce autocorrelation; if None, an automatic estimate from emcee&#39;s `get_autocorr_time` is used.
        log_prior (t.Callable, optional): Function that takes as input a single length-2 iterable with scale and shape parameters and outputs the prior log-likelihood. If None, a constant prior on the valid parameter support is used.
    
    Returns:
        BayesianGPTail: fitted model
    
    &#34;&#34;&#34;
    exceedances = data[data &gt; threshold]
    x_max = max(data - threshold)

    # def log_likelihood(theta, data):
    #   scale, shape = theta
    #   return np.sum(gpdist.logpdf(data, c=shape, scale=scale, loc=threshold))

    if log_prior is None:
      def log_prior(theta):
        scale, shape = theta
        if scale &gt; 0 and shape &gt; -scale/(x_max):
          return 0.0
        else:
          return -np.Inf

    def log_probability(theta, data):
      prior = log_prior(theta)
      if np.isfinite(prior):
        ll = prior + GPTail.loglik(theta, threshold, data)#log_likelihood(theta, data)
        #print(theta, ll)
        return ll
      else:
        return -np.Inf

    exceedances = data[data &gt; threshold]
    ndim = 2

    if x0 is None:
      # make initial guess
      mle_model = GPTail.fit(data=exceedances, threshold=threshold)
      shape, scale = mle_model.shape, mle_model.scale
      x0 = np.array([scale, shape])

    # create random walkers
    pos =  x0 + 1e-4 * np.random.randn(n_walkers, ndim)

    with Pool(n_cores) as pool:
      sampler = emcee.EnsembleSampler(nwalkers=n_walkers, ndim=ndim, log_prob_fn=log_probability, args=(exceedances, ))
      sampler.run_mcmc(pos, chain_length, progress=True)

    samples = sampler.get_chain()

    tau = sampler.get_autocorr_time()
    thinning = int(np.round(np.mean(tau)))
    print(f&#34;Using a thinning factor of {thinning} (from emcee.EnsembleSampler.get_autocorr_time)&#34;)
    flat_samples = sampler.get_chain(discard=burn_in, thin=thinning, flat=True)

    if flat_samples.shape[0] &gt; max_posterior_samples:
      np.random.shuffle(flat_samples)
      flat_samples = flat_samples[0:max_posterior_samples,:]

    n_samples = flat_samples.shape[0]
    print(f&#34;Got {n_samples} posterior samples.&#34;)

    if plot_diagnostics:
      fig, axes = plt.subplots(2, figsize=(10, 7), sharex=True)
      labels = [&#34;scale&#34;, &#34;shape&#34;]
      for i in range(ndim):
          ax = axes[i]
          ax.plot(samples[:, :, i], alpha=0.3, color=cls._figure_color_palette[0])
          ax.set_xlim(0, len(samples))
          ax.set_ylabel(labels[i])
          if i == 0:
            ax.set_title(&#34;Chain mixing&#34;)
          ax.yaxis.set_label_coords(-0.1, 0.5)
      print(&#34;Use pyplot.show() to view chain diagnostics.&#34;)


    scale_posterior = flat_samples[:,0]
    shape_posterior = flat_samples[:,1]

    return cls(
      weights = 1/n_samples*np.ones((n_samples,), dtype=np.float64),
      thresholds = threshold*np.ones((n_samples,)),
      data = exceedances,
      shapes = shape_posterior,
      scales = scale_posterior)

  def plot_diagnostics(self) -&gt; matplotlib.figure.Figure:
    &#34;&#34;&#34;Returns a figure with fit diagnostic plots for the GP model
    
    Returns:
        matplotlib.figure.Figure: figure
    
    &#34;&#34;&#34;
    if self.data is None:
      raise ValueError(&#34;Exceedance data was not provided for this model.&#34;)

    def map_to_colors( vals ):
      colours = np.zeros( (len(vals),3) )
      norm = Normalize( vmin=vals.min(), vmax=vals.max() )
      #Can put any colormap you like here.
      colours = [cm.ScalarMappable( norm=norm, cmap=&#39;cool&#39;).to_rgba( val ) for val in vals]
      return colours

    fig, axs = plt.subplots(3, 2)


    #################### Bayesian inference diagnostics: posterior histograms and scatterplot

    axs[0,0].hist(self.scales, bins=25, edgecolor=&#34;white&#34;, color=self._figure_color_palette[0])
    axs[0,0].title.set_text(&#34;Posterior scale histogram&#34;)

    axs[0,1].hist(self.shapes, bins=25, edgecolor=&#34;white&#34;, color=self._figure_color_palette[0])
    axs[0,1].title.set_text(&#34;Posterior shape histogram&#34;)

    posterior = np.concatenate([self.scales.reshape((-1,1)), self.shapes.reshape((-1,1))], axis = 1)
    kernel = kde(posterior.T)

    colours = map_to_colors(kernel.evaluate(posterior.T))

    axs[1,0].scatter(self.scales, self.shapes, color=colours )
    axs[1,0].title.set_text(&#34;Posterior sample&#34;)
    axs[1,0].set_xlabel(&#34;Scale&#34;)
    axs[1,0].set_ylabel(&#34;Shape&#34;)

    ############## histogram vs density ################
    hist_data = axs[1,1].hist(self.data, bins=25, edgecolor=&#34;white&#34;, color=self._figure_color_palette[0])

    range_min, range_max = min(self.data), max(self.data)
    x_axis = np.linspace(range_min, range_max, 100)
    
    mean_pdf_vals = np.array( [self.pdf(x) for x in x_axis])
    # q025_pdf_vals = np.array( [np.quantile(self.pdf(x, return_all=True),0.025) for x in x_axis])
    # q975_pdf_vals = np.array( [np.quantile(self.pdf(x, return_all=True),0.975) for x in x_axis])

    y_axis = hist_data[0][0] / mean_pdf_vals[0] * mean_pdf_vals

    axs[1,1].plot(x_axis, y_axis, color=self._figure_color_palette[1])
    #axs[1,1].fill_between(x_axis, q025_pdf_vals, q975_pdf_vals, alpha=0.2, color=self._figure_color_palette[1])
    axs[1,1].title.set_text(&#34;Data vs fitted density&#34;)
    axs[1,1].set_xlabel(&#39;Exceedance data&#39;)
    axs[1,1].yaxis.set_visible(False) # Hide only x axis
    #axs[0, 0].set_aspect(&#39;equal&#39;, &#39;box&#39;)


    ############# Q-Q plot ################
    probability_range = np.linspace(0.01,0.99, 99)
    empirical_quantiles = np.quantile(self.data, probability_range)
    
    posterior_quantiles = [self.ppf(p, return_all=True) for p in probability_range]

    #hat_return_levels are not the mean of posterior return level samples, as the mean is not an unbiased estimator
    hat_tail_quantiles = np.array([self.ppf(p) for p in probability_range])

    #q025_tail_quantiles = np.array([np.quantile(q, 0.025) for q in posterior_quantiles])
    #q975_tail_quantiles = np.array([np.quantile(q, 0.975) for q in posterior_quantiles])

    axs[2,0].scatter(hat_tail_quantiles, empirical_quantiles, color = self._figure_color_palette[0])
    min_x, max_x = min(hat_tail_quantiles), max(hat_tail_quantiles)
    #axs[0,1].set_aspect(&#39;equal&#39;, &#39;box&#39;)
    axs[2,0].title.set_text(&#39;Q-Q plot&#39;)
    axs[2,0].set_xlabel(&#39;model quantiles&#39;)
    axs[2,0].set_ylabel(&#39;Exceedance quantiles&#39;)
    axs[2,0].grid()
    axs[2,0].plot([min_x,max_x],[min_x,max_x], linestyle=&#34;--&#34;, color=&#34;black&#34;)
    #axs[2,0].fill_between(hat_tail_quantiles, q025_tail_quantiles, q975_tail_quantiles, alpha=0.2, color=self._figure_color_palette[1])

    plt.tight_layout()
    return fig






class EmpiricalWithBayesianGPTail(EmpiricalWithGPTail):

  &#34;&#34;&#34;Semiparametric Bayesian model with an empirical data distribution below a specified threshold and a Generalised Pareto exceedance model above it, fitted through Bayesian inference.
  &#34;&#34;&#34;
  
  @classmethod
  def from_data(
    cls, 
    data: np.ndarray, 
    threshold: float, 
    bin_empirical: bool = False,
    **kwargs) -&gt; EmpiricalWithBayesianGPTail:
    &#34;&#34;&#34;Fits a Generalied Pareto tail model from a given data array and threshold value, using Jeffrey&#39;s priors 
    
    Args:
        data (np.ndarray): data array 
        threshold (float): Threshold value to use for the tail model
        bin_empirical (bool, optional): Whether to cast empirical mixture component to an integer distribution by rounding
        **kwargs: Additional arguments to be passed to BayesianGPTail.fit
    
    Returns:
        EmpiricalWithBayesianGPTail: Fitted model
    
    Deleted Parameters:
        n_posterior_samples (int): Number of samples from posterior distribution
    
    &#34;&#34;&#34;
    exs_prob = 1 - Empirical.from_data(data).cdf(threshold)

    exceedances = data[data &gt; threshold]

    empirical = Empirical.from_data(data[data &lt;= threshold])
    if bin_empirical:
      empirical = empirical.to_integer()

    tail = BayesianGPTail.fit(data = exceedances, threshold = threshold, **kwargs)

    return cls(
      weights = np.array([1 -exs_prob, exs_prob]),
      distributions = [empirical, tail],
      threshold = threshold)

  def ppf(self, q: t.Union[float, np.ndarray], return_all=False) -&gt; t.Union[float, np.ndarray]:
    &#34;&#34;&#34;Returns the quantile function evaluated at some probability level
    
    Args:
        q (t.Union[float, np.ndarray]): probability level
        return_all (bool, optional): If True, returns posterior ppf sample; otherwise return pointwise estimator
    
    Returns:
        t.Union[float, np.ndarray]: ppf value(s).
    &#34;&#34;&#34;
    if isinstance(q, np.ndarray):
      return np.array([self.ppf(elem) for elem in q])

    if q &lt;= 1 - self.exs_prob:
      val = self.empirical.ppf(q/(1-self.exs_prob), return_all=return_all)
      # if a vector is expected as output, vectorize scalar
      if return_all:
        return val * np.ones((len(self.tail.shapes),))
      else:
        return val
    else:
      return self.tail.ppf((q - (1-self.exs_prob))/self.exs_prob, return_all=return_all)

  def plot_return_levels(self) -&gt; matplotlib.figure.Figure:
    &#34;&#34;&#34;Returns a figure with a return level plot using the fitted tail model
    
    Returns:
        matplotlib.figure.Figure: figure
    
    &#34;&#34;&#34;
    fig = plt.figure()
    exs_prob = self.exs_prob

    exceedance_frequency = 1/np.logspace(1,4,20)
    exceedance_frequency = exceedance_frequency[exceedance_frequency &lt; exs_prob] #plot only levels inside fitted tail model

    return_levels = [self.ppf(1-x, return_all=True) for x in exceedance_frequency]

    #hat_return_levels are not the mean of posterior return level samples, as the mean is not an unbiased estimator
    hat_return_levels = np.array([self.ppf(1-x, return_all=False) for x in exceedance_frequency])
    q025_return_levels = np.array([np.quantile(r, 0.025) for r in return_levels])
    q975_return_levels = np.array([np.quantile(r, 0.975) for r in return_levels])

    plt.plot(1.0/exceedance_frequency,hat_return_levels,color=self._figure_color_palette[0])
    plt.fill_between(1.0/exceedance_frequency, q025_return_levels, q975_return_levels, alpha=0.2, color=self._figure_color_palette[1])
    plt.xscale(&#34;log&#34;)
    plt.title(&#39;Exceedance return levels&#39;)
    plt.xlabel(&#39;1/frequency&#39;)
    plt.ylabel(&#39;Return level&#39;)
    plt.grid()

    return fig</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="riskmodels.univariate.BaseDistribution"><code class="flex name class">
<span>class <span class="ident">BaseDistribution</span></span>
<span>(</span><span>**data:Â Any)</span>
</code></dt>
<dd>
<div class="desc"><p>Base interface for available data model types</p>
<p>Create a new model by parsing and validating input data from keyword arguments.</p>
<p>Raises ValidationError if the input data cannot be parsed to form a valid model.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BaseDistribution(BaseModel):

  &#34;&#34;&#34;Base interface for available data model types
  &#34;&#34;&#34;
  _allowed_scalar_types = (int, float, np.int64, np.int32, np.float32, np.float64)
  _figure_color_palette = [&#34;tab:cyan&#34;, &#34;deeppink&#34;]
  _error_tol = 1e-6

  def __repr__(self):
    return &#34;Base distribution object&#34;

  def __str__(self):
    return self.__repr__()

  class Config:
    arbitrary_types_allowed = True

  @abstractmethod
  def simulate(self, size: int) -&gt; np.ndarray:
    &#34;&#34;&#34;Produces simulated values from model
    
    Args:
        n (int): Number of samples
    &#34;&#34;&#34;
    pass

  @abstractmethod
  def moment(self, n: int) -&gt; float:
    &#34;&#34;&#34;Calculates non-centered moments
    
    Args:
        n (int): moment order
    &#34;&#34;&#34;
    pass

  @abstractmethod
  def ppf(self, q: float) -&gt; float:
    &#34;&#34;&#34;Calculates the corresponding quantile for a given probability value
    
    Args:
        q (float): probability level
    &#34;&#34;&#34;
    pass

  def mean(self, **kwargs) -&gt; float:
    &#34;&#34;&#34;Calculates the expected value
    
    Args:
        **kwargs: Additional arguments passed to `moments`. This is needed for Bayesian model instances in which a `return_all` parameter can be passed.
    
    Returns:
        float: mean value
    
    &#34;&#34;&#34;
    return self.moment(1, **kwargs)

  def std(self, **kwargs) -&gt; float:
    &#34;&#34;&#34;Calculates the standard deviation
    
    Args:
        **kwargs: Additional arguments passed to `moments`. This is needed for Bayesian model instances in which a `return_all` parameter can be passed.
    
    Returns:
        float: standard deviation value
    
    &#34;&#34;&#34;
    return np.sqrt(self.moment(2, **kwargs) - self.mean(**kwargs)**2)

  @abstractmethod
  def cdf(self, x:float) -&gt; float:
    &#34;&#34;&#34;Evaluates the cumulative probability function
    
    Args:
        x (float): a point in the support
    &#34;&#34;&#34;
    pass

  @abstractmethod
  def pdf(self, x:float) -&gt; float:
    &#34;&#34;&#34;Calculates the probability mass or probability density function
    
    Args:
        x (float): a point in the support
    &#34;&#34;&#34;
    pass

  def histogram(self, size: int = 1000) -&gt; None:
    &#34;&#34;&#34;Plots a histogram of a simulated sample
    
    Args:
        size (int, optional): sample size
    
    &#34;&#34;&#34;

    #show histogram from 1k samples
    samples = self.simulate(size=size)
    plt.hist(samples, bins=25, edgecolor=&#34;white&#34;, color=self._figure_color_palette[0])
    plt.title(f&#34;Histogram from {np.round(size/1000,1)}K simulated samples&#34;)
    plt.show()

  def plot(self, size: int = 1000) -&gt; None:
    &#34;&#34;&#34;Plots a histogram of a simulated sample
    
    Args:
        size (int, optional): sample size
    
    &#34;&#34;&#34;
    self.histogram(size)

  def cvar(self, p: float, **kwargs) -&gt; float:
    &#34;&#34;&#34;Calculates conditional value at risk for a probability level p, defined as the mean conditioned to an exceedance above the p-quantile.
    
    Args:
        p (float): Description
        **kwargs: Additional arguments passed to `moments`. This is needed for Bayesian model instances in which a `return_all` parameter can be passed.
    
    Returns:
        float: conditional value at risk
    
    Raises:
        ValueError: Description
    
    &#34;&#34;&#34;
    if not isinstance(p, float) or p &lt; 0 or p &gt;= 1:
      raise ValueError(&#34;p must be in the open interval (0,1)&#34;)

    return (self &gt;= self.ppf(p)).mean(**kwargs)

  @abstractmethod
  def __gt__(self, other: float) -&gt; BaseDistribution:
    pass

  @abstractmethod
  def __ge__(self, other: float) -&gt; BaseDistribution:
    pass

  @abstractmethod
  def __add__(self, other: self._allowed_scalar_types): 
    pass

  @abstractmethod
  def __mul__(self, other: self._allowed_scalar_types):
    pass

  def __sub__(self, other: float):

    if not isinstance(other, self._allowed_scalar_types):
      raise TypeError(f&#34;- is implemented for instances of types: {self._allowed_scalar_types}&#34;)

    return self.__add__(-other)

  def __rmul__(self, other):
    return self.__mul__(other)

  def __radd__(self, other):
    return self.__add__(other)

  def __rsub__(self, other):
    return self.__sub__(other)

  #__radd__ = __add__

  #__rmul__ = __mul__

  #__rsub__ = __sub__</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>pydantic.main.BaseModel</li>
<li>pydantic.utils.Representation</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="riskmodels.univariate.Empirical" href="#riskmodels.univariate.Empirical">Empirical</a></li>
<li><a title="riskmodels.univariate.GPTail" href="#riskmodels.univariate.GPTail">GPTail</a></li>
<li><a title="riskmodels.univariate.GPTailMixture" href="#riskmodels.univariate.GPTailMixture">GPTailMixture</a></li>
<li><a title="riskmodels.univariate.Mixture" href="#riskmodels.univariate.Mixture">Mixture</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="riskmodels.univariate.BaseDistribution.Config"><code class="name">var <span class="ident">Config</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="riskmodels.univariate.BaseDistribution.cdf"><code class="name flex">
<span>def <span class="ident">cdf</span></span>(<span>self, x:Â float) â€‘>Â float</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluates the cumulative probability function</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>float</code></dt>
<dd>a point in the support</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def cdf(self, x:float) -&gt; float:
  &#34;&#34;&#34;Evaluates the cumulative probability function
  
  Args:
      x (float): a point in the support
  &#34;&#34;&#34;
  pass</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.BaseDistribution.cvar"><code class="name flex">
<span>def <span class="ident">cvar</span></span>(<span>self, p:Â float, **kwargs) â€‘>Â float</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates conditional value at risk for a probability level p, defined as the mean conditioned to an exceedance above the p-quantile.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>p</code></strong> :&ensp;<code>float</code></dt>
<dd>Description</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional arguments passed to <code>moments</code>. This is needed for Bayesian model instances in which a <code>return_all</code> parameter can be passed.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>conditional value at risk</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>Description</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cvar(self, p: float, **kwargs) -&gt; float:
  &#34;&#34;&#34;Calculates conditional value at risk for a probability level p, defined as the mean conditioned to an exceedance above the p-quantile.
  
  Args:
      p (float): Description
      **kwargs: Additional arguments passed to `moments`. This is needed for Bayesian model instances in which a `return_all` parameter can be passed.
  
  Returns:
      float: conditional value at risk
  
  Raises:
      ValueError: Description
  
  &#34;&#34;&#34;
  if not isinstance(p, float) or p &lt; 0 or p &gt;= 1:
    raise ValueError(&#34;p must be in the open interval (0,1)&#34;)

  return (self &gt;= self.ppf(p)).mean(**kwargs)</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.BaseDistribution.histogram"><code class="name flex">
<span>def <span class="ident">histogram</span></span>(<span>self, size:Â intÂ =Â 1000) â€‘>Â None</span>
</code></dt>
<dd>
<div class="desc"><p>Plots a histogram of a simulated sample</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>sample size</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def histogram(self, size: int = 1000) -&gt; None:
  &#34;&#34;&#34;Plots a histogram of a simulated sample
  
  Args:
      size (int, optional): sample size
  
  &#34;&#34;&#34;

  #show histogram from 1k samples
  samples = self.simulate(size=size)
  plt.hist(samples, bins=25, edgecolor=&#34;white&#34;, color=self._figure_color_palette[0])
  plt.title(f&#34;Histogram from {np.round(size/1000,1)}K simulated samples&#34;)
  plt.show()</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.BaseDistribution.mean"><code class="name flex">
<span>def <span class="ident">mean</span></span>(<span>self, **kwargs) â€‘>Â float</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the expected value</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional arguments passed to <code>moments</code>. This is needed for Bayesian model instances in which a <code>return_all</code> parameter can be passed.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>mean value</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mean(self, **kwargs) -&gt; float:
  &#34;&#34;&#34;Calculates the expected value
  
  Args:
      **kwargs: Additional arguments passed to `moments`. This is needed for Bayesian model instances in which a `return_all` parameter can be passed.
  
  Returns:
      float: mean value
  
  &#34;&#34;&#34;
  return self.moment(1, **kwargs)</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.BaseDistribution.moment"><code class="name flex">
<span>def <span class="ident">moment</span></span>(<span>self, n:Â int) â€‘>Â float</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates non-centered moments</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code></dt>
<dd>moment order</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def moment(self, n: int) -&gt; float:
  &#34;&#34;&#34;Calculates non-centered moments
  
  Args:
      n (int): moment order
  &#34;&#34;&#34;
  pass</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.BaseDistribution.pdf"><code class="name flex">
<span>def <span class="ident">pdf</span></span>(<span>self, x:Â float) â€‘>Â float</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the probability mass or probability density function</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>float</code></dt>
<dd>a point in the support</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def pdf(self, x:float) -&gt; float:
  &#34;&#34;&#34;Calculates the probability mass or probability density function
  
  Args:
      x (float): a point in the support
  &#34;&#34;&#34;
  pass</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.BaseDistribution.plot"><code class="name flex">
<span>def <span class="ident">plot</span></span>(<span>self, size:Â intÂ =Â 1000) â€‘>Â None</span>
</code></dt>
<dd>
<div class="desc"><p>Plots a histogram of a simulated sample</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>sample size</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot(self, size: int = 1000) -&gt; None:
  &#34;&#34;&#34;Plots a histogram of a simulated sample
  
  Args:
      size (int, optional): sample size
  
  &#34;&#34;&#34;
  self.histogram(size)</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.BaseDistribution.ppf"><code class="name flex">
<span>def <span class="ident">ppf</span></span>(<span>self, q:Â float) â€‘>Â float</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the corresponding quantile for a given probability value</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>q</code></strong> :&ensp;<code>float</code></dt>
<dd>probability level</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def ppf(self, q: float) -&gt; float:
  &#34;&#34;&#34;Calculates the corresponding quantile for a given probability value
  
  Args:
      q (float): probability level
  &#34;&#34;&#34;
  pass</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.BaseDistribution.simulate"><code class="name flex">
<span>def <span class="ident">simulate</span></span>(<span>self, size:Â int) â€‘>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Produces simulated values from model</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of samples</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def simulate(self, size: int) -&gt; np.ndarray:
  &#34;&#34;&#34;Produces simulated values from model
  
  Args:
      n (int): Number of samples
  &#34;&#34;&#34;
  pass</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.BaseDistribution.std"><code class="name flex">
<span>def <span class="ident">std</span></span>(<span>self, **kwargs) â€‘>Â float</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the standard deviation</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional arguments passed to <code>moments</code>. This is needed for Bayesian model instances in which a <code>return_all</code> parameter can be passed.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>standard deviation value</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def std(self, **kwargs) -&gt; float:
  &#34;&#34;&#34;Calculates the standard deviation
  
  Args:
      **kwargs: Additional arguments passed to `moments`. This is needed for Bayesian model instances in which a `return_all` parameter can be passed.
  
  Returns:
      float: standard deviation value
  
  &#34;&#34;&#34;
  return np.sqrt(self.moment(2, **kwargs) - self.mean(**kwargs)**2)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="riskmodels.univariate.BayesianGPTail"><code class="flex name class">
<span>class <span class="ident">BayesianGPTail</span></span>
<span>(</span><span>**data:Â Any)</span>
</code></dt>
<dd>
<div class="desc"><p>Generalised Pareto tail model which is fitted through Bayesian inference, using uninformative (uniform) priors for the shape and scale parameters</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>threshold</code></strong> :&ensp;<code>float</code></dt>
<dd>modeling threshold</dd>
<dt><strong><code>data</code></strong> :&ensp;<code>np.array</code>, optional</dt>
<dd>exceedance data</dd>
<dt><strong><code>shape</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>sample from posterior shape distribution</dd>
<dt><strong><code>scale</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>sample from posterior scale distribution</dd>
</dl>
<p>Create a new model by parsing and validating input data from keyword arguments.</p>
<p>Raises ValidationError if the input data cannot be parsed to form a valid model.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BayesianGPTail(GPTailMixture):

  &#34;&#34;&#34;Generalised Pareto tail model which is fitted through Bayesian inference, using uninformative (uniform) priors for the shape and scale parameters

  Args:
      threshold (float): modeling threshold
      data (np.array, optional): exceedance data
      shape (np.ndarray): sample from posterior shape distribution
      scale (np.ndarray): sample from posterior scale distribution
  &#34;&#34;&#34;

  #_self.posterior_trace = None

  @classmethod
  def fit(
    cls, 
    data: np.ndarray, 
    threshold: float, 
    max_posterior_samples: int = 1000,
    chain_length: int = 2000,
    x0: np.ndarray = None,
    plot_diagnostics: bool = False,
    n_walkers: int = 32,
    n_cores: int = 4,
    burn_in: int = 100,
    thinning: int = None,
    log_prior: t.Callable = None) -&gt; BayesianGPTail:
    &#34;&#34;&#34;Fits a Generalised Pareto model through Bayesian inference using exceedance data, starting with flat, uninformative priors for both and sampling from posterior shape and scale parameter distributions.
    
    Args:
        data (np.ndarray): observational data
        threshold (float): modeling threshold; location parameter for Generalised Pareto model
        max_posterior_samples (int, optional): Maximum number of posterior samples to keep
        chain_length (int, optional): timesteps in each chain
        x0 (np.ndarray, optional): Starting point for the chains. If None, MLE estimates are used.
        plot_diagnostics (bool, optional): If True, plots MCMC diagnostics in the background.
        n_walkers (int, optional): Number of concurrent paths to use
        n_cores (int, optional): Number of cores to use
        burn_in (int, optional): Number of initial samples to drop
        thinning (int, optional): Thinning factor to reduce autocorrelation; if None, an automatic estimate from emcee&#39;s `get_autocorr_time` is used.
        log_prior (t.Callable, optional): Function that takes as input a single length-2 iterable with scale and shape parameters and outputs the prior log-likelihood. If None, a constant prior on the valid parameter support is used.
    
    Returns:
        BayesianGPTail: fitted model
    
    &#34;&#34;&#34;
    exceedances = data[data &gt; threshold]
    x_max = max(data - threshold)

    # def log_likelihood(theta, data):
    #   scale, shape = theta
    #   return np.sum(gpdist.logpdf(data, c=shape, scale=scale, loc=threshold))

    if log_prior is None:
      def log_prior(theta):
        scale, shape = theta
        if scale &gt; 0 and shape &gt; -scale/(x_max):
          return 0.0
        else:
          return -np.Inf

    def log_probability(theta, data):
      prior = log_prior(theta)
      if np.isfinite(prior):
        ll = prior + GPTail.loglik(theta, threshold, data)#log_likelihood(theta, data)
        #print(theta, ll)
        return ll
      else:
        return -np.Inf

    exceedances = data[data &gt; threshold]
    ndim = 2

    if x0 is None:
      # make initial guess
      mle_model = GPTail.fit(data=exceedances, threshold=threshold)
      shape, scale = mle_model.shape, mle_model.scale
      x0 = np.array([scale, shape])

    # create random walkers
    pos =  x0 + 1e-4 * np.random.randn(n_walkers, ndim)

    with Pool(n_cores) as pool:
      sampler = emcee.EnsembleSampler(nwalkers=n_walkers, ndim=ndim, log_prob_fn=log_probability, args=(exceedances, ))
      sampler.run_mcmc(pos, chain_length, progress=True)

    samples = sampler.get_chain()

    tau = sampler.get_autocorr_time()
    thinning = int(np.round(np.mean(tau)))
    print(f&#34;Using a thinning factor of {thinning} (from emcee.EnsembleSampler.get_autocorr_time)&#34;)
    flat_samples = sampler.get_chain(discard=burn_in, thin=thinning, flat=True)

    if flat_samples.shape[0] &gt; max_posterior_samples:
      np.random.shuffle(flat_samples)
      flat_samples = flat_samples[0:max_posterior_samples,:]

    n_samples = flat_samples.shape[0]
    print(f&#34;Got {n_samples} posterior samples.&#34;)

    if plot_diagnostics:
      fig, axes = plt.subplots(2, figsize=(10, 7), sharex=True)
      labels = [&#34;scale&#34;, &#34;shape&#34;]
      for i in range(ndim):
          ax = axes[i]
          ax.plot(samples[:, :, i], alpha=0.3, color=cls._figure_color_palette[0])
          ax.set_xlim(0, len(samples))
          ax.set_ylabel(labels[i])
          if i == 0:
            ax.set_title(&#34;Chain mixing&#34;)
          ax.yaxis.set_label_coords(-0.1, 0.5)
      print(&#34;Use pyplot.show() to view chain diagnostics.&#34;)


    scale_posterior = flat_samples[:,0]
    shape_posterior = flat_samples[:,1]

    return cls(
      weights = 1/n_samples*np.ones((n_samples,), dtype=np.float64),
      thresholds = threshold*np.ones((n_samples,)),
      data = exceedances,
      shapes = shape_posterior,
      scales = scale_posterior)

  def plot_diagnostics(self) -&gt; matplotlib.figure.Figure:
    &#34;&#34;&#34;Returns a figure with fit diagnostic plots for the GP model
    
    Returns:
        matplotlib.figure.Figure: figure
    
    &#34;&#34;&#34;
    if self.data is None:
      raise ValueError(&#34;Exceedance data was not provided for this model.&#34;)

    def map_to_colors( vals ):
      colours = np.zeros( (len(vals),3) )
      norm = Normalize( vmin=vals.min(), vmax=vals.max() )
      #Can put any colormap you like here.
      colours = [cm.ScalarMappable( norm=norm, cmap=&#39;cool&#39;).to_rgba( val ) for val in vals]
      return colours

    fig, axs = plt.subplots(3, 2)


    #################### Bayesian inference diagnostics: posterior histograms and scatterplot

    axs[0,0].hist(self.scales, bins=25, edgecolor=&#34;white&#34;, color=self._figure_color_palette[0])
    axs[0,0].title.set_text(&#34;Posterior scale histogram&#34;)

    axs[0,1].hist(self.shapes, bins=25, edgecolor=&#34;white&#34;, color=self._figure_color_palette[0])
    axs[0,1].title.set_text(&#34;Posterior shape histogram&#34;)

    posterior = np.concatenate([self.scales.reshape((-1,1)), self.shapes.reshape((-1,1))], axis = 1)
    kernel = kde(posterior.T)

    colours = map_to_colors(kernel.evaluate(posterior.T))

    axs[1,0].scatter(self.scales, self.shapes, color=colours )
    axs[1,0].title.set_text(&#34;Posterior sample&#34;)
    axs[1,0].set_xlabel(&#34;Scale&#34;)
    axs[1,0].set_ylabel(&#34;Shape&#34;)

    ############## histogram vs density ################
    hist_data = axs[1,1].hist(self.data, bins=25, edgecolor=&#34;white&#34;, color=self._figure_color_palette[0])

    range_min, range_max = min(self.data), max(self.data)
    x_axis = np.linspace(range_min, range_max, 100)
    
    mean_pdf_vals = np.array( [self.pdf(x) for x in x_axis])
    # q025_pdf_vals = np.array( [np.quantile(self.pdf(x, return_all=True),0.025) for x in x_axis])
    # q975_pdf_vals = np.array( [np.quantile(self.pdf(x, return_all=True),0.975) for x in x_axis])

    y_axis = hist_data[0][0] / mean_pdf_vals[0] * mean_pdf_vals

    axs[1,1].plot(x_axis, y_axis, color=self._figure_color_palette[1])
    #axs[1,1].fill_between(x_axis, q025_pdf_vals, q975_pdf_vals, alpha=0.2, color=self._figure_color_palette[1])
    axs[1,1].title.set_text(&#34;Data vs fitted density&#34;)
    axs[1,1].set_xlabel(&#39;Exceedance data&#39;)
    axs[1,1].yaxis.set_visible(False) # Hide only x axis
    #axs[0, 0].set_aspect(&#39;equal&#39;, &#39;box&#39;)


    ############# Q-Q plot ################
    probability_range = np.linspace(0.01,0.99, 99)
    empirical_quantiles = np.quantile(self.data, probability_range)
    
    posterior_quantiles = [self.ppf(p, return_all=True) for p in probability_range]

    #hat_return_levels are not the mean of posterior return level samples, as the mean is not an unbiased estimator
    hat_tail_quantiles = np.array([self.ppf(p) for p in probability_range])

    #q025_tail_quantiles = np.array([np.quantile(q, 0.025) for q in posterior_quantiles])
    #q975_tail_quantiles = np.array([np.quantile(q, 0.975) for q in posterior_quantiles])

    axs[2,0].scatter(hat_tail_quantiles, empirical_quantiles, color = self._figure_color_palette[0])
    min_x, max_x = min(hat_tail_quantiles), max(hat_tail_quantiles)
    #axs[0,1].set_aspect(&#39;equal&#39;, &#39;box&#39;)
    axs[2,0].title.set_text(&#39;Q-Q plot&#39;)
    axs[2,0].set_xlabel(&#39;model quantiles&#39;)
    axs[2,0].set_ylabel(&#39;Exceedance quantiles&#39;)
    axs[2,0].grid()
    axs[2,0].plot([min_x,max_x],[min_x,max_x], linestyle=&#34;--&#34;, color=&#34;black&#34;)
    #axs[2,0].fill_between(hat_tail_quantiles, q025_tail_quantiles, q975_tail_quantiles, alpha=0.2, color=self._figure_color_palette[1])

    plt.tight_layout()
    return fig</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="riskmodels.univariate.GPTailMixture" href="#riskmodels.univariate.GPTailMixture">GPTailMixture</a></li>
<li><a title="riskmodels.univariate.BaseDistribution" href="#riskmodels.univariate.BaseDistribution">BaseDistribution</a></li>
<li>pydantic.main.BaseModel</li>
<li>pydantic.utils.Representation</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="riskmodels.univariate.BayesianGPTail.data"><code class="name">var <span class="ident">data</span> :Â Optional[numpy.ndarray]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="riskmodels.univariate.BayesianGPTail.scales"><code class="name">var <span class="ident">scales</span> :Â numpy.ndarray</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="riskmodels.univariate.BayesianGPTail.shapes"><code class="name">var <span class="ident">shapes</span> :Â numpy.ndarray</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="riskmodels.univariate.BayesianGPTail.thresholds"><code class="name">var <span class="ident">thresholds</span> :Â numpy.ndarray</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="riskmodels.univariate.BayesianGPTail.weights"><code class="name">var <span class="ident">weights</span> :Â numpy.ndarray</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="riskmodels.univariate.BayesianGPTail.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>data:Â np.ndarray, threshold:Â float, max_posterior_samples:Â intÂ =Â 1000, chain_length:Â intÂ =Â 2000, x0:Â np.ndarrayÂ =Â None, plot_diagnostics:Â boolÂ =Â False, n_walkers:Â intÂ =Â 32, n_cores:Â intÂ =Â 4, burn_in:Â intÂ =Â 100, thinning:Â intÂ =Â None, log_prior:Â t.CallableÂ =Â None) â€‘>Â <a title="riskmodels.univariate.BayesianGPTail" href="#riskmodels.univariate.BayesianGPTail">BayesianGPTail</a></span>
</code></dt>
<dd>
<div class="desc"><p>Fits a Generalised Pareto model through Bayesian inference using exceedance data, starting with flat, uninformative priors for both and sampling from posterior shape and scale parameter distributions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>observational data</dd>
<dt><strong><code>threshold</code></strong> :&ensp;<code>float</code></dt>
<dd>modeling threshold; location parameter for Generalised Pareto model</dd>
<dt><strong><code>max_posterior_samples</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Maximum number of posterior samples to keep</dd>
<dt><strong><code>chain_length</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>timesteps in each chain</dd>
<dt><strong><code>x0</code></strong> :&ensp;<code>np.ndarray</code>, optional</dt>
<dd>Starting point for the chains. If None, MLE estimates are used.</dd>
<dt><strong><code>plot_diagnostics</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If True, plots MCMC diagnostics in the background.</dd>
<dt><strong><code>n_walkers</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of concurrent paths to use</dd>
<dt><strong><code>n_cores</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of cores to use</dd>
<dt><strong><code>burn_in</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of initial samples to drop</dd>
<dt><strong><code>thinning</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Thinning factor to reduce autocorrelation; if None, an automatic estimate from emcee's <code>get_autocorr_time</code> is used.</dd>
<dt><strong><code>log_prior</code></strong> :&ensp;<code>t.Callable</code>, optional</dt>
<dd>Function that takes as input a single length-2 iterable with scale and shape parameters and outputs the prior log-likelihood. If None, a constant prior on the valid parameter support is used.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="riskmodels.univariate.BayesianGPTail" href="#riskmodels.univariate.BayesianGPTail">BayesianGPTail</a></code></dt>
<dd>fitted model</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def fit(
  cls, 
  data: np.ndarray, 
  threshold: float, 
  max_posterior_samples: int = 1000,
  chain_length: int = 2000,
  x0: np.ndarray = None,
  plot_diagnostics: bool = False,
  n_walkers: int = 32,
  n_cores: int = 4,
  burn_in: int = 100,
  thinning: int = None,
  log_prior: t.Callable = None) -&gt; BayesianGPTail:
  &#34;&#34;&#34;Fits a Generalised Pareto model through Bayesian inference using exceedance data, starting with flat, uninformative priors for both and sampling from posterior shape and scale parameter distributions.
  
  Args:
      data (np.ndarray): observational data
      threshold (float): modeling threshold; location parameter for Generalised Pareto model
      max_posterior_samples (int, optional): Maximum number of posterior samples to keep
      chain_length (int, optional): timesteps in each chain
      x0 (np.ndarray, optional): Starting point for the chains. If None, MLE estimates are used.
      plot_diagnostics (bool, optional): If True, plots MCMC diagnostics in the background.
      n_walkers (int, optional): Number of concurrent paths to use
      n_cores (int, optional): Number of cores to use
      burn_in (int, optional): Number of initial samples to drop
      thinning (int, optional): Thinning factor to reduce autocorrelation; if None, an automatic estimate from emcee&#39;s `get_autocorr_time` is used.
      log_prior (t.Callable, optional): Function that takes as input a single length-2 iterable with scale and shape parameters and outputs the prior log-likelihood. If None, a constant prior on the valid parameter support is used.
  
  Returns:
      BayesianGPTail: fitted model
  
  &#34;&#34;&#34;
  exceedances = data[data &gt; threshold]
  x_max = max(data - threshold)

  # def log_likelihood(theta, data):
  #   scale, shape = theta
  #   return np.sum(gpdist.logpdf(data, c=shape, scale=scale, loc=threshold))

  if log_prior is None:
    def log_prior(theta):
      scale, shape = theta
      if scale &gt; 0 and shape &gt; -scale/(x_max):
        return 0.0
      else:
        return -np.Inf

  def log_probability(theta, data):
    prior = log_prior(theta)
    if np.isfinite(prior):
      ll = prior + GPTail.loglik(theta, threshold, data)#log_likelihood(theta, data)
      #print(theta, ll)
      return ll
    else:
      return -np.Inf

  exceedances = data[data &gt; threshold]
  ndim = 2

  if x0 is None:
    # make initial guess
    mle_model = GPTail.fit(data=exceedances, threshold=threshold)
    shape, scale = mle_model.shape, mle_model.scale
    x0 = np.array([scale, shape])

  # create random walkers
  pos =  x0 + 1e-4 * np.random.randn(n_walkers, ndim)

  with Pool(n_cores) as pool:
    sampler = emcee.EnsembleSampler(nwalkers=n_walkers, ndim=ndim, log_prob_fn=log_probability, args=(exceedances, ))
    sampler.run_mcmc(pos, chain_length, progress=True)

  samples = sampler.get_chain()

  tau = sampler.get_autocorr_time()
  thinning = int(np.round(np.mean(tau)))
  print(f&#34;Using a thinning factor of {thinning} (from emcee.EnsembleSampler.get_autocorr_time)&#34;)
  flat_samples = sampler.get_chain(discard=burn_in, thin=thinning, flat=True)

  if flat_samples.shape[0] &gt; max_posterior_samples:
    np.random.shuffle(flat_samples)
    flat_samples = flat_samples[0:max_posterior_samples,:]

  n_samples = flat_samples.shape[0]
  print(f&#34;Got {n_samples} posterior samples.&#34;)

  if plot_diagnostics:
    fig, axes = plt.subplots(2, figsize=(10, 7), sharex=True)
    labels = [&#34;scale&#34;, &#34;shape&#34;]
    for i in range(ndim):
        ax = axes[i]
        ax.plot(samples[:, :, i], alpha=0.3, color=cls._figure_color_palette[0])
        ax.set_xlim(0, len(samples))
        ax.set_ylabel(labels[i])
        if i == 0:
          ax.set_title(&#34;Chain mixing&#34;)
        ax.yaxis.set_label_coords(-0.1, 0.5)
    print(&#34;Use pyplot.show() to view chain diagnostics.&#34;)


  scale_posterior = flat_samples[:,0]
  shape_posterior = flat_samples[:,1]

  return cls(
    weights = 1/n_samples*np.ones((n_samples,), dtype=np.float64),
    thresholds = threshold*np.ones((n_samples,)),
    data = exceedances,
    shapes = shape_posterior,
    scales = scale_posterior)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="riskmodels.univariate.BayesianGPTail.plot_diagnostics"><code class="name flex">
<span>def <span class="ident">plot_diagnostics</span></span>(<span>self) â€‘>Â matplotlib.figure.Figure</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a figure with fit diagnostic plots for the GP model</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>matplotlib.figure.Figure</code></dt>
<dd>figure</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_diagnostics(self) -&gt; matplotlib.figure.Figure:
  &#34;&#34;&#34;Returns a figure with fit diagnostic plots for the GP model
  
  Returns:
      matplotlib.figure.Figure: figure
  
  &#34;&#34;&#34;
  if self.data is None:
    raise ValueError(&#34;Exceedance data was not provided for this model.&#34;)

  def map_to_colors( vals ):
    colours = np.zeros( (len(vals),3) )
    norm = Normalize( vmin=vals.min(), vmax=vals.max() )
    #Can put any colormap you like here.
    colours = [cm.ScalarMappable( norm=norm, cmap=&#39;cool&#39;).to_rgba( val ) for val in vals]
    return colours

  fig, axs = plt.subplots(3, 2)


  #################### Bayesian inference diagnostics: posterior histograms and scatterplot

  axs[0,0].hist(self.scales, bins=25, edgecolor=&#34;white&#34;, color=self._figure_color_palette[0])
  axs[0,0].title.set_text(&#34;Posterior scale histogram&#34;)

  axs[0,1].hist(self.shapes, bins=25, edgecolor=&#34;white&#34;, color=self._figure_color_palette[0])
  axs[0,1].title.set_text(&#34;Posterior shape histogram&#34;)

  posterior = np.concatenate([self.scales.reshape((-1,1)), self.shapes.reshape((-1,1))], axis = 1)
  kernel = kde(posterior.T)

  colours = map_to_colors(kernel.evaluate(posterior.T))

  axs[1,0].scatter(self.scales, self.shapes, color=colours )
  axs[1,0].title.set_text(&#34;Posterior sample&#34;)
  axs[1,0].set_xlabel(&#34;Scale&#34;)
  axs[1,0].set_ylabel(&#34;Shape&#34;)

  ############## histogram vs density ################
  hist_data = axs[1,1].hist(self.data, bins=25, edgecolor=&#34;white&#34;, color=self._figure_color_palette[0])

  range_min, range_max = min(self.data), max(self.data)
  x_axis = np.linspace(range_min, range_max, 100)
  
  mean_pdf_vals = np.array( [self.pdf(x) for x in x_axis])
  # q025_pdf_vals = np.array( [np.quantile(self.pdf(x, return_all=True),0.025) for x in x_axis])
  # q975_pdf_vals = np.array( [np.quantile(self.pdf(x, return_all=True),0.975) for x in x_axis])

  y_axis = hist_data[0][0] / mean_pdf_vals[0] * mean_pdf_vals

  axs[1,1].plot(x_axis, y_axis, color=self._figure_color_palette[1])
  #axs[1,1].fill_between(x_axis, q025_pdf_vals, q975_pdf_vals, alpha=0.2, color=self._figure_color_palette[1])
  axs[1,1].title.set_text(&#34;Data vs fitted density&#34;)
  axs[1,1].set_xlabel(&#39;Exceedance data&#39;)
  axs[1,1].yaxis.set_visible(False) # Hide only x axis
  #axs[0, 0].set_aspect(&#39;equal&#39;, &#39;box&#39;)


  ############# Q-Q plot ################
  probability_range = np.linspace(0.01,0.99, 99)
  empirical_quantiles = np.quantile(self.data, probability_range)
  
  posterior_quantiles = [self.ppf(p, return_all=True) for p in probability_range]

  #hat_return_levels are not the mean of posterior return level samples, as the mean is not an unbiased estimator
  hat_tail_quantiles = np.array([self.ppf(p) for p in probability_range])

  #q025_tail_quantiles = np.array([np.quantile(q, 0.025) for q in posterior_quantiles])
  #q975_tail_quantiles = np.array([np.quantile(q, 0.975) for q in posterior_quantiles])

  axs[2,0].scatter(hat_tail_quantiles, empirical_quantiles, color = self._figure_color_palette[0])
  min_x, max_x = min(hat_tail_quantiles), max(hat_tail_quantiles)
  #axs[0,1].set_aspect(&#39;equal&#39;, &#39;box&#39;)
  axs[2,0].title.set_text(&#39;Q-Q plot&#39;)
  axs[2,0].set_xlabel(&#39;model quantiles&#39;)
  axs[2,0].set_ylabel(&#39;Exceedance quantiles&#39;)
  axs[2,0].grid()
  axs[2,0].plot([min_x,max_x],[min_x,max_x], linestyle=&#34;--&#34;, color=&#34;black&#34;)
  #axs[2,0].fill_between(hat_tail_quantiles, q025_tail_quantiles, q975_tail_quantiles, alpha=0.2, color=self._figure_color_palette[1])

  plt.tight_layout()
  return fig</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="riskmodels.univariate.GPTailMixture" href="#riskmodels.univariate.GPTailMixture">GPTailMixture</a></b></code>:
<ul class="hlist">
<li><code><a title="riskmodels.univariate.GPTailMixture.cdf" href="#riskmodels.univariate.GPTailMixture.cdf">cdf</a></code></li>
<li><code><a title="riskmodels.univariate.GPTailMixture.cvar" href="#riskmodels.univariate.GPTailMixture.cvar">cvar</a></code></li>
<li><code><a title="riskmodels.univariate.GPTailMixture.histogram" href="#riskmodels.univariate.BaseDistribution.histogram">histogram</a></code></li>
<li><code><a title="riskmodels.univariate.GPTailMixture.mean" href="#riskmodels.univariate.GPTailMixture.mean">mean</a></code></li>
<li><code><a title="riskmodels.univariate.GPTailMixture.moment" href="#riskmodels.univariate.GPTailMixture.moment">moment</a></code></li>
<li><code><a title="riskmodels.univariate.GPTailMixture.pdf" href="#riskmodels.univariate.GPTailMixture.pdf">pdf</a></code></li>
<li><code><a title="riskmodels.univariate.GPTailMixture.plot" href="#riskmodels.univariate.BaseDistribution.plot">plot</a></code></li>
<li><code><a title="riskmodels.univariate.GPTailMixture.ppf" href="#riskmodels.univariate.GPTailMixture.ppf">ppf</a></code></li>
<li><code><a title="riskmodels.univariate.GPTailMixture.simulate" href="#riskmodels.univariate.BaseDistribution.simulate">simulate</a></code></li>
<li><code><a title="riskmodels.univariate.GPTailMixture.std" href="#riskmodels.univariate.GPTailMixture.std">std</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="riskmodels.univariate.Binned"><code class="flex name class">
<span>class <span class="ident">Binned</span></span>
<span>(</span><span>**data:Â Any)</span>
</code></dt>
<dd>
<div class="desc"><p>Empirical distribution with an integer support. This allows it to be convolved with other integer distribution to obtain the distribution of a sum of random variables, assuming independence between the summands. </p>
<p>Create a new model by parsing and validating input data from keyword arguments.</p>
<p>Raises ValidationError if the input data cannot be parsed to form a valid model.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Binned(Empirical):

  &#34;&#34;&#34;Empirical distribution with an integer support. This allows it to be convolved with other integer distribution to obtain the distribution of a sum of random variables, assuming independence between the summands. 
  &#34;&#34;&#34;
  
  _supported_types = [np.int64, int]

  def __repr__(self):
    return f&#34;Integer distribution with support of size {len(self.pdf_values)} ({np.sum(self.pdf_values&gt; 0)} non-zero)&#34;

  @validator(&#34;support&#34;, allow_reuse=True)
  def integer_support(cls, support):
    n = len(support)
    if not np.all(np.diff(support) == 1):
      raise ValueError(&#34;The support vector must contain every integer between its minimum and maximum value&#34;)
    elif support.dtype not in cls._supported_types:
      raise ValueError(f&#34;Support entry types must be one of {self._supported_types}&#34;)
    else:
      return support

  def __mul__(self, factor: self._allowed_scalar_types):

    if not isinstance(factor, self._allowed_scalar_types) or factor == 0:
      raise TypeError(f&#34;multiplication is supported only for nonzero instances of type:{self._allowed_scalar_types}&#34;)

    if isinstance(factor, int):
      return self.from_empirical(float(factor) * self)
      #new_data = None if self.data is None else self.data*factor
      #return Binned(support = factor*self.support, pdf_values = self.pdf_values, data = new_data)

    else:
      return super().__mul__(factor)

  def __add__(self, other: t.Union[float, int, Binned, GPTail, Mixture]):

    if isinstance(other, int):
      new_support = np.arange(min(self.support) + other, max(self.support) + other + 1)
      new_data = None if self.data is None else self.data + other
      return Binned(
        support = new_support, 
        pdf_values = np.copy(self.pdf_values, order=&#34;C&#34;), 
        data = new_data)

    if isinstance(other, Binned):
      new_support = np.arange(min(self.support) + min(other.support), max(self.support) + max(other.support) + 1)

      pdf_vals = fftconvolve(self.pdf_values, other.pdf_values)
      pdf_vals = np.abs(pdf_vals) #some values are negative due to numerical rounding error, set to positive (they are infinitesimal in any case)
      pdf_vals = pdf_vals/np.sum(pdf_vals)

      # this block removes trailing support elements with zero probability
      while pdf_vals[-1] == 0.0:
        new_support = new_support[0:len(pdf_vals)-1]
        pdf_vals = pdf_vals[0:len(pdf_vals)-1]

      # add any missing probability mass
      error = 1.0 - np.sum(pdf_vals)
      pdf_vals[0] += np.sign(error)*np.abs(error)

      return Binned(
        support = new_support, 
        pdf_values = np.abs(pdf_vals), #some negative values persist
        data = None)

    else:
      return super().__add__(other)

  def __sub__(self, other: float):

    if isinstance(other, (int, Binned)):
      return self + (-other)
    else:
      super().__sub__(other)

  def __neg__(self):

    return super().__neg__().to_integer() 

  @classmethod
  def from_data(cls, data: np.ndarray) -&gt; Binned:
    &#34;&#34;&#34;Instantiates a Binned object from observed data
    
    Args:
        data (np.ndarray): Observed data
    
    Returns:
        Binned: Integer distribution object
    &#34;&#34;&#34;
    data = np.array(data)
    if data.dtype not in self._supported_types:
      warnings.warn(&#34;Casting input data to integer values by rounding&#34;, stacklevel=2)
      data = data.astype(np.int64)

    return super().from_data(data).to_integer()

  @classmethod
  def from_empirical(cls, dist: Empirical) -&gt; Binned:
    &#34;&#34;&#34;Takes an Empirical instance with discrete support and creates a Binned instance by casting the support to integer values and filling the gaps in the support
    
    
    Args:
        empirical_dist (Empirical): Empirical instance
    
    Returns:
        Binned: Binned distribution
    
    
    &#34;&#34;&#34;
    empirical_dist = dist.map(lambda x: np.round(x))
    base_support = empirical_dist.support.astype(Binned._supported_types[0])
    full_support = np.arange(min(base_support), max(base_support) + 1)
    
    full_pdf = np.zeros((len(full_support,)), dtype=np.float64)
    indices = base_support - min(base_support)
    full_pdf[indices] = empirical_dist.pdf_values
    # try:
    #   full_pdf[indices] = empirical_dist.pdf_values
    # except Exception as e:
    #   print(f&#34;base support: {base_support}&#34;)
    #   print(f&#34;full support: {full_support}&#34;)

    data = None if empirical_dist.data is None else empirical_dist.data.astype(Binned._supported_types[0])

    return Binned(
      support = full_support.astype(cls._supported_types[0]), 
      pdf_values = full_pdf,
      data = data)

  def pdf(self, x: t.Union[float, np.ndarray], **kwargs):

    if isinstance(x, np.ndarray):
      return np.array([self.pdf(elem) for elem in x])

    if not isinstance(x, (int, np.int32, np.int64)) or x &gt; self.max or x &lt; self.min:
      return 0.0
    else:
      return self.pdf_values[x - self.min]</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="riskmodels.univariate.Empirical" href="#riskmodels.univariate.Empirical">Empirical</a></li>
<li><a title="riskmodels.univariate.BaseDistribution" href="#riskmodels.univariate.BaseDistribution">BaseDistribution</a></li>
<li>pydantic.main.BaseModel</li>
<li>pydantic.utils.Representation</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="riskmodels.powersys.iid.convgen.IndependentFleetModel" href="powersys/iid/convgen.html#riskmodels.powersys.iid.convgen.IndependentFleetModel">IndependentFleetModel</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="riskmodels.univariate.Binned.data"><code class="name">var <span class="ident">data</span> :Â Optional[numpy.ndarray]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="riskmodels.univariate.Binned.pdf_values"><code class="name">var <span class="ident">pdf_values</span> :Â numpy.ndarray</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="riskmodels.univariate.Binned.support"><code class="name">var <span class="ident">support</span> :Â numpy.ndarray</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="riskmodels.univariate.Binned.from_data"><code class="name flex">
<span>def <span class="ident">from_data</span></span>(<span>data:Â np.ndarray) â€‘>Â <a title="riskmodels.univariate.Binned" href="#riskmodels.univariate.Binned">Binned</a></span>
</code></dt>
<dd>
<div class="desc"><p>Instantiates a Binned object from observed data</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Observed data</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="riskmodels.univariate.Binned" href="#riskmodels.univariate.Binned">Binned</a></code></dt>
<dd>Integer distribution object</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def from_data(cls, data: np.ndarray) -&gt; Binned:
  &#34;&#34;&#34;Instantiates a Binned object from observed data
  
  Args:
      data (np.ndarray): Observed data
  
  Returns:
      Binned: Integer distribution object
  &#34;&#34;&#34;
  data = np.array(data)
  if data.dtype not in self._supported_types:
    warnings.warn(&#34;Casting input data to integer values by rounding&#34;, stacklevel=2)
    data = data.astype(np.int64)

  return super().from_data(data).to_integer()</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.Binned.from_empirical"><code class="name flex">
<span>def <span class="ident">from_empirical</span></span>(<span>dist:Â <a title="riskmodels.univariate.Empirical" href="#riskmodels.univariate.Empirical">Empirical</a>) â€‘>Â <a title="riskmodels.univariate.Binned" href="#riskmodels.univariate.Binned">Binned</a></span>
</code></dt>
<dd>
<div class="desc"><p>Takes an Empirical instance with discrete support and creates a Binned instance by casting the support to integer values and filling the gaps in the support</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>empirical_dist</code></strong> :&ensp;<code><a title="riskmodels.univariate.Empirical" href="#riskmodels.univariate.Empirical">Empirical</a></code></dt>
<dd>Empirical instance</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="riskmodels.univariate.Binned" href="#riskmodels.univariate.Binned">Binned</a></code></dt>
<dd>Binned distribution</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def from_empirical(cls, dist: Empirical) -&gt; Binned:
  &#34;&#34;&#34;Takes an Empirical instance with discrete support and creates a Binned instance by casting the support to integer values and filling the gaps in the support
  
  
  Args:
      empirical_dist (Empirical): Empirical instance
  
  Returns:
      Binned: Binned distribution
  
  
  &#34;&#34;&#34;
  empirical_dist = dist.map(lambda x: np.round(x))
  base_support = empirical_dist.support.astype(Binned._supported_types[0])
  full_support = np.arange(min(base_support), max(base_support) + 1)
  
  full_pdf = np.zeros((len(full_support,)), dtype=np.float64)
  indices = base_support - min(base_support)
  full_pdf[indices] = empirical_dist.pdf_values
  # try:
  #   full_pdf[indices] = empirical_dist.pdf_values
  # except Exception as e:
  #   print(f&#34;base support: {base_support}&#34;)
  #   print(f&#34;full support: {full_support}&#34;)

  data = None if empirical_dist.data is None else empirical_dist.data.astype(Binned._supported_types[0])

  return Binned(
    support = full_support.astype(cls._supported_types[0]), 
    pdf_values = full_pdf,
    data = data)</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.Binned.integer_support"><code class="name flex">
<span>def <span class="ident">integer_support</span></span>(<span>support)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@validator(&#34;support&#34;, allow_reuse=True)
def integer_support(cls, support):
  n = len(support)
  if not np.all(np.diff(support) == 1):
    raise ValueError(&#34;The support vector must contain every integer between its minimum and maximum value&#34;)
  elif support.dtype not in cls._supported_types:
    raise ValueError(f&#34;Support entry types must be one of {self._supported_types}&#34;)
  else:
    return support</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="riskmodels.univariate.Empirical" href="#riskmodels.univariate.Empirical">Empirical</a></b></code>:
<ul class="hlist">
<li><code><a title="riskmodels.univariate.Empirical.cdf" href="#riskmodels.univariate.BaseDistribution.cdf">cdf</a></code></li>
<li><code><a title="riskmodels.univariate.Empirical.cdf_values" href="#riskmodels.univariate.Empirical.cdf_values">cdf_values</a></code></li>
<li><code><a title="riskmodels.univariate.Empirical.cvar" href="#riskmodels.univariate.BaseDistribution.cvar">cvar</a></code></li>
<li><code><a title="riskmodels.univariate.Empirical.ecdf" href="#riskmodels.univariate.Empirical.ecdf">ecdf</a></code></li>
<li><code><a title="riskmodels.univariate.Empirical.ecdf_inv" href="#riskmodels.univariate.Empirical.ecdf_inv">ecdf_inv</a></code></li>
<li><code><a title="riskmodels.univariate.Empirical.fit_tail_model" href="#riskmodels.univariate.Empirical.fit_tail_model">fit_tail_model</a></code></li>
<li><code><a title="riskmodels.univariate.Empirical.histogram" href="#riskmodels.univariate.BaseDistribution.histogram">histogram</a></code></li>
<li><code><a title="riskmodels.univariate.Empirical.map" href="#riskmodels.univariate.Empirical.map">map</a></code></li>
<li><code><a title="riskmodels.univariate.Empirical.max" href="#riskmodels.univariate.Empirical.max">max</a></code></li>
<li><code><a title="riskmodels.univariate.Empirical.mean" href="#riskmodels.univariate.BaseDistribution.mean">mean</a></code></li>
<li><code><a title="riskmodels.univariate.Empirical.min" href="#riskmodels.univariate.Empirical.min">min</a></code></li>
<li><code><a title="riskmodels.univariate.Empirical.moment" href="#riskmodels.univariate.Empirical.moment">moment</a></code></li>
<li><code><a title="riskmodels.univariate.Empirical.pdf" href="#riskmodels.univariate.BaseDistribution.pdf">pdf</a></code></li>
<li><code><a title="riskmodels.univariate.Empirical.pdf_lookup" href="#riskmodels.univariate.Empirical.pdf_lookup">pdf_lookup</a></code></li>
<li><code><a title="riskmodels.univariate.Empirical.plot" href="#riskmodels.univariate.BaseDistribution.plot">plot</a></code></li>
<li><code><a title="riskmodels.univariate.Empirical.plot_mean_residual_life" href="#riskmodels.univariate.Empirical.plot_mean_residual_life">plot_mean_residual_life</a></code></li>
<li><code><a title="riskmodels.univariate.Empirical.ppf" href="#riskmodels.univariate.Empirical.ppf">ppf</a></code></li>
<li><code><a title="riskmodels.univariate.Empirical.simulate" href="#riskmodels.univariate.Empirical.simulate">simulate</a></code></li>
<li><code><a title="riskmodels.univariate.Empirical.std" href="#riskmodels.univariate.BaseDistribution.std">std</a></code></li>
<li><code><a title="riskmodels.univariate.Empirical.to_integer" href="#riskmodels.univariate.Empirical.to_integer">to_integer</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="riskmodels.univariate.Empirical"><code class="flex name class">
<span>class <span class="ident">Empirical</span></span>
<span>(</span><span>**data:Â Any)</span>
</code></dt>
<dd>
<div class="desc"><p>Model for an empirical probability distribution, induced by a sample of data.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>support</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>distribution support</dd>
<dt><strong><code>pdf_values</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>pdf array</dd>
<dt><strong><code>data</code></strong> :&ensp;<code>np.array</code>, optional</dt>
<dd>data</dd>
</dl>
<p>Create a new model by parsing and validating input data from keyword arguments.</p>
<p>Raises ValidationError if the input data cannot be parsed to form a valid model.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Empirical(BaseDistribution):

  &#34;&#34;&#34;Model for an empirical probability distribution, induced by a sample of data.

  Args:
      support (np.ndarray): distribution support
      pdf_values (np.ndarray): pdf array
      data (np.array, optional): data
  &#34;&#34;&#34;
  _sum_compatible = (GPTail, Mixture, GPTailMixture)

  support: np.ndarray
  pdf_values: np.ndarray
  data: t.Optional[np.ndarray]

  def __repr__(self):
    if self.data is not None:
      return f&#34;Empirical distribution with {len(self.data)} points&#34;
    else:
      return f&#34;Discrete distribution with support of size {len(self.pdf_values)}&#34;

  @validator(&#34;pdf_values&#34;, allow_reuse=True)
  def check_pdf_values(cls, pdf_values):
    if np.any(pdf_values &lt; -cls._error_tol):
      raise ValueError(&#34;There are negative pdf values&#34;)
    if not np.isclose(np.sum(pdf_values), 1, atol=cls._error_tol):
      print(f&#34;sum: {np.sum(pdf_values)}, pdf vals: {pdf_values}&#34;)
      raise ValueError(&#34;pdf values don&#39;t sum 1&#34;)
    # pdf_values = np.clip(pdf_values, a_min = 0.0, a_max = 1.0)
    # # normalise
    # pdf_values = pdf_values/np.sum(pdf_values)

    return pdf_values

  @property
  def is_valid(self):
    n = len(self.support)
    m = len(self.pdf_values)

    if n != m:
      raise ValueError(&#34;Lengths of support and pdf arrays don&#39;t match&#34;)

    if not np.all(np.diff(self.support) &gt; 0):
      raise ValueError(&#34;Support array must be in increasing order&#34;)

    return True 
  
  @property
  def pdf_lookup(self):
    &#34;&#34;&#34;Mapping from values in the support to their probability mass
    
    &#34;&#34;&#34;
    return {key: val for key, val in zip(self.support, self.pdf_values)}

  @property
  def min(self):
    &#34;&#34;&#34;Minimum value in the support
    &#34;&#34;&#34;
    return self.support[0]

  @property
  def max(self):
    &#34;&#34;&#34;Maximum value in the support
    
    &#34;&#34;&#34;
    return self.support[-1]

  @property
  def cdf_values(self):
    &#34;&#34;&#34;Mapping from values in the support to their cumulative probability
    
    &#34;&#34;&#34;
    x = np.cumsum(self.pdf_values)
    # make sure cdf reaches 1
    x[-1] = 1.0
    return x

  @property
  def ecdf(self):
    &#34;&#34;&#34;Mapping from values in the support to their cumulative probability
    
    &#34;&#34;&#34;
    cdf_vals = np.cumsum(self.pdf_values)
    # make sure cdf reaches 1
    cdf_vals[-1] = 1.0

    return ed.StepFunction(x=self.support, y=cdf_vals, side=&#34;right&#34;)

  @property
  def ecdf_inv(self):
    &#34;&#34;&#34;Linearly interpolated mapping from probability values to their quantiles
    
    &#34;&#34;&#34;
    return ed.monotone_fn_inverter(self.ecdf, self.support)
  
  def __mul__(self, factor: float):

    if not isinstance(factor, self._allowed_scalar_types) or factor == 0:
      raise TypeError(f&#34;multiplication is supported only for nonzero instances of type:{self._allowed_scalar_types}&#34;)

    new_data = None if self.data is None else self.data*factor
    return Empirical(support = factor*self.support, pdf_values = np.copy(self.pdf_values, order=&#34;C&#34;), data = new_data)

  def __add__(self, other: t.Union[int, float, GPTail, Mixture, GPTailMixture]):
    
    if isinstance(other, self._allowed_scalar_types):
      new_data = None if self.data is None else self.data + other
      return Empirical(support = self.support + other, pdf_values = np.copy(self.pdf_values, order=&#34;C&#34;), data = new_data)

    elif isinstance(other, GPTail):

      indices = (self.pdf_values &gt; 0).nonzero()[0]
      nz_pdf = self.pdf_values[indices]
      nz_support = self.support[indices]
      # mixed GPTails don&#39;t carry over exceedance data for efficient memory use
      #dists = [GPTail(threshold=other.threshold + x, scale=other.scale, shape=other.shape) for x in nz_support]

      return GPTailMixture(
        data = other.data,
        weights = nz_pdf,
        thresholds = other.threshold + nz_support,
        scales = np.array([other.scale for w in nz_support]),
        shapes = np.array([other.shape for w in nz_support])
        )

    elif isinstance(other, Mixture):
      return Mixture(weights=other.weights, distributions = [self + dist for dist in other.distributions])

    elif isinstance(other, GPTailMixture):
      # Return a new mixture where the old mixture is replicated for each point in the discrete support
      new_weights = np.concatenate([w*other.weights for w in self.pdf_values], axis = 0)
      new_th = np.concatenate([other.thresholds + t for t in self.support], axis = 0)
      new_scales = np.tile(other.scales, len(self.support))
      new_shapes = np.tile(other.shapes, len(self.support))
      return GPTailMixture(
        data = other.data,
        weights = new_weights[new_weights &gt; 0],
        thresholds = new_th[new_weights &gt; 0],
        scales = new_scales[new_weights &gt; 0],
        shapes = new_shapes[new_weights &gt; 0])

    else:
      raise TypeError(f&#34;+ is supported only for types: {self._sum_compatible}&#34;)

  def __neg__(self):

    return self.map(lambda x: -x)

  def __sub__(self, other: Empirical):

    if isinstance(other, (Empirical,)+ self._allowed_scalar_types):

      return self + (-other)

    else:
      raise TypeError(&#34;Subtraction is only defined for instances of Empirical or float &#34;)

  def __ge__(self, other:float):

    if not isinstance(other, self._allowed_scalar_types):
      raise TypeError(f&#34;&gt;= is implemented for instances of types : {self._allowed_scalar_types}&#34;)

    if 1 - self.cdf(other) + self.pdf(other) == 0.0:
      raise ValueError(f&#34;No probability mass above conditional threshold ({other}).&#34;)

    index = self.support &gt;= other

    new_data = None if self.data is None else self.data[self.data &gt;= other]

    pdf_vals = self.pdf_values[index]/np.sum(self.pdf_values[index])

    return type(self)(
      support = self.support[index],
      pdf_values = pdf_vals, 
      data = new_data)

  def __gt__(self, other:float):

    if not isinstance(other, self._allowed_scalar_types):
      raise TypeError(f&#34;&gt; is implemented for instances of types: {self._allowed_scalar_types}&#34;)

    if 1 - self.cdf(other) == 0:
      raise ValueError(f&#34;No probability mass above conditional threshold ({other}).&#34;)

    index = self.support &gt; other

    new_data = None if self.data is None else self.data[self.data &gt; other]

    return type(self)(
      support = self.support[index],
      pdf_values = self.pdf_values[index]/np.sum(self.pdf_values[index]), 
      data = new_data)


  def simulate(self, size: int) -&gt; np.ndarray:
    &#34;&#34;&#34;Draws simulated values from the distribution
    
    Args:
        size (int): Sample size
    
    Returns:
        np.ndarray: simulated sample
    &#34;&#34;&#34;
    return np.random.choice(self.support, size=size, p=self.pdf_values)

  def moment(self, n: int, **kwargs) -&gt; float:
    &#34;&#34;&#34;Evaluates the n-th moment of the distribution
    
    Args:
        n (int): moment order
        **kwargs: dummy additional arguments (not used)
    
    Returns:
        float: n-th moment value
    &#34;&#34;&#34;
    return np.sum(self.pdf_values * self.support**n)

  def ppf(self, q: t.Union[float, np.ndarray], **kwargs) -&gt; t.Union[float, np.ndarray]:
    &#34;&#34;&#34;Inverse CDF function; it uses linear interpolation.
    
    Args:
        q (t.Union[float, np.ndarray]): probability level
    
    Returns:
        t.Union[float, np.ndarray]: Linearly interpolated quantile function
    
    &#34;&#34;&#34;
    is_scalar = isinstance(q, self._allowed_scalar_types)

    if is_scalar:
      q = np.array([q])

    if np.any(q &lt; 0) or np.any(q &gt;1):
      raise ValueError(f&#34;q needs to be in the interval [0,1]&#34;)

    ppf_values = np.empty((len(q),))

    left_vals_idx = q &lt;= self.ecdf_inv.x[0]
    right_vals_idx = q &gt;= self.ecdf_inv.x[-1]
    inside_vals_idx = np.logical_and(np.logical_not(left_vals_idx), np.logical_not(right_vals_idx))

    ppf_values[left_vals_idx] = self.ecdf_inv.y[0]
    ppf_values[right_vals_idx] = self.ecdf_inv.y[-1]
    ppf_values[inside_vals_idx] = self.ecdf_inv(q[inside_vals_idx])

    if is_scalar:
      return ppf_values[0]
    else:
      return ppf_values

  def cdf(self, x: t.Union[float, np.ndarray], **kwargs) -&gt; t.Union[float, np.ndarray]:
    return self.ecdf(x)

  def pdf(self, x: t.Union[float, np.ndarray], **kwargs):

    if isinstance(x, np.ndarray):
      return np.array([self.pdf(elem) for elem in x])

    try:
      pdf_val = self.pdf_lookup[x]
      return pdf_val
    except KeyError as e:
      return 0.0

  def std(self, **kwargs):
    return np.sqrt(self.map(lambda x: x - self.mean()).moment(2))

  @classmethod
  def from_data(cls, data: np.array):

    support, unnorm_pdf = np.unique(data, return_counts=True)
    n = np.sum(unnorm_pdf)
    return cls(
      support=support, 
      pdf_values=unnorm_pdf/n, 
      data=data)

  def plot_mean_residual_life(self, threshold: float) -&gt; matplotlib.figure.Figure:
    &#34;&#34;&#34;Produces a mean residual life plot for the tail of the distribution above a given threshold
    
    Args:
        threshold (float): threshold value
    
    Returns:
        matplotlib.figure.Figure: figure
    
    
    &#34;&#34;&#34;
    fig = plt.figure()
    fitted = self.fit_tail_model(threshold)
    scale, shape = fitted.tail.scale, fitted.tail.shape

    x_vals = np.linspace(threshold, max(self.data))
    if shape &gt;= 1:
      raise ValueError(f&#34;Expectation is not finite: fitted shape parameter is {shape}&#34;)
    #y_vals = (scale + shape*x_vals)/(1-shape)
    y_vals = np.array([np.mean(fitted.tail.data[fitted.tail.data &gt;= x]) for x in x_vals])
    plt.plot(x_vals,y_vals, color=self._figure_color_palette[0])
    plt.scatter(x_vals, y_vals, color=self._figure_color_palette[0])
    plt.title(&#34;Mean residual life plot&#34;)
    plt.xlabel(&#34;Threshold&#34;)
    plt.ylabel(&#34;Mean exceedance&#34;)
    return fig

  def fit_tail_model(self, threshold: float, bayesian=False, **kwargs) -&gt; t.Union[EmpiricalWithGPTail, EmpiricalWithBayesianGPTail]:
    &#34;&#34;&#34;Fits a tail GP model above a specified threshold and return the fitted semiparametric model
    
    Args:
        threshold (float): Threshold above which a Generalised Pareto distribution will be fitted
        bayesian (bool, optional): If True, fit model through Bayesian inference 
        **kwargs: Additional parameters passed to BayesianGPTail.fit or to GPTail.fit
    
    Returns:
        t.Union[EmpiricalWithGPTail, EmpiricalWithBayesianGPTail]
    
    &#34;&#34;&#34;

    if threshold &gt;= self.max:
      raise ValueError(&#34;Empirical pdf is 0 above the provided threshold. Select a lower threshold for estimation.&#34;)

    if self.data is None:
      raise ValueError(&#34;Data is not set for this distribution, so a tail model cannot be fitted. You can simulate from it and use the sampled data instead&#34;)
    else:
      data = self.data

    if bayesian:
      return EmpiricalWithBayesianGPTail.from_data(data, threshold, bin_empirical=isinstance(self, Binned), **kwargs)
    else:
      return EmpiricalWithGPTail.from_data(data, threshold, bin_empirical=isinstance(self, Binned), **kwargs)

  def map(self, f: t.Callable) -&gt; Empirical:
    &#34;&#34;&#34;Returns the distribution resulting from an arbitrary transformation
    
    Args:
        f (t.Callable): Target transformation; it should take a numpy array as input
    
    &#34;&#34;&#34;
    dist_df = pd.DataFrame({&#34;pdf&#34;: self.pdf_values, &#34;support&#34;: f(self.support)})
    mapped_dist_df = dist_df.groupby(&#34;support&#34;).sum().reset_index().sort_values(&#34;support&#34;)

    return Empirical(
      support = np.array(mapped_dist_df[&#34;support&#34;]),
      pdf_values = np.array(mapped_dist_df[&#34;pdf&#34;]),
      data = f(self.data) if self.data is not None else None)

  def to_integer(self):
    &#34;&#34;&#34;Convert to Binned distribution
    
    &#34;&#34;&#34;
    return Binned.from_empirical(self)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="riskmodels.univariate.BaseDistribution" href="#riskmodels.univariate.BaseDistribution">BaseDistribution</a></li>
<li>pydantic.main.BaseModel</li>
<li>pydantic.utils.Representation</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="riskmodels.univariate.Binned" href="#riskmodels.univariate.Binned">Binned</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="riskmodels.univariate.Empirical.data"><code class="name">var <span class="ident">data</span> :Â Optional[numpy.ndarray]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="riskmodels.univariate.Empirical.pdf_values"><code class="name">var <span class="ident">pdf_values</span> :Â numpy.ndarray</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="riskmodels.univariate.Empirical.support"><code class="name">var <span class="ident">support</span> :Â numpy.ndarray</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="riskmodels.univariate.Empirical.check_pdf_values"><code class="name flex">
<span>def <span class="ident">check_pdf_values</span></span>(<span>pdf_values)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@validator(&#34;pdf_values&#34;, allow_reuse=True)
def check_pdf_values(cls, pdf_values):
  if np.any(pdf_values &lt; -cls._error_tol):
    raise ValueError(&#34;There are negative pdf values&#34;)
  if not np.isclose(np.sum(pdf_values), 1, atol=cls._error_tol):
    print(f&#34;sum: {np.sum(pdf_values)}, pdf vals: {pdf_values}&#34;)
    raise ValueError(&#34;pdf values don&#39;t sum 1&#34;)
  # pdf_values = np.clip(pdf_values, a_min = 0.0, a_max = 1.0)
  # # normalise
  # pdf_values = pdf_values/np.sum(pdf_values)

  return pdf_values</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.Empirical.from_data"><code class="name flex">
<span>def <span class="ident">from_data</span></span>(<span>data:Â np.array)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def from_data(cls, data: np.array):

  support, unnorm_pdf = np.unique(data, return_counts=True)
  n = np.sum(unnorm_pdf)
  return cls(
    support=support, 
    pdf_values=unnorm_pdf/n, 
    data=data)</code></pre>
</details>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="riskmodels.univariate.Empirical.cdf_values"><code class="name">var <span class="ident">cdf_values</span></code></dt>
<dd>
<div class="desc"><p>Mapping from values in the support to their cumulative probability</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def cdf_values(self):
  &#34;&#34;&#34;Mapping from values in the support to their cumulative probability
  
  &#34;&#34;&#34;
  x = np.cumsum(self.pdf_values)
  # make sure cdf reaches 1
  x[-1] = 1.0
  return x</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.Empirical.ecdf"><code class="name">var <span class="ident">ecdf</span></code></dt>
<dd>
<div class="desc"><p>Mapping from values in the support to their cumulative probability</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def ecdf(self):
  &#34;&#34;&#34;Mapping from values in the support to their cumulative probability
  
  &#34;&#34;&#34;
  cdf_vals = np.cumsum(self.pdf_values)
  # make sure cdf reaches 1
  cdf_vals[-1] = 1.0

  return ed.StepFunction(x=self.support, y=cdf_vals, side=&#34;right&#34;)</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.Empirical.ecdf_inv"><code class="name">var <span class="ident">ecdf_inv</span></code></dt>
<dd>
<div class="desc"><p>Linearly interpolated mapping from probability values to their quantiles</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def ecdf_inv(self):
  &#34;&#34;&#34;Linearly interpolated mapping from probability values to their quantiles
  
  &#34;&#34;&#34;
  return ed.monotone_fn_inverter(self.ecdf, self.support)</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.Empirical.is_valid"><code class="name">var <span class="ident">is_valid</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def is_valid(self):
  n = len(self.support)
  m = len(self.pdf_values)

  if n != m:
    raise ValueError(&#34;Lengths of support and pdf arrays don&#39;t match&#34;)

  if not np.all(np.diff(self.support) &gt; 0):
    raise ValueError(&#34;Support array must be in increasing order&#34;)

  return True </code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.Empirical.max"><code class="name">var <span class="ident">max</span></code></dt>
<dd>
<div class="desc"><p>Maximum value in the support</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def max(self):
  &#34;&#34;&#34;Maximum value in the support
  
  &#34;&#34;&#34;
  return self.support[-1]</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.Empirical.min"><code class="name">var <span class="ident">min</span></code></dt>
<dd>
<div class="desc"><p>Minimum value in the support</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def min(self):
  &#34;&#34;&#34;Minimum value in the support
  &#34;&#34;&#34;
  return self.support[0]</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.Empirical.pdf_lookup"><code class="name">var <span class="ident">pdf_lookup</span></code></dt>
<dd>
<div class="desc"><p>Mapping from values in the support to their probability mass</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def pdf_lookup(self):
  &#34;&#34;&#34;Mapping from values in the support to their probability mass
  
  &#34;&#34;&#34;
  return {key: val for key, val in zip(self.support, self.pdf_values)}</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="riskmodels.univariate.Empirical.fit_tail_model"><code class="name flex">
<span>def <span class="ident">fit_tail_model</span></span>(<span>self, threshold:Â float, bayesian=False, **kwargs) â€‘>Â Union[<a title="riskmodels.univariate.EmpiricalWithGPTail" href="#riskmodels.univariate.EmpiricalWithGPTail">EmpiricalWithGPTail</a>,Â <a title="riskmodels.univariate.EmpiricalWithBayesianGPTail" href="#riskmodels.univariate.EmpiricalWithBayesianGPTail">EmpiricalWithBayesianGPTail</a>]</span>
</code></dt>
<dd>
<div class="desc"><p>Fits a tail GP model above a specified threshold and return the fitted semiparametric model</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>threshold</code></strong> :&ensp;<code>float</code></dt>
<dd>Threshold above which a Generalised Pareto distribution will be fitted</dd>
<dt><strong><code>bayesian</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If True, fit model through Bayesian inference </dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional parameters passed to BayesianGPTail.fit or to GPTail.fit</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>t.Union[EmpiricalWithGPTail, EmpiricalWithBayesianGPTail]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit_tail_model(self, threshold: float, bayesian=False, **kwargs) -&gt; t.Union[EmpiricalWithGPTail, EmpiricalWithBayesianGPTail]:
  &#34;&#34;&#34;Fits a tail GP model above a specified threshold and return the fitted semiparametric model
  
  Args:
      threshold (float): Threshold above which a Generalised Pareto distribution will be fitted
      bayesian (bool, optional): If True, fit model through Bayesian inference 
      **kwargs: Additional parameters passed to BayesianGPTail.fit or to GPTail.fit
  
  Returns:
      t.Union[EmpiricalWithGPTail, EmpiricalWithBayesianGPTail]
  
  &#34;&#34;&#34;

  if threshold &gt;= self.max:
    raise ValueError(&#34;Empirical pdf is 0 above the provided threshold. Select a lower threshold for estimation.&#34;)

  if self.data is None:
    raise ValueError(&#34;Data is not set for this distribution, so a tail model cannot be fitted. You can simulate from it and use the sampled data instead&#34;)
  else:
    data = self.data

  if bayesian:
    return EmpiricalWithBayesianGPTail.from_data(data, threshold, bin_empirical=isinstance(self, Binned), **kwargs)
  else:
    return EmpiricalWithGPTail.from_data(data, threshold, bin_empirical=isinstance(self, Binned), **kwargs)</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.Empirical.map"><code class="name flex">
<span>def <span class="ident">map</span></span>(<span>self, f:Â t.Callable) â€‘>Â <a title="riskmodels.univariate.Empirical" href="#riskmodels.univariate.Empirical">Empirical</a></span>
</code></dt>
<dd>
<div class="desc"><p>Returns the distribution resulting from an arbitrary transformation</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong> :&ensp;<code>t.Callable</code></dt>
<dd>Target transformation; it should take a numpy array as input</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def map(self, f: t.Callable) -&gt; Empirical:
  &#34;&#34;&#34;Returns the distribution resulting from an arbitrary transformation
  
  Args:
      f (t.Callable): Target transformation; it should take a numpy array as input
  
  &#34;&#34;&#34;
  dist_df = pd.DataFrame({&#34;pdf&#34;: self.pdf_values, &#34;support&#34;: f(self.support)})
  mapped_dist_df = dist_df.groupby(&#34;support&#34;).sum().reset_index().sort_values(&#34;support&#34;)

  return Empirical(
    support = np.array(mapped_dist_df[&#34;support&#34;]),
    pdf_values = np.array(mapped_dist_df[&#34;pdf&#34;]),
    data = f(self.data) if self.data is not None else None)</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.Empirical.moment"><code class="name flex">
<span>def <span class="ident">moment</span></span>(<span>self, n:Â int, **kwargs) â€‘>Â float</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluates the n-th moment of the distribution</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code></dt>
<dd>moment order</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>dummy additional arguments (not used)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>n-th moment value</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def moment(self, n: int, **kwargs) -&gt; float:
  &#34;&#34;&#34;Evaluates the n-th moment of the distribution
  
  Args:
      n (int): moment order
      **kwargs: dummy additional arguments (not used)
  
  Returns:
      float: n-th moment value
  &#34;&#34;&#34;
  return np.sum(self.pdf_values * self.support**n)</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.Empirical.plot_mean_residual_life"><code class="name flex">
<span>def <span class="ident">plot_mean_residual_life</span></span>(<span>self, threshold:Â float) â€‘>Â matplotlib.figure.Figure</span>
</code></dt>
<dd>
<div class="desc"><p>Produces a mean residual life plot for the tail of the distribution above a given threshold</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>threshold</code></strong> :&ensp;<code>float</code></dt>
<dd>threshold value</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>matplotlib.figure.Figure</code></dt>
<dd>figure</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_mean_residual_life(self, threshold: float) -&gt; matplotlib.figure.Figure:
  &#34;&#34;&#34;Produces a mean residual life plot for the tail of the distribution above a given threshold
  
  Args:
      threshold (float): threshold value
  
  Returns:
      matplotlib.figure.Figure: figure
  
  
  &#34;&#34;&#34;
  fig = plt.figure()
  fitted = self.fit_tail_model(threshold)
  scale, shape = fitted.tail.scale, fitted.tail.shape

  x_vals = np.linspace(threshold, max(self.data))
  if shape &gt;= 1:
    raise ValueError(f&#34;Expectation is not finite: fitted shape parameter is {shape}&#34;)
  #y_vals = (scale + shape*x_vals)/(1-shape)
  y_vals = np.array([np.mean(fitted.tail.data[fitted.tail.data &gt;= x]) for x in x_vals])
  plt.plot(x_vals,y_vals, color=self._figure_color_palette[0])
  plt.scatter(x_vals, y_vals, color=self._figure_color_palette[0])
  plt.title(&#34;Mean residual life plot&#34;)
  plt.xlabel(&#34;Threshold&#34;)
  plt.ylabel(&#34;Mean exceedance&#34;)
  return fig</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.Empirical.ppf"><code class="name flex">
<span>def <span class="ident">ppf</span></span>(<span>self, q:Â t.Union[float,Â np.ndarray], **kwargs) â€‘>Â Union[float,Â numpy.ndarray]</span>
</code></dt>
<dd>
<div class="desc"><p>Inverse CDF function; it uses linear interpolation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>q</code></strong> :&ensp;<code>t.Union[float, np.ndarray]</code></dt>
<dd>probability level</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>t.Union[float, np.ndarray]</code></dt>
<dd>Linearly interpolated quantile function</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ppf(self, q: t.Union[float, np.ndarray], **kwargs) -&gt; t.Union[float, np.ndarray]:
  &#34;&#34;&#34;Inverse CDF function; it uses linear interpolation.
  
  Args:
      q (t.Union[float, np.ndarray]): probability level
  
  Returns:
      t.Union[float, np.ndarray]: Linearly interpolated quantile function
  
  &#34;&#34;&#34;
  is_scalar = isinstance(q, self._allowed_scalar_types)

  if is_scalar:
    q = np.array([q])

  if np.any(q &lt; 0) or np.any(q &gt;1):
    raise ValueError(f&#34;q needs to be in the interval [0,1]&#34;)

  ppf_values = np.empty((len(q),))

  left_vals_idx = q &lt;= self.ecdf_inv.x[0]
  right_vals_idx = q &gt;= self.ecdf_inv.x[-1]
  inside_vals_idx = np.logical_and(np.logical_not(left_vals_idx), np.logical_not(right_vals_idx))

  ppf_values[left_vals_idx] = self.ecdf_inv.y[0]
  ppf_values[right_vals_idx] = self.ecdf_inv.y[-1]
  ppf_values[inside_vals_idx] = self.ecdf_inv(q[inside_vals_idx])

  if is_scalar:
    return ppf_values[0]
  else:
    return ppf_values</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.Empirical.simulate"><code class="name flex">
<span>def <span class="ident">simulate</span></span>(<span>self, size:Â int) â€‘>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Draws simulated values from the distribution</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code></dt>
<dd>Sample size</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>simulated sample</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def simulate(self, size: int) -&gt; np.ndarray:
  &#34;&#34;&#34;Draws simulated values from the distribution
  
  Args:
      size (int): Sample size
  
  Returns:
      np.ndarray: simulated sample
  &#34;&#34;&#34;
  return np.random.choice(self.support, size=size, p=self.pdf_values)</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.Empirical.to_integer"><code class="name flex">
<span>def <span class="ident">to_integer</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Convert to Binned distribution</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_integer(self):
  &#34;&#34;&#34;Convert to Binned distribution
  
  &#34;&#34;&#34;
  return Binned.from_empirical(self)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="riskmodels.univariate.BaseDistribution" href="#riskmodels.univariate.BaseDistribution">BaseDistribution</a></b></code>:
<ul class="hlist">
<li><code><a title="riskmodels.univariate.BaseDistribution.cdf" href="#riskmodels.univariate.BaseDistribution.cdf">cdf</a></code></li>
<li><code><a title="riskmodels.univariate.BaseDistribution.cvar" href="#riskmodels.univariate.BaseDistribution.cvar">cvar</a></code></li>
<li><code><a title="riskmodels.univariate.BaseDistribution.histogram" href="#riskmodels.univariate.BaseDistribution.histogram">histogram</a></code></li>
<li><code><a title="riskmodels.univariate.BaseDistribution.mean" href="#riskmodels.univariate.BaseDistribution.mean">mean</a></code></li>
<li><code><a title="riskmodels.univariate.BaseDistribution.pdf" href="#riskmodels.univariate.BaseDistribution.pdf">pdf</a></code></li>
<li><code><a title="riskmodels.univariate.BaseDistribution.plot" href="#riskmodels.univariate.BaseDistribution.plot">plot</a></code></li>
<li><code><a title="riskmodels.univariate.BaseDistribution.std" href="#riskmodels.univariate.BaseDistribution.std">std</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="riskmodels.univariate.EmpiricalWithBayesianGPTail"><code class="flex name class">
<span>class <span class="ident">EmpiricalWithBayesianGPTail</span></span>
<span>(</span><span>**data:Â Any)</span>
</code></dt>
<dd>
<div class="desc"><p>Semiparametric Bayesian model with an empirical data distribution below a specified threshold and a Generalised Pareto exceedance model above it, fitted through Bayesian inference.</p>
<p>Create a new model by parsing and validating input data from keyword arguments.</p>
<p>Raises ValidationError if the input data cannot be parsed to form a valid model.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class EmpiricalWithBayesianGPTail(EmpiricalWithGPTail):

  &#34;&#34;&#34;Semiparametric Bayesian model with an empirical data distribution below a specified threshold and a Generalised Pareto exceedance model above it, fitted through Bayesian inference.
  &#34;&#34;&#34;
  
  @classmethod
  def from_data(
    cls, 
    data: np.ndarray, 
    threshold: float, 
    bin_empirical: bool = False,
    **kwargs) -&gt; EmpiricalWithBayesianGPTail:
    &#34;&#34;&#34;Fits a Generalied Pareto tail model from a given data array and threshold value, using Jeffrey&#39;s priors 
    
    Args:
        data (np.ndarray): data array 
        threshold (float): Threshold value to use for the tail model
        bin_empirical (bool, optional): Whether to cast empirical mixture component to an integer distribution by rounding
        **kwargs: Additional arguments to be passed to BayesianGPTail.fit
    
    Returns:
        EmpiricalWithBayesianGPTail: Fitted model
    
    Deleted Parameters:
        n_posterior_samples (int): Number of samples from posterior distribution
    
    &#34;&#34;&#34;
    exs_prob = 1 - Empirical.from_data(data).cdf(threshold)

    exceedances = data[data &gt; threshold]

    empirical = Empirical.from_data(data[data &lt;= threshold])
    if bin_empirical:
      empirical = empirical.to_integer()

    tail = BayesianGPTail.fit(data = exceedances, threshold = threshold, **kwargs)

    return cls(
      weights = np.array([1 -exs_prob, exs_prob]),
      distributions = [empirical, tail],
      threshold = threshold)

  def ppf(self, q: t.Union[float, np.ndarray], return_all=False) -&gt; t.Union[float, np.ndarray]:
    &#34;&#34;&#34;Returns the quantile function evaluated at some probability level
    
    Args:
        q (t.Union[float, np.ndarray]): probability level
        return_all (bool, optional): If True, returns posterior ppf sample; otherwise return pointwise estimator
    
    Returns:
        t.Union[float, np.ndarray]: ppf value(s).
    &#34;&#34;&#34;
    if isinstance(q, np.ndarray):
      return np.array([self.ppf(elem) for elem in q])

    if q &lt;= 1 - self.exs_prob:
      val = self.empirical.ppf(q/(1-self.exs_prob), return_all=return_all)
      # if a vector is expected as output, vectorize scalar
      if return_all:
        return val * np.ones((len(self.tail.shapes),))
      else:
        return val
    else:
      return self.tail.ppf((q - (1-self.exs_prob))/self.exs_prob, return_all=return_all)

  def plot_return_levels(self) -&gt; matplotlib.figure.Figure:
    &#34;&#34;&#34;Returns a figure with a return level plot using the fitted tail model
    
    Returns:
        matplotlib.figure.Figure: figure
    
    &#34;&#34;&#34;
    fig = plt.figure()
    exs_prob = self.exs_prob

    exceedance_frequency = 1/np.logspace(1,4,20)
    exceedance_frequency = exceedance_frequency[exceedance_frequency &lt; exs_prob] #plot only levels inside fitted tail model

    return_levels = [self.ppf(1-x, return_all=True) for x in exceedance_frequency]

    #hat_return_levels are not the mean of posterior return level samples, as the mean is not an unbiased estimator
    hat_return_levels = np.array([self.ppf(1-x, return_all=False) for x in exceedance_frequency])
    q025_return_levels = np.array([np.quantile(r, 0.025) for r in return_levels])
    q975_return_levels = np.array([np.quantile(r, 0.975) for r in return_levels])

    plt.plot(1.0/exceedance_frequency,hat_return_levels,color=self._figure_color_palette[0])
    plt.fill_between(1.0/exceedance_frequency, q025_return_levels, q975_return_levels, alpha=0.2, color=self._figure_color_palette[1])
    plt.xscale(&#34;log&#34;)
    plt.title(&#39;Exceedance return levels&#39;)
    plt.xlabel(&#39;1/frequency&#39;)
    plt.ylabel(&#39;Return level&#39;)
    plt.grid()

    return fig</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="riskmodels.univariate.EmpiricalWithGPTail" href="#riskmodels.univariate.EmpiricalWithGPTail">EmpiricalWithGPTail</a></li>
<li><a title="riskmodels.univariate.Mixture" href="#riskmodels.univariate.Mixture">Mixture</a></li>
<li><a title="riskmodels.univariate.BaseDistribution" href="#riskmodels.univariate.BaseDistribution">BaseDistribution</a></li>
<li>pydantic.main.BaseModel</li>
<li>pydantic.utils.Representation</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="riskmodels.univariate.EmpiricalWithBayesianGPTail.threshold"><code class="name">var <span class="ident">threshold</span> :Â float</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="riskmodels.univariate.EmpiricalWithBayesianGPTail.from_data"><code class="name flex">
<span>def <span class="ident">from_data</span></span>(<span>data:Â np.ndarray, threshold:Â float, bin_empirical:Â boolÂ =Â False, **kwargs) â€‘>Â <a title="riskmodels.univariate.EmpiricalWithBayesianGPTail" href="#riskmodels.univariate.EmpiricalWithBayesianGPTail">EmpiricalWithBayesianGPTail</a></span>
</code></dt>
<dd>
<div class="desc"><p>Fits a Generalied Pareto tail model from a given data array and threshold value, using Jeffrey's priors </p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>data array </dd>
<dt><strong><code>threshold</code></strong> :&ensp;<code>float</code></dt>
<dd>Threshold value to use for the tail model</dd>
<dt><strong><code>bin_empirical</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to cast empirical mixture component to an integer distribution by rounding</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional arguments to be passed to BayesianGPTail.fit</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="riskmodels.univariate.EmpiricalWithBayesianGPTail" href="#riskmodels.univariate.EmpiricalWithBayesianGPTail">EmpiricalWithBayesianGPTail</a></code></dt>
<dd>Fitted model</dd>
</dl>
<p>Deleted Parameters:
n_posterior_samples (int): Number of samples from posterior distribution</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def from_data(
  cls, 
  data: np.ndarray, 
  threshold: float, 
  bin_empirical: bool = False,
  **kwargs) -&gt; EmpiricalWithBayesianGPTail:
  &#34;&#34;&#34;Fits a Generalied Pareto tail model from a given data array and threshold value, using Jeffrey&#39;s priors 
  
  Args:
      data (np.ndarray): data array 
      threshold (float): Threshold value to use for the tail model
      bin_empirical (bool, optional): Whether to cast empirical mixture component to an integer distribution by rounding
      **kwargs: Additional arguments to be passed to BayesianGPTail.fit
  
  Returns:
      EmpiricalWithBayesianGPTail: Fitted model
  
  Deleted Parameters:
      n_posterior_samples (int): Number of samples from posterior distribution
  
  &#34;&#34;&#34;
  exs_prob = 1 - Empirical.from_data(data).cdf(threshold)

  exceedances = data[data &gt; threshold]

  empirical = Empirical.from_data(data[data &lt;= threshold])
  if bin_empirical:
    empirical = empirical.to_integer()

  tail = BayesianGPTail.fit(data = exceedances, threshold = threshold, **kwargs)

  return cls(
    weights = np.array([1 -exs_prob, exs_prob]),
    distributions = [empirical, tail],
    threshold = threshold)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="riskmodels.univariate.EmpiricalWithBayesianGPTail.ppf"><code class="name flex">
<span>def <span class="ident">ppf</span></span>(<span>self, q:Â t.Union[float,Â np.ndarray], return_all=False) â€‘>Â Union[float,Â numpy.ndarray]</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the quantile function evaluated at some probability level</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>q</code></strong> :&ensp;<code>t.Union[float, np.ndarray]</code></dt>
<dd>probability level</dd>
<dt><strong><code>return_all</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If True, returns posterior ppf sample; otherwise return pointwise estimator</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>t.Union[float, np.ndarray]</code></dt>
<dd>ppf value(s).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ppf(self, q: t.Union[float, np.ndarray], return_all=False) -&gt; t.Union[float, np.ndarray]:
  &#34;&#34;&#34;Returns the quantile function evaluated at some probability level
  
  Args:
      q (t.Union[float, np.ndarray]): probability level
      return_all (bool, optional): If True, returns posterior ppf sample; otherwise return pointwise estimator
  
  Returns:
      t.Union[float, np.ndarray]: ppf value(s).
  &#34;&#34;&#34;
  if isinstance(q, np.ndarray):
    return np.array([self.ppf(elem) for elem in q])

  if q &lt;= 1 - self.exs_prob:
    val = self.empirical.ppf(q/(1-self.exs_prob), return_all=return_all)
    # if a vector is expected as output, vectorize scalar
    if return_all:
      return val * np.ones((len(self.tail.shapes),))
    else:
      return val
  else:
    return self.tail.ppf((q - (1-self.exs_prob))/self.exs_prob, return_all=return_all)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="riskmodels.univariate.EmpiricalWithGPTail" href="#riskmodels.univariate.EmpiricalWithGPTail">EmpiricalWithGPTail</a></b></code>:
<ul class="hlist">
<li><code><a title="riskmodels.univariate.EmpiricalWithGPTail.cdf" href="#riskmodels.univariate.Mixture.cdf">cdf</a></code></li>
<li><code><a title="riskmodels.univariate.EmpiricalWithGPTail.cvar" href="#riskmodels.univariate.BaseDistribution.cvar">cvar</a></code></li>
<li><code><a title="riskmodels.univariate.EmpiricalWithGPTail.empirical" href="#riskmodels.univariate.EmpiricalWithGPTail.empirical">empirical</a></code></li>
<li><code><a title="riskmodels.univariate.EmpiricalWithGPTail.exs_prob" href="#riskmodels.univariate.EmpiricalWithGPTail.exs_prob">exs_prob</a></code></li>
<li><code><a title="riskmodels.univariate.EmpiricalWithGPTail.histogram" href="#riskmodels.univariate.BaseDistribution.histogram">histogram</a></code></li>
<li><code><a title="riskmodels.univariate.EmpiricalWithGPTail.mean" href="#riskmodels.univariate.Mixture.mean">mean</a></code></li>
<li><code><a title="riskmodels.univariate.EmpiricalWithGPTail.moment" href="#riskmodels.univariate.Mixture.moment">moment</a></code></li>
<li><code><a title="riskmodels.univariate.EmpiricalWithGPTail.pdf" href="#riskmodels.univariate.Mixture.pdf">pdf</a></code></li>
<li><code><a title="riskmodels.univariate.EmpiricalWithGPTail.plot" href="#riskmodels.univariate.BaseDistribution.plot">plot</a></code></li>
<li><code><a title="riskmodels.univariate.EmpiricalWithGPTail.plot_return_levels" href="#riskmodels.univariate.EmpiricalWithGPTail.plot_return_levels">plot_return_levels</a></code></li>
<li><code><a title="riskmodels.univariate.EmpiricalWithGPTail.simulate" href="#riskmodels.univariate.Mixture.simulate">simulate</a></code></li>
<li><code><a title="riskmodels.univariate.EmpiricalWithGPTail.std" href="#riskmodels.univariate.Mixture.std">std</a></code></li>
<li><code><a title="riskmodels.univariate.EmpiricalWithGPTail.tail" href="#riskmodels.univariate.EmpiricalWithGPTail.tail">tail</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="riskmodels.univariate.EmpiricalWithGPTail"><code class="flex name class">
<span>class <span class="ident">EmpiricalWithGPTail</span></span>
<span>(</span><span>**data:Â Any)</span>
</code></dt>
<dd>
<div class="desc"><p>Represents a semiparametric extreme value model with a fitted Generalized Pareto distribution above a certain threshold, and an empirical distribution below it</p>
<p>Create a new model by parsing and validating input data from keyword arguments.</p>
<p>Raises ValidationError if the input data cannot be parsed to form a valid model.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class EmpiricalWithGPTail(Mixture):

  &#34;&#34;&#34;Represents a semiparametric extreme value model with a fitted Generalized Pareto distribution above a certain threshold, and an empirical distribution below it

  &#34;&#34;&#34;
  threshold: float

  def __repr__(self):
    return f&#34;Semiparametric model with generalised Pareto tail. Modeling threshold: {self.threshold}, exceedance probability: {self.exs_prob}&#34;

  @property
  def empirical(self) -&gt; Empirical:
    &#34;&#34;&#34;Empirical distribution below the modeling threshold
    
    Returns:
        Empirical: Distribution object
    &#34;&#34;&#34;
    return self.distributions[0]

  @property
  def tail(self) -&gt; GPTail:
    &#34;&#34;&#34;Generalised Pareto tail model above modeling threshold
    
    Returns:
        GPTail: Distribution
    &#34;&#34;&#34;
    return self.distributions[1]
  
  # @property
  # def threshold(self) -&gt; float:
  #   &#34;&#34;&#34;Modeling threshold
    
  #   Returns:
  #       float: threshold value
  #   &#34;&#34;&#34;
  #   return self.distributions[1].thresholds

  @property
  def exs_prob(self) -&gt; float:
    &#34;&#34;&#34;Probability mass above threshold; probability weight of tail model.
    
    Returns:
        float: weight
    &#34;&#34;&#34;
    return self.weights[1]

  def ppf(self, q: t.Union[float, np.ndarray]) -&gt; t.Union[float, np.ndarray]:

    
    is_scalar = isinstance(q, self._allowed_scalar_types)

    if is_scalar:
      q = np.array([q])

    lower_idx = q &lt;= 1 - self.exs_prob
    higher_idx = np.logical_not(lower_idx)

    ppf_values = np.empty((len(q),))
    ppf_values[lower_idx] = self.empirical.ppf(q[lower_idx]/(1-self.exs_prob))
    ppf_values[higher_idx] = self.tail.ppf((q[higher_idx] - (1-self.exs_prob))/self.exs_prob)
    
    if is_scalar:
      return ppf_values[0]
    else:
      return ppf_values


  @classmethod
  def from_data(cls, data: np.ndarray, threshold: float, bin_empirical: bool = False, **kwargs) -&gt; EmpiricalWithGPTail:
    &#34;&#34;&#34;Fits a model from a given data array and threshold value
    
    Args:
        data (np.ndarray): Data 
        threshold (float): Threshold value to use for the tail model
        bin_empirical (bool, optional): Whether to cast empirical mixture component to an integer distribution by rounding
        **kwargs: Additional arguments passed to GPTail.fit
    
    
    Returns:
        EmpiricalWithGPTail: Fitted model
    &#34;&#34;&#34;
    exs_prob = 1 - Empirical.from_data(data).cdf(threshold)

    exceedances = data[data &gt; threshold]
    
    empirical = Empirical.from_data(data[data &lt;= threshold])
    if bin_empirical:
      empirical = empirical.to_integer()

    tail = GPTail.fit(data=exceedances, threshold=threshold, **kwargs)

    return cls(
      distributions = [empirical, tail],
      weights = np.array([1 - exs_prob, exs_prob]),
      threshold = threshold)

  def plot_diagnostics(self) -&gt; matplotlib.figure.Figure:
    return self.tail.plot_diagnostics()

  def plot_return_levels(self) -&gt; matplotlib.figure.Figure:
    &#34;&#34;&#34;Returns a figure with a return level plot using the fitted tail model
    
    Returns:
        matplotlib.figure.Figure: figure
    
    &#34;&#34;&#34;
    fig = plt.figure()
    scale, shape = self.tail.scale, self.tail.shape
    exs_prob = self.exs_prob
    
    if self.tail.data is None:
      n_obs = np.Inf # this only means that threshold estimation variance is ignored in figure confidence bounds
    else:
      n_obs = len(self.tail.data)

    #exceedance_frequency = 1/np.logspace(1,4,20)
    #exceedance_frequency = exceedance_frequency[exceedance_frequency &lt; exs_prob] #plot only levels inside fitted tail model
    # shown return levels go from largest power of 10th below exceedance prob, to 1/10000-th of that.

    x_min = np.floor(np.log(exs_prob)/np.log(10))
    x_max = x_min - 4
    exceedance_frequency = 10**(np.linspace(x_min, x_max, 50))
    return_levels = self.ppf(1 - exceedance_frequency)

    plt.plot(1.0/exceedance_frequency,return_levels,color=self._figure_color_palette[0])
    plt.xscale(&#34;log&#34;)
    plt.title(&#34; Return levels&#34;)
    plt.xlabel(&#39;Return period&#39;)
    plt.ylabel(&#39;Return level&#39;)
    plt.grid()

    try:
      #for this bit, look at An Introduction to Statistical selfing of Extreme Values, p.82
      mle_cov = self.tail.mle_cov()
      eigenvals, eigenvecs = np.linalg.eig(mle_cov)
      if np.all(eigenvals &gt; 0):
        covariance = np.eye(3)
        covariance[1::,1::] = mle_cov
        covariance[0,0] = exs_prob*(1-exs_prob)/n_obs
        #
        return_stdevs = []
        for m in 1.0/exceedance_frequency:
          quantile_grad = np.array([
            scale*m*(m*exs_prob)**(shape-1),
            1/shape*((exs_prob*m)**shape-1),
            -scale/shape**2*((exs_prob*m)**shape-1)+scale/shape*(exs_prob*m)**shape*np.log(exs_prob*m)
            ])
          #
          sdev = np.sqrt(quantile_grad.T.dot(covariance).dot(quantile_grad))
          return_stdevs.append(sdev)
        #
        return_stdevs = np.array(return_stdevs)
        plt.fill_between(1.0/exceedance_frequency, return_levels - 1.96*return_stdevs, return_levels + 1.96*return_stdevs, alpha=0.2, color=self._figure_color_palette[1])
      else:
        warnings.warn(&#34;Covariance MLE matrix is not positive definite; it might be ill-conditioned&#34;, stacklevel=2)
    except Exception as e:
      warnings.warn(f&#34;Confidence bands for return level could not be calculated; covariance matrix might be ill-conditioned; full trace: {traceback.format_exc()}&#34;, stacklevel=2)

    return fig</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="riskmodels.univariate.Mixture" href="#riskmodels.univariate.Mixture">Mixture</a></li>
<li><a title="riskmodels.univariate.BaseDistribution" href="#riskmodels.univariate.BaseDistribution">BaseDistribution</a></li>
<li>pydantic.main.BaseModel</li>
<li>pydantic.utils.Representation</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="riskmodels.univariate.EmpiricalWithBayesianGPTail" href="#riskmodels.univariate.EmpiricalWithBayesianGPTail">EmpiricalWithBayesianGPTail</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="riskmodels.univariate.EmpiricalWithGPTail.threshold"><code class="name">var <span class="ident">threshold</span> :Â float</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="riskmodels.univariate.EmpiricalWithGPTail.from_data"><code class="name flex">
<span>def <span class="ident">from_data</span></span>(<span>data:Â np.ndarray, threshold:Â float, bin_empirical:Â boolÂ =Â False, **kwargs) â€‘>Â <a title="riskmodels.univariate.EmpiricalWithGPTail" href="#riskmodels.univariate.EmpiricalWithGPTail">EmpiricalWithGPTail</a></span>
</code></dt>
<dd>
<div class="desc"><p>Fits a model from a given data array and threshold value</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Data </dd>
<dt><strong><code>threshold</code></strong> :&ensp;<code>float</code></dt>
<dd>Threshold value to use for the tail model</dd>
<dt><strong><code>bin_empirical</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to cast empirical mixture component to an integer distribution by rounding</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional arguments passed to GPTail.fit</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="riskmodels.univariate.EmpiricalWithGPTail" href="#riskmodels.univariate.EmpiricalWithGPTail">EmpiricalWithGPTail</a></code></dt>
<dd>Fitted model</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def from_data(cls, data: np.ndarray, threshold: float, bin_empirical: bool = False, **kwargs) -&gt; EmpiricalWithGPTail:
  &#34;&#34;&#34;Fits a model from a given data array and threshold value
  
  Args:
      data (np.ndarray): Data 
      threshold (float): Threshold value to use for the tail model
      bin_empirical (bool, optional): Whether to cast empirical mixture component to an integer distribution by rounding
      **kwargs: Additional arguments passed to GPTail.fit
  
  
  Returns:
      EmpiricalWithGPTail: Fitted model
  &#34;&#34;&#34;
  exs_prob = 1 - Empirical.from_data(data).cdf(threshold)

  exceedances = data[data &gt; threshold]
  
  empirical = Empirical.from_data(data[data &lt;= threshold])
  if bin_empirical:
    empirical = empirical.to_integer()

  tail = GPTail.fit(data=exceedances, threshold=threshold, **kwargs)

  return cls(
    distributions = [empirical, tail],
    weights = np.array([1 - exs_prob, exs_prob]),
    threshold = threshold)</code></pre>
</details>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="riskmodels.univariate.EmpiricalWithGPTail.empirical"><code class="name">var <span class="ident">empirical</span> :Â <a title="riskmodels.univariate.Empirical" href="#riskmodels.univariate.Empirical">Empirical</a></code></dt>
<dd>
<div class="desc"><p>Empirical distribution below the modeling threshold</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="riskmodels.univariate.Empirical" href="#riskmodels.univariate.Empirical">Empirical</a></code></dt>
<dd>Distribution object</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def empirical(self) -&gt; Empirical:
  &#34;&#34;&#34;Empirical distribution below the modeling threshold
  
  Returns:
      Empirical: Distribution object
  &#34;&#34;&#34;
  return self.distributions[0]</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.EmpiricalWithGPTail.exs_prob"><code class="name">var <span class="ident">exs_prob</span> :Â float</code></dt>
<dd>
<div class="desc"><p>Probability mass above threshold; probability weight of tail model.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>weight</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def exs_prob(self) -&gt; float:
  &#34;&#34;&#34;Probability mass above threshold; probability weight of tail model.
  
  Returns:
      float: weight
  &#34;&#34;&#34;
  return self.weights[1]</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.EmpiricalWithGPTail.tail"><code class="name">var <span class="ident">tail</span> :Â <a title="riskmodels.univariate.GPTail" href="#riskmodels.univariate.GPTail">GPTail</a></code></dt>
<dd>
<div class="desc"><p>Generalised Pareto tail model above modeling threshold</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="riskmodels.univariate.GPTail" href="#riskmodels.univariate.GPTail">GPTail</a></code></dt>
<dd>Distribution</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def tail(self) -&gt; GPTail:
  &#34;&#34;&#34;Generalised Pareto tail model above modeling threshold
  
  Returns:
      GPTail: Distribution
  &#34;&#34;&#34;
  return self.distributions[1]</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="riskmodels.univariate.EmpiricalWithGPTail.plot_diagnostics"><code class="name flex">
<span>def <span class="ident">plot_diagnostics</span></span>(<span>self) â€‘>Â matplotlib.figure.Figure</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_diagnostics(self) -&gt; matplotlib.figure.Figure:
  return self.tail.plot_diagnostics()</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.EmpiricalWithGPTail.plot_return_levels"><code class="name flex">
<span>def <span class="ident">plot_return_levels</span></span>(<span>self) â€‘>Â matplotlib.figure.Figure</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a figure with a return level plot using the fitted tail model</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>matplotlib.figure.Figure</code></dt>
<dd>figure</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_return_levels(self) -&gt; matplotlib.figure.Figure:
  &#34;&#34;&#34;Returns a figure with a return level plot using the fitted tail model
  
  Returns:
      matplotlib.figure.Figure: figure
  
  &#34;&#34;&#34;
  fig = plt.figure()
  scale, shape = self.tail.scale, self.tail.shape
  exs_prob = self.exs_prob
  
  if self.tail.data is None:
    n_obs = np.Inf # this only means that threshold estimation variance is ignored in figure confidence bounds
  else:
    n_obs = len(self.tail.data)

  #exceedance_frequency = 1/np.logspace(1,4,20)
  #exceedance_frequency = exceedance_frequency[exceedance_frequency &lt; exs_prob] #plot only levels inside fitted tail model
  # shown return levels go from largest power of 10th below exceedance prob, to 1/10000-th of that.

  x_min = np.floor(np.log(exs_prob)/np.log(10))
  x_max = x_min - 4
  exceedance_frequency = 10**(np.linspace(x_min, x_max, 50))
  return_levels = self.ppf(1 - exceedance_frequency)

  plt.plot(1.0/exceedance_frequency,return_levels,color=self._figure_color_palette[0])
  plt.xscale(&#34;log&#34;)
  plt.title(&#34; Return levels&#34;)
  plt.xlabel(&#39;Return period&#39;)
  plt.ylabel(&#39;Return level&#39;)
  plt.grid()

  try:
    #for this bit, look at An Introduction to Statistical selfing of Extreme Values, p.82
    mle_cov = self.tail.mle_cov()
    eigenvals, eigenvecs = np.linalg.eig(mle_cov)
    if np.all(eigenvals &gt; 0):
      covariance = np.eye(3)
      covariance[1::,1::] = mle_cov
      covariance[0,0] = exs_prob*(1-exs_prob)/n_obs
      #
      return_stdevs = []
      for m in 1.0/exceedance_frequency:
        quantile_grad = np.array([
          scale*m*(m*exs_prob)**(shape-1),
          1/shape*((exs_prob*m)**shape-1),
          -scale/shape**2*((exs_prob*m)**shape-1)+scale/shape*(exs_prob*m)**shape*np.log(exs_prob*m)
          ])
        #
        sdev = np.sqrt(quantile_grad.T.dot(covariance).dot(quantile_grad))
        return_stdevs.append(sdev)
      #
      return_stdevs = np.array(return_stdevs)
      plt.fill_between(1.0/exceedance_frequency, return_levels - 1.96*return_stdevs, return_levels + 1.96*return_stdevs, alpha=0.2, color=self._figure_color_palette[1])
    else:
      warnings.warn(&#34;Covariance MLE matrix is not positive definite; it might be ill-conditioned&#34;, stacklevel=2)
  except Exception as e:
    warnings.warn(f&#34;Confidence bands for return level could not be calculated; covariance matrix might be ill-conditioned; full trace: {traceback.format_exc()}&#34;, stacklevel=2)

  return fig</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="riskmodels.univariate.Mixture" href="#riskmodels.univariate.Mixture">Mixture</a></b></code>:
<ul class="hlist">
<li><code><a title="riskmodels.univariate.Mixture.cdf" href="#riskmodels.univariate.Mixture.cdf">cdf</a></code></li>
<li><code><a title="riskmodels.univariate.Mixture.cvar" href="#riskmodels.univariate.BaseDistribution.cvar">cvar</a></code></li>
<li><code><a title="riskmodels.univariate.Mixture.histogram" href="#riskmodels.univariate.BaseDistribution.histogram">histogram</a></code></li>
<li><code><a title="riskmodels.univariate.Mixture.mean" href="#riskmodels.univariate.Mixture.mean">mean</a></code></li>
<li><code><a title="riskmodels.univariate.Mixture.moment" href="#riskmodels.univariate.Mixture.moment">moment</a></code></li>
<li><code><a title="riskmodels.univariate.Mixture.pdf" href="#riskmodels.univariate.Mixture.pdf">pdf</a></code></li>
<li><code><a title="riskmodels.univariate.Mixture.plot" href="#riskmodels.univariate.BaseDistribution.plot">plot</a></code></li>
<li><code><a title="riskmodels.univariate.Mixture.ppf" href="#riskmodels.univariate.Mixture.ppf">ppf</a></code></li>
<li><code><a title="riskmodels.univariate.Mixture.simulate" href="#riskmodels.univariate.Mixture.simulate">simulate</a></code></li>
<li><code><a title="riskmodels.univariate.Mixture.std" href="#riskmodels.univariate.Mixture.std">std</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="riskmodels.univariate.GPTail"><code class="flex name class">
<span>class <span class="ident">GPTail</span></span>
<span>(</span><span>**data:Â Any)</span>
</code></dt>
<dd>
<div class="desc"><p>Representation of a fitted Generalized Pareto distribution as an exceedance model. It's density is given by</p>
<p><span><span class="MathJax_Preview"> f(x) = \frac{1}{\sigma} \left( 1 + \xi \left( \frac{x - \mu}{\sigma} \right) \right)_{+}^{-(1 + 1/\xi)} </span><script type="math/tex; mode=display"> f(x) = \frac{1}{\sigma} \left( 1 + \xi \left( \frac{x - \mu}{\sigma} \right) \right)_{+}^{-(1 + 1/\xi)} </script></span></p>
<p>where <span><span class="MathJax_Preview"> \mu, \sigma, \xi </span><script type="math/tex"> \mu, \sigma, \xi </script></span> are the location, scale and shape parameters, and <span><span class="MathJax_Preview"> (\cdot)_{+} = \max(\cdot, 0)</span><script type="math/tex"> (\cdot)_{+} = \max(\cdot, 0)</script></span>. The location parameter is also the lower endpoint (or threshold) of the distribution.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>threshold</code></strong> :&ensp;<code>float</code></dt>
<dd>modeling threshold</dd>
<dt><strong><code>shape</code></strong> :&ensp;<code>float</code></dt>
<dd>fitted shape parameter</dd>
<dt><strong><code>scale</code></strong> :&ensp;<code>float</code></dt>
<dd>fitted scale parameter</dd>
<dt><strong><code>data</code></strong> :&ensp;<code>np.array</code>, optional</dt>
<dd>exceedance data</dd>
</dl>
<p>Create a new model by parsing and validating input data from keyword arguments.</p>
<p>Raises ValidationError if the input data cannot be parsed to form a valid model.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class GPTail(BaseDistribution):
  &#34;&#34;&#34;Representation of a fitted Generalized Pareto distribution as an exceedance model. It&#39;s density is given by

  $$ f(x) = \\frac{1}{\\sigma} \\left( 1 + \\xi \\left( \\frac{x - \\mu}{\\sigma} \\right) \\right)_{+}^{-(1 + 1/\\xi)} $$
  
  where \\( \\mu, \\sigma, \\xi \\) are the location, scale and shape parameters, and \\( (\\cdot)_{+} = \\max(\\cdot, 0)\\). The location parameter is also the lower endpoint (or threshold) of the distribution.

  Args:
      threshold (float): modeling threshold
      shape (float): fitted shape parameter
      scale (float): fitted scale parameter
      data (np.array, optional): exceedance data
  &#34;&#34;&#34;

  threshold: float
  shape: float
  scale: PositiveFloat
  data: t.Optional[np.ndarray]

  def __repr__(self):
    return f&#34;Generalised Pareto tail model with (mu, scale, shape) = ({self.threshold},{self.scale},{self.shape}) components&#34;

  @property
  def endpoint(self):
    return self.threshold - self.scale/self.shape if self.shape &lt; 0 else np.Inf
  

  @property
  def model(self):
    return gpdist(loc=self.threshold, c=self.shape, scale=self.scale)

  # @classmethod
  # def fit(cls, data: np.array, threshold: float) -&gt; GPTail:
  #   exceedances = data[data &gt; threshold]
  #   shape, _, scale = gpdist.fit(exceedances, floc=threshold)
  #   return cls(
  #     threshold = threshold,
  #     shape = shape,
  #     scale = scale,
  #     data = exceedances)

  def __add__(self, other: float):

    if not isinstance(other, self._allowed_scalar_types):
      raise TypeError(f&#34;+ is implemented for instances of types: {self._allowed_scalar_types}&#34;)

    return GPTail(
      threshold = self.threshold + other,
      shape = self.shape,
      scale = self.scale,
      data = self.data + other if self.data is not None else None)

  def __ge__(self, other: float):

    if not isinstance(other, self._allowed_scalar_types):
      raise TypeError(f&#34;&gt;= and &gt; are implemented for instances of types: {self._allowed_scalar_types}&#34;)

    if other &gt;= self.endpoint:
      raise ValueError(f&#34;No probability mass above endpoint ({self.endpoint}); conditional distribution X | X &gt;= {other} does not exist&#34;)

    if other &lt; self.threshold:
      return self &gt;= self.threshold
    else:
      # condition on empirical data if applicable
      if self.data is None or max(self.data) &lt; other:
        new_data = None
        warnings.warn(f&#34;No observed data above {other}; setting data to None in conditional model.&#34;, stacklevel=2)
      else:
        new_data = self.data[self.data &gt;= other]
      
      return GPTail(
        threshold=other,
        shape = self.shape,
        scale = self.scale + self.shape*(other - self.threshold),
        data = new_data)

  def __gt__(self, other: float) -&gt; GPTail:

    return self.__ge__(other)

  def __mul__(self, other: float) -&gt; GPTail:

    if not isinstance(other, self._allowed_scalar_types) or other &lt;= 0:
      raise TypeError(f&#34;* is implemented for positive instances of: {self._allowed_scalar_types}&#34;)

    new_data = other*self.data if self.data is not None else None

    return GPTail(
      threshold= other*self.threshold,
      shape = self.shape,
      scale = other*self.scale,
      data = new_data)

  def simulate(self, size: int) -&gt; np.ndarray:
    return self.model.rvs(size=size)

  def moment(self, n: int, **kwargs) -&gt; float:
    return self.model.moment(n)

  def ppf(self, q: t.Union[float, np.ndarray], **kwargs) -&gt; t.Union[float, np.ndarray]:
    return self.model.ppf(q)

  def cdf(self, x:t.Union[float, np.ndarray], **kwargs) -&gt; t.Union[float, np.ndarray]:
    return self.model.cdf(x)

  def pdf(self, x:t.Union[float, np.ndarray], **kwargs) -&gt; t.Union[float, np.ndarray]:
    return self.model.pdf(x)

  def std(self, **kwargs):
    return self.model.std()

  def mle_cov(self) -&gt; np.ndarray:
    &#34;&#34;&#34;Returns the estimated parameter covariance matrix evaluated at the fitted parameters
    
    Returns:
        np.ndarray: Covariance matrix
    &#34;&#34;&#34;

    if self.data is None:
      raise ValueError(&#34;exceedance data not provided for this instance of GPTail; covariance matrix can&#39;t be estimated&#34;)
    else:
      hess = self.loglik_hessian([self.scale,self.shape], threshold=self.threshold, data=self.data)
      return np.linalg.inv(-hess)

  @classmethod
  def loglik(cls, params: t.List[float], threshold: float, data: np.ndarray) -&gt; float:
    &#34;&#34;&#34;Returns the negative log-likelihood for a Generalised Pareto model
    
    Args:
        *params (t.List[float]): Vector parameter with scale and shape values, in that order
        threshold (float): model threshold
        data (np.ndarray): exceedance data
    
    Returns:
        float
    &#34;&#34;&#34;
    scale, shape = params
    return np.sum(gpdist.logpdf(data, loc=threshold, c=shape, scale=scale))

  @classmethod
  def loglik_grad(cls, params: t.List[float], threshold: float, data: np.ndarray) -&gt; np.ndarray:
    &#34;&#34;&#34;Returns the gradient of the negative log-likelihood for a Generalised Pareto model
    
    Args:
        *params (t.List[float]): Vector parameter with scale and shape values, in that order
        threshold (float): model threshold
        data (np.ndarray): exceedance data
    
    Returns:
        np.ndarray: gradient array
    &#34;&#34;&#34;
    scale, shape = params
    y = data - threshold

    if not np.isclose(shape, 0, atol=cls._error_tol):
      grad_scale = np.sum((y - scale)/(scale*(y*shape + scale)))
      grad_shape = np.sum(-((y*(1 + shape))/(shape*(y*shape + scale))) + np.log(1 + (y*shape)/scale)/shape**2)
    else:
      grad_scale = np.sum((y-scale)/scale**2)
      grad_shape = np.sum(y*(y-2*scale)/(2*scale**2))

    return np.array([grad_scale,grad_shape])


  @classmethod
  def loglik_hessian(cls, params: t.List[float], threshold: float, data: np.ndarray) -&gt; np.ndarray:
    &#34;&#34;&#34;Returns the Hessian matrix of the negative log-likelihood for a Generalised Pareto model
    
    Args:
        *params (t.List[float]): Vector parameter with scale and shape values, in that order
        threshold (float): model threshold
        data (np.ndarray): exceedance data above the threshold
    
    Returns:
        np.ndarray: hessian matrix array
    &#34;&#34;&#34;
    scale, shape = params
    y = data - threshold

    if not np.isclose(shape, 0, atol=cls._error_tol):
      d2scale = np.sum(-(1/(shape*scale**2)) + (1 + shape)/(shape*(y*shape + scale)**2))

      #d2shape = (y (3 y Î¾ + y Î¾^2 + 2 Ïƒ))/(Î¾^2 (y Î¾ + Ïƒ)^2) - (2 Log[1 + (y Î¾)/Ïƒ])/Î¾^3
      d2shape = np.sum((y*(3*y*shape + y*shape**2 + 2*scale))/(shape**2*(y*shape + scale)**2) - (2*np.log(1 + (y*shape)/scale))/shape**3)

      #dscale_dshape = (y (-y + Ïƒ))/(Ïƒ (y Î¾ + Ïƒ)^2)
      dscale_dshape = np.sum((y*(-y + scale))/(scale*(y*shape + scale)**2))
    else:
      d2scale = np.sum((scale-2*y)/scale**3)
      dscale_dshape = np.sum(-y*(y-scale)/scale**3)
      d2shape = np.sum(y**2*(3*scale-2*y)/(3*scale**3))

    hessian = np.array([[d2scale,dscale_dshape],[dscale_dshape,d2shape]])

    return hessian

  @classmethod
  def logreg(cls, params: t.List[float]) -&gt; float:
    &#34;&#34;&#34;The regularisation terms are inspired in uninformative and zero-mean Gaussian priors for the scale and shape respectively, thus it is given by

    $$r(\\sigma, \\xi) = 0.5 \\cdot \\log(\\sigma) - 0.5 \\xi^2$$
    
    Args:
        params (t.List[float]): scale and shape parameters in that order
    
    Returns:
        float: Description
    &#34;&#34;&#34;
    scale, shape = params

    return 0.5*(np.log(scale) + shape**2)

  @classmethod
  def logreg_grad(cls, params: t.List[float]) -&gt; float:
    &#34;&#34;&#34;Returns the gradient of the regularisation term
    
    Args:
        params (t.List[float]): scale and shape parameters in that order

    &#34;&#34;&#34;
    scale, shape = params
    return 0.5*np.array([1.0/scale, 2*shape])

  @classmethod
  def logreg_hessian(cls, params: t.List[float]) -&gt; float:
    &#34;&#34;&#34;Returns the Hessian of the regularisation term
    
    Args:
        params (t.List[float]): scale and shape parameters in that order

    &#34;&#34;&#34;
    scale, shape = params
    return 0.5*np.array([-1.0/scale**2,0,0,2]).reshape((2,2))

  @classmethod
  def loss(cls, params: t.List[float], threshold: float, data: np.ndarray) -&gt; np.ndarray:
    &#34;&#34;&#34;Calculate the loss function on the provided parameters; this is the sum of the (negative) data log-likelihood and (negative) regularisation terms for the scale and shape. Everything is divided by the number of data points.
    
    Args:
        params (t.List[float]): Description
        threshold (float): Model threshold; eequivalently, location parameter
        data (np.ndarray): exceedance data above threshold
    
    &#34;&#34;&#34;
    scale, shape = params
    n = len(data)
    unnorm = cls.loglik(params, threshold, data) + cls.logreg(params)
    return -unnorm/n

  @classmethod
  def loss_grad(cls, params: t.List[float], threshold: float, data: np.ndarray) -&gt; np.ndarray:
    &#34;&#34;&#34;Calculate the loss function&#39;s gradient on the provided parameters
    
    Args:
        params (t.List[float]): Description
        threshold (float): Model threshold; eequivalently, location parameter
        data (np.ndarray): exceedance data above threshold
    
    &#34;&#34;&#34;
    n = len(data)
    return -(cls.loglik_grad(params, threshold, data) + cls.logreg_grad(params))/n

  @classmethod
  def loss_hessian(cls, params: t.List[float], threshold: float, data: np.ndarray) -&gt; np.ndarray:
    &#34;&#34;&#34;Calculate the loss function&#39;s Hessian on the provided parameters
    
    Args:
        params (t.List[float]): Description
        threshold (float): Model threshold; eequivalently, location parameter
        data (np.ndarray): exceedance data above threshold
    
    &#34;&#34;&#34;
    n = len(data)
    return -(cls.loglik_hessian(params, threshold, data) + cls.logreg_hessian(params))/n

  @classmethod
  def fit(
    cls, 
    data: np.ndarray, 
    threshold: float, 
    x0: np.ndarray = None,
    return_opt_results=False) -&gt; t.Union[GPTail, sp.optimize.OptimizeResult]:
    &#34;&#34;&#34;Fits a eneralised Pareto tail model using a constrained trust region method
    
    Args:
        data (np.ndarray): exceedance data above threshold
        threshold (float): Model threshold
        x0 (np.ndarray, optional): Initial guess for optimization; if None, the result of scipy.stats.genpareto.fit is used as a starting point.
        return_opt_results (bool, optional): If True, return the OptimizeResult object; otherwise return fitted instance of GPTail
    
    Returns:
        t.Union[GPTail, sp.optimize.OptimizeResult]: Description
    &#34;&#34;&#34;
    exceedances = data[data &gt; threshold]

    #rescale exceedances and threshold so that both parameters are in roughly the same scale, improving numerical conditioning
    sdev = np.std(exceedances)
    
    # rescaling the data rescales the location and scale parameter, and leaves the shape parameter unchanged
    norm_exceedances = exceedances/sdev
    norm_threshold = threshold/sdev

    norm_max = max(norm_exceedances)

    constraints = LinearConstraint(
      A = np.array([[1/(norm_max-norm_threshold),1], [1, 0]]), 
      lb=np.zeros((2,)), 
      ub=np.Inf)

    if x0 is None:
      # use default scipy fitter to get initial estimate
      # this is almost always good enough
      shape, _, scale = gpdist.fit(norm_exceedances, floc=norm_threshold)
      x0 = np.array([scale, shape])

    loss_func = lambda params: GPTail.loss(params, norm_threshold, norm_exceedances)
    loss_grad = lambda params: GPTail. loss_grad(params, norm_threshold, norm_exceedances)
    loss_hessian = lambda params: GPTail.loss_hessian(params, norm_threshold, norm_exceedances)

    res = minimize(
      fun=loss_func, 
      x0 = x0,
      method = &#34;trust-constr&#34;,
      jac = loss_grad,
      hess = loss_hessian,
      constraints = [constraints])

    #print(res)
    if return_opt_results:
      warnings.warn(&#34;Returning raw results for rescaled exceedance data (sdev ~ 1).&#34;)
      return res
    else:
      scale, shape = list(res.x)

      return cls(
        threshold = sdev*norm_threshold,
        scale = sdev*scale,
        shape = shape,
        data = sdev*norm_exceedances)

  def plot_diagnostics(self) -&gt; None:
    &#34;&#34;&#34;Returns a figure with fit diagnostic for the GP model
    
    &#34;&#34;&#34;
    if self.data is None:
      raise ValueError(&#34;Exceedance data was not provided for this model.&#34;)

    fig, axs = plt.subplots(3, 2)


    #################### Profile log-likelihood

    # set profile intervals based on MLE variance
    mle_cov = self.mle_cov()
    scale_bounds, shape_bounds = (self.scale - np.sqrt(mle_cov[0,0]), self.scale + np.sqrt(mle_cov[0,0])), (self.shape - np.sqrt(mle_cov[1,1]), self.shape + np.sqrt(mle_cov[1,1]))

    #set profile grids
    scale_grid = np.linspace(scale_bounds[0], scale_bounds[1], 50)
    shape_grid = np.linspace(shape_bounds[0], shape_bounds[1], 50)


    #declare profile functions
    scale_profile_func = lambda x: self.loglik([x, self.shape], self.threshold, self.data)
    shape_profile_func = lambda x: self.loglik([self.scale, x], self.threshold, self.data)

    loss_value = scale_profile_func(self.scale)

    scale_profile = np.array([scale_profile_func(x) for x in scale_grid])
    shape_profile = np.array([shape_profile_func(x) for x in shape_grid])

    alpha = 2 if loss_value &gt; 0 else 0.5

    # filter to almost-optimal values

    def filter_grid(grid, optimum):
      radius = 2*np.abs(optimum)
      return np.logical_and(np.logical_not(np.isnan(grid)), np.isfinite(grid), np.abs(grid - optimum) &lt; radius)

    scale_filter = filter_grid(scale_profile, loss_value)
    shape_filter = filter_grid(shape_profile, loss_value)


    valid_scales = scale_grid[scale_filter]
    valid_scale_profile = scale_profile[scale_filter]

    axs[0,0].plot(valid_scales, valid_scale_profile, color=self._figure_color_palette[0])
    axs[0,0].vlines(x=self.scale, ymin=min(valid_scale_profile), ymax = max(valid_scale_profile), linestyle=&#34;dashed&#34;, colors = self._figure_color_palette[1])
    axs[0,0].title.set_text(&#39;Profile scale log-likelihood&#39;)
    axs[0,0].set_xlabel(&#39;Scale&#39;)
    axs[0,0].set_ylabel(&#39;log-likelihood&#39;)

    valid_shapes = shape_grid[shape_filter]
    valid_shape_profile = shape_profile[shape_filter]

    axs[0,1].plot(valid_shapes, valid_shape_profile, color=self._figure_color_palette[0])
    axs[0,1].vlines(x=self.shape, ymin=min(valid_shape_profile), ymax = max(valid_shape_profile), linestyle=&#34;dashed&#34;, colors = self._figure_color_palette[1])
    axs[0,1].title.set_text(&#39;Profile shape log-likelihood&#39;)
    axs[0,1].set_xlabel(&#39;Shape&#39;)
    axs[0,1].set_ylabel(&#39;log-likelihood&#39;)

    ######################## Log-likelihood surface ###############
    scale_grid, shape_grid = np.mgrid[
    min(valid_scales):max(valid_scales):2*np.sqrt(mle_cov[0,0])/50,
    shape_bounds[0]:shape_bounds[1]:2*np.sqrt(mle_cov[0,0])/50]

    scale_mesh, shape_mesh = np.meshgrid(
      np.linspace(min(valid_scales), max(valid_scales), 50), 
      np.linspace(min(valid_shapes), max(valid_shapes), 50))

    max_x = max(self.data)

    z = np.empty(scale_mesh.shape)
    for i in range(scale_mesh.shape[0]):
      for j in range(scale_mesh.shape[1]):
        shape = shape_mesh[i,j]
        scale = scale_mesh[i,j]
        if shape &lt; 0 and self.threshold - scale/shape &lt; max_x:
          z[i,j] = np.nan
        else:
          z[i,j] = self.loglik([scale, shape], self.threshold, self.data)

    # negate z to recover true loglikelihood
    axs[1,0].contourf(scale_mesh, shape_mesh, z, levels = 15)
    axs[1,0].scatter([self.scale], [self.shape], color=&#34;darkorange&#34;, s=2)
    axs[1,0].annotate(text=&#34;MLE&#34;, xy = (self.scale, self.shape), color=&#34;darkorange&#34;)
    axs[1,0].title.set_text(&#39;Log-likelihood surface&#39;)
    axs[1,0].set_xlabel(&#39;Scale&#39;)
    axs[1,0].set_ylabel(&#39;Shape&#39;)



    ############## histogram vs density ################
    hist_data = axs[1,1].hist(self.data, bins=25, edgecolor=&#34;white&#34;, color=self._figure_color_palette[0])

    range_min, range_max = min(self.data), max(self.data)
    x_axis = np.linspace(range_min, range_max, 100)
    pdf_vals = self.pdf(x_axis)
    y_axis = hist_data[0][0] / pdf_vals[0] * pdf_vals

    axs[1,1].plot(x_axis, y_axis, color=self._figure_color_palette[1])
    axs[1,1].title.set_text(&#34;Data vs fitted density&#34;)
    axs[1,1].set_xlabel(&#39;Exceedance data&#39;)
    axs[1,1].yaxis.set_visible(False) # Hide only x axis
    #axs[0, 0].set_aspect(&#39;equal&#39;, &#39;box&#39;)


    ############# Q-Q plot ################
    probability_range = np.linspace(0.01,0.99, 99)
    empirical_quantiles = np.quantile(self.data, probability_range)
    tail_quantiles = self.ppf(probability_range)

    axs[2,0].scatter(tail_quantiles, empirical_quantiles, color = self._figure_color_palette[0])
    min_x, max_x = min(tail_quantiles), max(tail_quantiles)
    #axs[0,1].set_aspect(&#39;equal&#39;, &#39;box&#39;)
    axs[2,0].title.set_text(&#39;Q-Q plot&#39;)
    axs[2,0].set_xlabel(&#39;model quantiles&#39;)
    axs[2,0].set_ylabel(&#39;Data quantiles&#39;)
    axs[2,0].grid()
    axs[2,0].plot([min_x,max_x],[min_x,max_x], linestyle=&#34;--&#34;, color=&#34;black&#34;)

    ############ Mean return plot ###############
    # scale, shape = self.scale, self.shape

    # n_obs = len(self.data)
    # exceedance_frequency = 1/np.logspace(1,4,20)
    # return_levels = self.ppf(1 - exceedance_frequency)

    # axs[2,1].plot(1.0/exceedance_frequency,return_levels,color=self._figure_color_palette[0])
    # axs[2,1].set_xscale(&#34;log&#34;)
    # axs[2,1].title.set_text(&#34;Exceedance model&#39;s return levels&#34;)
    # axs[2,1].set_xlabel(&#39;1/frequency&#39;)
    # axs[2,1].set_ylabel(&#39;Return level&#39;)
    # axs[2,1].grid()




    # exs_prob = 1 #carried over from older code

    # m = 10**np.linspace(np.log(1/exs_prob + 1)/np.log(10), 3,20)
    # return_levels = self.ppf(1 - 1/(exs_prob*m))

    # axs[2,1].plot(m,return_levels,color=self._figure_color_palette[0])
    # axs[2,1].set_xscale(&#34;log&#34;)
    # axs[2,1].title.set_text(&#39;Exceedance return levels&#39;)
    # axs[2,1].set_xlabel(&#39;1/frequency&#39;)
    # axs[2,1].set_ylabel(&#39;Return level&#39;)
    # axs[2,1].grid()

    # try:
    #   #for this bit, look at An Introduction to Statistical selfing of Extreme Values, p.82
    #   mle_cov = self.mle_cov()
    #   eigenvals, eigenvecs = np.linalg.eig(mle_cov)
    #   if np.all(eigenvals &gt; 0):
    #     covariance = np.eye(3)
    #     covariance[1::,1::] = mle_cov
    #     covariance[0,0] = exs_prob*(1-exs_prob)/n_obs
    #     #
    #     return_stdevs = []
    #     for m_ in m:
    #       quantile_grad = np.array([
    #         scale*m_**(shape)*exs_prob**(shape-1),
    #         shape**(-1)*((exs_prob*m_)**shape-1),
    #         -scale*shape**(-2)*((exs_prob*m_)**shape-1)+scale*shape**(-1)*(exs_prob*m_)**shape*np.log(exs_prob*m_)
    #         ])
    #       #
    #       sdev = np.sqrt(quantile_grad.T.dot(covariance).dot(quantile_grad))
    #       return_stdevs.append(sdev)
    #     #
    #     axs[2,1].fill_between(m, return_levels - return_stdevs, return_levels + return_stdevs, alpha=0.2, color=self._figure_color_palette[1])
    #   else:
    #     warnings.warn(&#34;Covariance MLE matrix is not positive definite; it might be ill-conditioned&#34;, stacklevel=2)
    # except Exception as e:
    #   warnings.warn(f&#34;Confidence bands for return level could not be calculated; covariance matrix might be ill-conditioned; full trace: {traceback.format_exc()}&#34;, stacklevel=2)

    plt.tight_layout()
    return fig</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="riskmodels.univariate.BaseDistribution" href="#riskmodels.univariate.BaseDistribution">BaseDistribution</a></li>
<li>pydantic.main.BaseModel</li>
<li>pydantic.utils.Representation</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="riskmodels.univariate.GPTail.data"><code class="name">var <span class="ident">data</span> :Â Optional[numpy.ndarray]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="riskmodels.univariate.GPTail.scale"><code class="name">var <span class="ident">scale</span> :Â pydantic.types.PositiveFloat</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="riskmodels.univariate.GPTail.shape"><code class="name">var <span class="ident">shape</span> :Â float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="riskmodels.univariate.GPTail.threshold"><code class="name">var <span class="ident">threshold</span> :Â float</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="riskmodels.univariate.GPTail.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>data:Â np.ndarray, threshold:Â float, x0:Â np.ndarrayÂ =Â None, return_opt_results=False) â€‘>Â Union[<a title="riskmodels.univariate.GPTail" href="#riskmodels.univariate.GPTail">GPTail</a>,Â scipy.optimize.optimize.OptimizeResult]</span>
</code></dt>
<dd>
<div class="desc"><p>Fits a eneralised Pareto tail model using a constrained trust region method</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>exceedance data above threshold</dd>
<dt><strong><code>threshold</code></strong> :&ensp;<code>float</code></dt>
<dd>Model threshold</dd>
<dt><strong><code>x0</code></strong> :&ensp;<code>np.ndarray</code>, optional</dt>
<dd>Initial guess for optimization; if None, the result of scipy.stats.genpareto.fit is used as a starting point.</dd>
<dt><strong><code>return_opt_results</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If True, return the OptimizeResult object; otherwise return fitted instance of GPTail</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>t.Union[<a title="riskmodels.univariate.GPTail" href="#riskmodels.univariate.GPTail">GPTail</a>, sp.optimize.OptimizeResult]</code></dt>
<dd>Description</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def fit(
  cls, 
  data: np.ndarray, 
  threshold: float, 
  x0: np.ndarray = None,
  return_opt_results=False) -&gt; t.Union[GPTail, sp.optimize.OptimizeResult]:
  &#34;&#34;&#34;Fits a eneralised Pareto tail model using a constrained trust region method
  
  Args:
      data (np.ndarray): exceedance data above threshold
      threshold (float): Model threshold
      x0 (np.ndarray, optional): Initial guess for optimization; if None, the result of scipy.stats.genpareto.fit is used as a starting point.
      return_opt_results (bool, optional): If True, return the OptimizeResult object; otherwise return fitted instance of GPTail
  
  Returns:
      t.Union[GPTail, sp.optimize.OptimizeResult]: Description
  &#34;&#34;&#34;
  exceedances = data[data &gt; threshold]

  #rescale exceedances and threshold so that both parameters are in roughly the same scale, improving numerical conditioning
  sdev = np.std(exceedances)
  
  # rescaling the data rescales the location and scale parameter, and leaves the shape parameter unchanged
  norm_exceedances = exceedances/sdev
  norm_threshold = threshold/sdev

  norm_max = max(norm_exceedances)

  constraints = LinearConstraint(
    A = np.array([[1/(norm_max-norm_threshold),1], [1, 0]]), 
    lb=np.zeros((2,)), 
    ub=np.Inf)

  if x0 is None:
    # use default scipy fitter to get initial estimate
    # this is almost always good enough
    shape, _, scale = gpdist.fit(norm_exceedances, floc=norm_threshold)
    x0 = np.array([scale, shape])

  loss_func = lambda params: GPTail.loss(params, norm_threshold, norm_exceedances)
  loss_grad = lambda params: GPTail. loss_grad(params, norm_threshold, norm_exceedances)
  loss_hessian = lambda params: GPTail.loss_hessian(params, norm_threshold, norm_exceedances)

  res = minimize(
    fun=loss_func, 
    x0 = x0,
    method = &#34;trust-constr&#34;,
    jac = loss_grad,
    hess = loss_hessian,
    constraints = [constraints])

  #print(res)
  if return_opt_results:
    warnings.warn(&#34;Returning raw results for rescaled exceedance data (sdev ~ 1).&#34;)
    return res
  else:
    scale, shape = list(res.x)

    return cls(
      threshold = sdev*norm_threshold,
      scale = sdev*scale,
      shape = shape,
      data = sdev*norm_exceedances)</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.GPTail.loglik"><code class="name flex">
<span>def <span class="ident">loglik</span></span>(<span>params:Â t.List[float], threshold:Â float, data:Â np.ndarray) â€‘>Â float</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the negative log-likelihood for a Generalised Pareto model</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*params</code></strong> :&ensp;<code>t.List[float]</code></dt>
<dd>Vector parameter with scale and shape values, in that order</dd>
<dt><strong><code>threshold</code></strong> :&ensp;<code>float</code></dt>
<dd>model threshold</dd>
<dt><strong><code>data</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>exceedance data</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>float</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def loglik(cls, params: t.List[float], threshold: float, data: np.ndarray) -&gt; float:
  &#34;&#34;&#34;Returns the negative log-likelihood for a Generalised Pareto model
  
  Args:
      *params (t.List[float]): Vector parameter with scale and shape values, in that order
      threshold (float): model threshold
      data (np.ndarray): exceedance data
  
  Returns:
      float
  &#34;&#34;&#34;
  scale, shape = params
  return np.sum(gpdist.logpdf(data, loc=threshold, c=shape, scale=scale))</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.GPTail.loglik_grad"><code class="name flex">
<span>def <span class="ident">loglik_grad</span></span>(<span>params:Â t.List[float], threshold:Â float, data:Â np.ndarray) â€‘>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the gradient of the negative log-likelihood for a Generalised Pareto model</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*params</code></strong> :&ensp;<code>t.List[float]</code></dt>
<dd>Vector parameter with scale and shape values, in that order</dd>
<dt><strong><code>threshold</code></strong> :&ensp;<code>float</code></dt>
<dd>model threshold</dd>
<dt><strong><code>data</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>exceedance data</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>gradient array</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def loglik_grad(cls, params: t.List[float], threshold: float, data: np.ndarray) -&gt; np.ndarray:
  &#34;&#34;&#34;Returns the gradient of the negative log-likelihood for a Generalised Pareto model
  
  Args:
      *params (t.List[float]): Vector parameter with scale and shape values, in that order
      threshold (float): model threshold
      data (np.ndarray): exceedance data
  
  Returns:
      np.ndarray: gradient array
  &#34;&#34;&#34;
  scale, shape = params
  y = data - threshold

  if not np.isclose(shape, 0, atol=cls._error_tol):
    grad_scale = np.sum((y - scale)/(scale*(y*shape + scale)))
    grad_shape = np.sum(-((y*(1 + shape))/(shape*(y*shape + scale))) + np.log(1 + (y*shape)/scale)/shape**2)
  else:
    grad_scale = np.sum((y-scale)/scale**2)
    grad_shape = np.sum(y*(y-2*scale)/(2*scale**2))

  return np.array([grad_scale,grad_shape])</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.GPTail.loglik_hessian"><code class="name flex">
<span>def <span class="ident">loglik_hessian</span></span>(<span>params:Â t.List[float], threshold:Â float, data:Â np.ndarray) â€‘>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the Hessian matrix of the negative log-likelihood for a Generalised Pareto model</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*params</code></strong> :&ensp;<code>t.List[float]</code></dt>
<dd>Vector parameter with scale and shape values, in that order</dd>
<dt><strong><code>threshold</code></strong> :&ensp;<code>float</code></dt>
<dd>model threshold</dd>
<dt><strong><code>data</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>exceedance data above the threshold</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>hessian matrix array</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def loglik_hessian(cls, params: t.List[float], threshold: float, data: np.ndarray) -&gt; np.ndarray:
  &#34;&#34;&#34;Returns the Hessian matrix of the negative log-likelihood for a Generalised Pareto model
  
  Args:
      *params (t.List[float]): Vector parameter with scale and shape values, in that order
      threshold (float): model threshold
      data (np.ndarray): exceedance data above the threshold
  
  Returns:
      np.ndarray: hessian matrix array
  &#34;&#34;&#34;
  scale, shape = params
  y = data - threshold

  if not np.isclose(shape, 0, atol=cls._error_tol):
    d2scale = np.sum(-(1/(shape*scale**2)) + (1 + shape)/(shape*(y*shape + scale)**2))

    #d2shape = (y (3 y Î¾ + y Î¾^2 + 2 Ïƒ))/(Î¾^2 (y Î¾ + Ïƒ)^2) - (2 Log[1 + (y Î¾)/Ïƒ])/Î¾^3
    d2shape = np.sum((y*(3*y*shape + y*shape**2 + 2*scale))/(shape**2*(y*shape + scale)**2) - (2*np.log(1 + (y*shape)/scale))/shape**3)

    #dscale_dshape = (y (-y + Ïƒ))/(Ïƒ (y Î¾ + Ïƒ)^2)
    dscale_dshape = np.sum((y*(-y + scale))/(scale*(y*shape + scale)**2))
  else:
    d2scale = np.sum((scale-2*y)/scale**3)
    dscale_dshape = np.sum(-y*(y-scale)/scale**3)
    d2shape = np.sum(y**2*(3*scale-2*y)/(3*scale**3))

  hessian = np.array([[d2scale,dscale_dshape],[dscale_dshape,d2shape]])

  return hessian</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.GPTail.logreg"><code class="name flex">
<span>def <span class="ident">logreg</span></span>(<span>params:Â t.List[float]) â€‘>Â float</span>
</code></dt>
<dd>
<div class="desc"><p>The regularisation terms are inspired in uninformative and zero-mean Gaussian priors for the scale and shape respectively, thus it is given by</p>
<p><span><span class="MathJax_Preview">r(\sigma, \xi) = 0.5 \cdot \log(\sigma) - 0.5 \xi^2</span><script type="math/tex; mode=display">r(\sigma, \xi) = 0.5 \cdot \log(\sigma) - 0.5 \xi^2</script></span></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>params</code></strong> :&ensp;<code>t.List[float]</code></dt>
<dd>scale and shape parameters in that order</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>Description</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def logreg(cls, params: t.List[float]) -&gt; float:
  &#34;&#34;&#34;The regularisation terms are inspired in uninformative and zero-mean Gaussian priors for the scale and shape respectively, thus it is given by

  $$r(\\sigma, \\xi) = 0.5 \\cdot \\log(\\sigma) - 0.5 \\xi^2$$
  
  Args:
      params (t.List[float]): scale and shape parameters in that order
  
  Returns:
      float: Description
  &#34;&#34;&#34;
  scale, shape = params

  return 0.5*(np.log(scale) + shape**2)</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.GPTail.logreg_grad"><code class="name flex">
<span>def <span class="ident">logreg_grad</span></span>(<span>params:Â t.List[float]) â€‘>Â float</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the gradient of the regularisation term</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>params</code></strong> :&ensp;<code>t.List[float]</code></dt>
<dd>scale and shape parameters in that order</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def logreg_grad(cls, params: t.List[float]) -&gt; float:
  &#34;&#34;&#34;Returns the gradient of the regularisation term
  
  Args:
      params (t.List[float]): scale and shape parameters in that order

  &#34;&#34;&#34;
  scale, shape = params
  return 0.5*np.array([1.0/scale, 2*shape])</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.GPTail.logreg_hessian"><code class="name flex">
<span>def <span class="ident">logreg_hessian</span></span>(<span>params:Â t.List[float]) â€‘>Â float</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the Hessian of the regularisation term</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>params</code></strong> :&ensp;<code>t.List[float]</code></dt>
<dd>scale and shape parameters in that order</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def logreg_hessian(cls, params: t.List[float]) -&gt; float:
  &#34;&#34;&#34;Returns the Hessian of the regularisation term
  
  Args:
      params (t.List[float]): scale and shape parameters in that order

  &#34;&#34;&#34;
  scale, shape = params
  return 0.5*np.array([-1.0/scale**2,0,0,2]).reshape((2,2))</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.GPTail.loss"><code class="name flex">
<span>def <span class="ident">loss</span></span>(<span>params:Â t.List[float], threshold:Â float, data:Â np.ndarray) â€‘>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the loss function on the provided parameters; this is the sum of the (negative) data log-likelihood and (negative) regularisation terms for the scale and shape. Everything is divided by the number of data points.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>params</code></strong> :&ensp;<code>t.List[float]</code></dt>
<dd>Description</dd>
<dt><strong><code>threshold</code></strong> :&ensp;<code>float</code></dt>
<dd>Model threshold; eequivalently, location parameter</dd>
<dt><strong><code>data</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>exceedance data above threshold</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def loss(cls, params: t.List[float], threshold: float, data: np.ndarray) -&gt; np.ndarray:
  &#34;&#34;&#34;Calculate the loss function on the provided parameters; this is the sum of the (negative) data log-likelihood and (negative) regularisation terms for the scale and shape. Everything is divided by the number of data points.
  
  Args:
      params (t.List[float]): Description
      threshold (float): Model threshold; eequivalently, location parameter
      data (np.ndarray): exceedance data above threshold
  
  &#34;&#34;&#34;
  scale, shape = params
  n = len(data)
  unnorm = cls.loglik(params, threshold, data) + cls.logreg(params)
  return -unnorm/n</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.GPTail.loss_grad"><code class="name flex">
<span>def <span class="ident">loss_grad</span></span>(<span>params:Â t.List[float], threshold:Â float, data:Â np.ndarray) â€‘>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the loss function's gradient on the provided parameters</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>params</code></strong> :&ensp;<code>t.List[float]</code></dt>
<dd>Description</dd>
<dt><strong><code>threshold</code></strong> :&ensp;<code>float</code></dt>
<dd>Model threshold; eequivalently, location parameter</dd>
<dt><strong><code>data</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>exceedance data above threshold</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def loss_grad(cls, params: t.List[float], threshold: float, data: np.ndarray) -&gt; np.ndarray:
  &#34;&#34;&#34;Calculate the loss function&#39;s gradient on the provided parameters
  
  Args:
      params (t.List[float]): Description
      threshold (float): Model threshold; eequivalently, location parameter
      data (np.ndarray): exceedance data above threshold
  
  &#34;&#34;&#34;
  n = len(data)
  return -(cls.loglik_grad(params, threshold, data) + cls.logreg_grad(params))/n</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.GPTail.loss_hessian"><code class="name flex">
<span>def <span class="ident">loss_hessian</span></span>(<span>params:Â t.List[float], threshold:Â float, data:Â np.ndarray) â€‘>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the loss function's Hessian on the provided parameters</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>params</code></strong> :&ensp;<code>t.List[float]</code></dt>
<dd>Description</dd>
<dt><strong><code>threshold</code></strong> :&ensp;<code>float</code></dt>
<dd>Model threshold; eequivalently, location parameter</dd>
<dt><strong><code>data</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>exceedance data above threshold</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def loss_hessian(cls, params: t.List[float], threshold: float, data: np.ndarray) -&gt; np.ndarray:
  &#34;&#34;&#34;Calculate the loss function&#39;s Hessian on the provided parameters
  
  Args:
      params (t.List[float]): Description
      threshold (float): Model threshold; eequivalently, location parameter
      data (np.ndarray): exceedance data above threshold
  
  &#34;&#34;&#34;
  n = len(data)
  return -(cls.loglik_hessian(params, threshold, data) + cls.logreg_hessian(params))/n</code></pre>
</details>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="riskmodels.univariate.GPTail.endpoint"><code class="name">var <span class="ident">endpoint</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def endpoint(self):
  return self.threshold - self.scale/self.shape if self.shape &lt; 0 else np.Inf</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.GPTail.model"><code class="name">var <span class="ident">model</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def model(self):
  return gpdist(loc=self.threshold, c=self.shape, scale=self.scale)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="riskmodels.univariate.GPTail.mle_cov"><code class="name flex">
<span>def <span class="ident">mle_cov</span></span>(<span>self) â€‘>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the estimated parameter covariance matrix evaluated at the fitted parameters</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>Covariance matrix</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mle_cov(self) -&gt; np.ndarray:
  &#34;&#34;&#34;Returns the estimated parameter covariance matrix evaluated at the fitted parameters
  
  Returns:
      np.ndarray: Covariance matrix
  &#34;&#34;&#34;

  if self.data is None:
    raise ValueError(&#34;exceedance data not provided for this instance of GPTail; covariance matrix can&#39;t be estimated&#34;)
  else:
    hess = self.loglik_hessian([self.scale,self.shape], threshold=self.threshold, data=self.data)
    return np.linalg.inv(-hess)</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.GPTail.plot_diagnostics"><code class="name flex">
<span>def <span class="ident">plot_diagnostics</span></span>(<span>self) â€‘>Â None</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a figure with fit diagnostic for the GP model</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_diagnostics(self) -&gt; None:
  &#34;&#34;&#34;Returns a figure with fit diagnostic for the GP model
  
  &#34;&#34;&#34;
  if self.data is None:
    raise ValueError(&#34;Exceedance data was not provided for this model.&#34;)

  fig, axs = plt.subplots(3, 2)


  #################### Profile log-likelihood

  # set profile intervals based on MLE variance
  mle_cov = self.mle_cov()
  scale_bounds, shape_bounds = (self.scale - np.sqrt(mle_cov[0,0]), self.scale + np.sqrt(mle_cov[0,0])), (self.shape - np.sqrt(mle_cov[1,1]), self.shape + np.sqrt(mle_cov[1,1]))

  #set profile grids
  scale_grid = np.linspace(scale_bounds[0], scale_bounds[1], 50)
  shape_grid = np.linspace(shape_bounds[0], shape_bounds[1], 50)


  #declare profile functions
  scale_profile_func = lambda x: self.loglik([x, self.shape], self.threshold, self.data)
  shape_profile_func = lambda x: self.loglik([self.scale, x], self.threshold, self.data)

  loss_value = scale_profile_func(self.scale)

  scale_profile = np.array([scale_profile_func(x) for x in scale_grid])
  shape_profile = np.array([shape_profile_func(x) for x in shape_grid])

  alpha = 2 if loss_value &gt; 0 else 0.5

  # filter to almost-optimal values

  def filter_grid(grid, optimum):
    radius = 2*np.abs(optimum)
    return np.logical_and(np.logical_not(np.isnan(grid)), np.isfinite(grid), np.abs(grid - optimum) &lt; radius)

  scale_filter = filter_grid(scale_profile, loss_value)
  shape_filter = filter_grid(shape_profile, loss_value)


  valid_scales = scale_grid[scale_filter]
  valid_scale_profile = scale_profile[scale_filter]

  axs[0,0].plot(valid_scales, valid_scale_profile, color=self._figure_color_palette[0])
  axs[0,0].vlines(x=self.scale, ymin=min(valid_scale_profile), ymax = max(valid_scale_profile), linestyle=&#34;dashed&#34;, colors = self._figure_color_palette[1])
  axs[0,0].title.set_text(&#39;Profile scale log-likelihood&#39;)
  axs[0,0].set_xlabel(&#39;Scale&#39;)
  axs[0,0].set_ylabel(&#39;log-likelihood&#39;)

  valid_shapes = shape_grid[shape_filter]
  valid_shape_profile = shape_profile[shape_filter]

  axs[0,1].plot(valid_shapes, valid_shape_profile, color=self._figure_color_palette[0])
  axs[0,1].vlines(x=self.shape, ymin=min(valid_shape_profile), ymax = max(valid_shape_profile), linestyle=&#34;dashed&#34;, colors = self._figure_color_palette[1])
  axs[0,1].title.set_text(&#39;Profile shape log-likelihood&#39;)
  axs[0,1].set_xlabel(&#39;Shape&#39;)
  axs[0,1].set_ylabel(&#39;log-likelihood&#39;)

  ######################## Log-likelihood surface ###############
  scale_grid, shape_grid = np.mgrid[
  min(valid_scales):max(valid_scales):2*np.sqrt(mle_cov[0,0])/50,
  shape_bounds[0]:shape_bounds[1]:2*np.sqrt(mle_cov[0,0])/50]

  scale_mesh, shape_mesh = np.meshgrid(
    np.linspace(min(valid_scales), max(valid_scales), 50), 
    np.linspace(min(valid_shapes), max(valid_shapes), 50))

  max_x = max(self.data)

  z = np.empty(scale_mesh.shape)
  for i in range(scale_mesh.shape[0]):
    for j in range(scale_mesh.shape[1]):
      shape = shape_mesh[i,j]
      scale = scale_mesh[i,j]
      if shape &lt; 0 and self.threshold - scale/shape &lt; max_x:
        z[i,j] = np.nan
      else:
        z[i,j] = self.loglik([scale, shape], self.threshold, self.data)

  # negate z to recover true loglikelihood
  axs[1,0].contourf(scale_mesh, shape_mesh, z, levels = 15)
  axs[1,0].scatter([self.scale], [self.shape], color=&#34;darkorange&#34;, s=2)
  axs[1,0].annotate(text=&#34;MLE&#34;, xy = (self.scale, self.shape), color=&#34;darkorange&#34;)
  axs[1,0].title.set_text(&#39;Log-likelihood surface&#39;)
  axs[1,0].set_xlabel(&#39;Scale&#39;)
  axs[1,0].set_ylabel(&#39;Shape&#39;)



  ############## histogram vs density ################
  hist_data = axs[1,1].hist(self.data, bins=25, edgecolor=&#34;white&#34;, color=self._figure_color_palette[0])

  range_min, range_max = min(self.data), max(self.data)
  x_axis = np.linspace(range_min, range_max, 100)
  pdf_vals = self.pdf(x_axis)
  y_axis = hist_data[0][0] / pdf_vals[0] * pdf_vals

  axs[1,1].plot(x_axis, y_axis, color=self._figure_color_palette[1])
  axs[1,1].title.set_text(&#34;Data vs fitted density&#34;)
  axs[1,1].set_xlabel(&#39;Exceedance data&#39;)
  axs[1,1].yaxis.set_visible(False) # Hide only x axis
  #axs[0, 0].set_aspect(&#39;equal&#39;, &#39;box&#39;)


  ############# Q-Q plot ################
  probability_range = np.linspace(0.01,0.99, 99)
  empirical_quantiles = np.quantile(self.data, probability_range)
  tail_quantiles = self.ppf(probability_range)

  axs[2,0].scatter(tail_quantiles, empirical_quantiles, color = self._figure_color_palette[0])
  min_x, max_x = min(tail_quantiles), max(tail_quantiles)
  #axs[0,1].set_aspect(&#39;equal&#39;, &#39;box&#39;)
  axs[2,0].title.set_text(&#39;Q-Q plot&#39;)
  axs[2,0].set_xlabel(&#39;model quantiles&#39;)
  axs[2,0].set_ylabel(&#39;Data quantiles&#39;)
  axs[2,0].grid()
  axs[2,0].plot([min_x,max_x],[min_x,max_x], linestyle=&#34;--&#34;, color=&#34;black&#34;)

  ############ Mean return plot ###############
  # scale, shape = self.scale, self.shape

  # n_obs = len(self.data)
  # exceedance_frequency = 1/np.logspace(1,4,20)
  # return_levels = self.ppf(1 - exceedance_frequency)

  # axs[2,1].plot(1.0/exceedance_frequency,return_levels,color=self._figure_color_palette[0])
  # axs[2,1].set_xscale(&#34;log&#34;)
  # axs[2,1].title.set_text(&#34;Exceedance model&#39;s return levels&#34;)
  # axs[2,1].set_xlabel(&#39;1/frequency&#39;)
  # axs[2,1].set_ylabel(&#39;Return level&#39;)
  # axs[2,1].grid()




  # exs_prob = 1 #carried over from older code

  # m = 10**np.linspace(np.log(1/exs_prob + 1)/np.log(10), 3,20)
  # return_levels = self.ppf(1 - 1/(exs_prob*m))

  # axs[2,1].plot(m,return_levels,color=self._figure_color_palette[0])
  # axs[2,1].set_xscale(&#34;log&#34;)
  # axs[2,1].title.set_text(&#39;Exceedance return levels&#39;)
  # axs[2,1].set_xlabel(&#39;1/frequency&#39;)
  # axs[2,1].set_ylabel(&#39;Return level&#39;)
  # axs[2,1].grid()

  # try:
  #   #for this bit, look at An Introduction to Statistical selfing of Extreme Values, p.82
  #   mle_cov = self.mle_cov()
  #   eigenvals, eigenvecs = np.linalg.eig(mle_cov)
  #   if np.all(eigenvals &gt; 0):
  #     covariance = np.eye(3)
  #     covariance[1::,1::] = mle_cov
  #     covariance[0,0] = exs_prob*(1-exs_prob)/n_obs
  #     #
  #     return_stdevs = []
  #     for m_ in m:
  #       quantile_grad = np.array([
  #         scale*m_**(shape)*exs_prob**(shape-1),
  #         shape**(-1)*((exs_prob*m_)**shape-1),
  #         -scale*shape**(-2)*((exs_prob*m_)**shape-1)+scale*shape**(-1)*(exs_prob*m_)**shape*np.log(exs_prob*m_)
  #         ])
  #       #
  #       sdev = np.sqrt(quantile_grad.T.dot(covariance).dot(quantile_grad))
  #       return_stdevs.append(sdev)
  #     #
  #     axs[2,1].fill_between(m, return_levels - return_stdevs, return_levels + return_stdevs, alpha=0.2, color=self._figure_color_palette[1])
  #   else:
  #     warnings.warn(&#34;Covariance MLE matrix is not positive definite; it might be ill-conditioned&#34;, stacklevel=2)
  # except Exception as e:
  #   warnings.warn(f&#34;Confidence bands for return level could not be calculated; covariance matrix might be ill-conditioned; full trace: {traceback.format_exc()}&#34;, stacklevel=2)

  plt.tight_layout()
  return fig</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="riskmodels.univariate.BaseDistribution" href="#riskmodels.univariate.BaseDistribution">BaseDistribution</a></b></code>:
<ul class="hlist">
<li><code><a title="riskmodels.univariate.BaseDistribution.cdf" href="#riskmodels.univariate.BaseDistribution.cdf">cdf</a></code></li>
<li><code><a title="riskmodels.univariate.BaseDistribution.cvar" href="#riskmodels.univariate.BaseDistribution.cvar">cvar</a></code></li>
<li><code><a title="riskmodels.univariate.BaseDistribution.histogram" href="#riskmodels.univariate.BaseDistribution.histogram">histogram</a></code></li>
<li><code><a title="riskmodels.univariate.BaseDistribution.mean" href="#riskmodels.univariate.BaseDistribution.mean">mean</a></code></li>
<li><code><a title="riskmodels.univariate.BaseDistribution.moment" href="#riskmodels.univariate.BaseDistribution.moment">moment</a></code></li>
<li><code><a title="riskmodels.univariate.BaseDistribution.pdf" href="#riskmodels.univariate.BaseDistribution.pdf">pdf</a></code></li>
<li><code><a title="riskmodels.univariate.BaseDistribution.plot" href="#riskmodels.univariate.BaseDistribution.plot">plot</a></code></li>
<li><code><a title="riskmodels.univariate.BaseDistribution.ppf" href="#riskmodels.univariate.BaseDistribution.ppf">ppf</a></code></li>
<li><code><a title="riskmodels.univariate.BaseDistribution.simulate" href="#riskmodels.univariate.BaseDistribution.simulate">simulate</a></code></li>
<li><code><a title="riskmodels.univariate.BaseDistribution.std" href="#riskmodels.univariate.BaseDistribution.std">std</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="riskmodels.univariate.GPTailMixture"><code class="flex name class">
<span>class <span class="ident">GPTailMixture</span></span>
<span>(</span><span>**data:Â Any)</span>
</code></dt>
<dd>
<div class="desc"><p>Mixture distribution with generalised Pareto components. This is a base class for
Bayesian generalised Pareto tail models, which can be seen as an uniformly weighted mixture of the posterior samples; the convolution of a discrete (or empirical) distribution with a generalised Pareto distribution also results in a mixture of this kind. Most methods inherited from <code><a title="riskmodels.univariate.BaseDistribution" href="#riskmodels.univariate.BaseDistribution">BaseDistribution</a></code> have an extra argument <code>return_all</code>. When it is True, the full posterior sample of the method evaluation is returned.</p>
<h2 id="args">Args</h2>
<dl>
<dt>data(np.ndarray, optional): Data that induced the model, if applicable</dt>
<dt><strong><code>weights</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>component weights</dd>
<dt><strong><code>thresholds</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>vector of threshold parameters, one for each component</dd>
<dt><strong><code>scales</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>vector of scale parameters, one for each component</dd>
<dt><strong><code>shapes</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>vector of shape parameters, one for each component</dd>
</dl>
<p>Create a new model by parsing and validating input data from keyword arguments.</p>
<p>Raises ValidationError if the input data cannot be parsed to form a valid model.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class GPTailMixture(BaseDistribution):

  &#34;&#34;&#34;Mixture distribution with generalised Pareto components. This is a base class for  Bayesian generalised Pareto tail models, which can be seen as an uniformly weighted mixture of the posterior samples; the convolution of a discrete (or empirical) distribution with a generalised Pareto distribution also results in a mixture of this kind. Most methods inherited from `BaseDistribution` have an extra argument `return_all`. When it is True, the full posterior sample of the method evaluation is returned.

  Args:
      data(np.ndarray, optional): Data that induced the model, if applicable
      weights (np.ndarray): component weights
      thresholds (np.ndarray): vector of threshold parameters, one for each component
      scales (np.ndarray): vector of scale parameters, one for each component
      shapes (np.ndarray): vector of shape parameters, one for each component
  &#34;&#34;&#34;

  data: t.Optional[np.ndarray]
  weights: np.ndarray
  thresholds: np.ndarray
  scales: np.ndarray
  shapes: np.ndarray

  def __repr__(self):
    return f&#34;Mixture of generalised Pareto distributions with {len(self.weights)} components&#34;

  @validator(&#34;weights&#34;, allow_reuse=True)
  def check_weigths(cls, weights):
    if not np.isclose(np.sum(weights),1, atol=cls._error_tol):
      raise ValueError(f&#34;Weights don&#39;t sum 1 (sum = {np.sum(weights)})&#34;)
    elif np.any(weights &lt;= 0):
      raise ValueError(&#34;Negative or null weights are present&#34;)
    else:
      return weights

  def moment(self, n: int, return_all: bool = False) -&gt; float:
    &#34;&#34;&#34;Returns the n-th order moment
    
    Args:
        n (int): Moment&#39;s order
        return_all (bool, optional): If `True`, all posterior samples of the value are returned; otherwise  the moment value of the posterior predictive distribution is returned. The posterior distribution is taken to be the empirical distribution of posterior samples.
    
    Returns:
        float: moment value
    &#34;&#34;&#34;
    vals = np.array([gpdist.moment(
      n,
      loc=threshold, 
      c=shape, 
      scale=scale) for threshold, shape, scale in zip(self.thresholds, self.shapes, self.scales)])

    if return_all:
      return vals
    else:
      return np.dot(self.weights, vals)

  def mean(self, return_all: bool = False) -&gt; float:
    &#34;&#34;&#34;Returns the mean value
    
    Args:
        return_all (bool, optional): If `True`, all posterior samples of the value are returned; the mean of the posterior predictive distribution is evaluated. The posterior distribution is taken to be the empirical distribution of posterior samples.
    
    Returns:
        float: mean value
    &#34;&#34;&#34;
    vals = gpdist.mean(
      loc=self.thresholds, 
      c=self.shapes, 
      scale=self.scales)

    if return_all:
      return vals
    else:
      return np.dot(self.weights, vals)

  def std(self, return_all: bool = False) -&gt; float:
    &#34;&#34;&#34;Standard deviation value
    
    Args:
        return_all (bool, optional): If `True`, all posterior samples of the value are returned; otherwise the standard deviation of the posterior predictive distribution is returned. The posterior distribution is taken to be the empirical distribution of posterior samples.
    
    Returns:
        float: standard deviation value
    &#34;&#34;&#34;
    var = gpdist.var(
      loc=self.thresholds, 
      c=self.shapes, 
      scale=self.scales)

    mean = gpdist.mean(
      loc=self.thresholds, 
      c=self.shapes, 
      scale=self.scales)

    if return_all:
      return np.sqrt(var)
    else:
      return np.sqrt(np.dot(self.weights, var + mean**2) - self.mean()**2)

  def cdf(self, x: t.Union[float, np.ndarray], return_all: bool = False) -&gt; t.Union[float, np.ndarray]:
    &#34;&#34;&#34;Evaluates the CDF function
    
    Args:
        x (t.Union[float, np.ndarray]): Point at which to evaluate it
        return_all (bool, optional): If `True`, all posterior samples of the value are returned; the CDF of the posterior predictive distribution is evaluated. The posterior distribution is taken to be the empirical distribution of posterior samples.
    
    Returns:
        t.Union[float, np.ndarray]: CDF value
    &#34;&#34;&#34;

    if isinstance(x, np.ndarray):
      return np.array([self.cdf(elem, return_all) for elem in x])

    vals = gpdist.cdf(
      x,
      loc=self.thresholds, 
      c=self.shapes, 
      scale=self.scales)

    if return_all:
      return vals
    else:
      return np.dot(self.weights, vals)

  def pdf(self, x: t.Union[float, np.ndarray], return_all: bool = False) -&gt; t.Union[float, np.ndarray]:
    &#34;&#34;&#34;Evaluates the pdf function
    
    Args:
        x (t.Union[float, np.ndarray]): Point at which to evaluate it
        return_all (bool, optional): If `True`, all posterior samples of the value are returned; otherwise the pdf of the posterior predictive distribution is evaluated. The posterior distribution is taken to be the empirical distribution of posterior samples.
    
    Returns:
        t.Union[float, np.ndarray]: pdf value
    &#34;&#34;&#34;
    
    if isinstance(x, np.ndarray):
      return np.array([self.pdf(elem, return_all) for elem in x])

    vals = gpdist.pdf(
      x,
      loc=self.thresholds, 
      c=self.shapes, 
      scale=self.scales)

    if return_all:
      return vals
    else:
      return np.dot(self.weights, vals)

  def ppf(self, q: t.Union[float, np.ndarray], return_all: bool = False) -&gt; t.Union[float, np.ndarray]:
    &#34;&#34;&#34;Evaluates the quantile function
    
    Args:
        x (t.Union[float, np.ndarray]): probability level
        return_all (bool, optional): If `True`, all posterior samples of the value are returned; otherwise  the quantile function of the posterior predictive distribution is evaluated. The posterior distribution is taken to be the empirical distribution of posterior samples.
    
    Returns:
        t.Union[float, np.ndarray]: quantile function value value
    &#34;&#34;&#34;
    if isinstance(q, np.ndarray):
      return np.array([self.ppf(elem, return_all) for elem in q])

    vals = gpdist.ppf(
      q,
      loc=self.thresholds, 
      c=self.shapes, 
      scale=self.scales)


    if return_all:
      return vals
    else:
      def target_function(x):
        return self.cdf(x) - q
      x0 = np.dot(self.weights, vals)
      return root_scalar(target_function, x0 = x0, x1 = x0 + 1, method=&#34;secant&#34;).root

  def cvar(self, p:float, return_all: bool = False) -&gt; float:
    &#34;&#34;&#34;Returns the conditional value at risk for a given probability level
    
    Args:
        x (t.Union[float, np.ndarray]): probability level
        return_all (bool, optional): If `True`, all posterior samples of the value are returned; otherwise  the cvar of the posterior predictive distribution is evaluated. The posterior distribution is taken to be the empirical distribution of posterior samples.
    
    Returns:
        t.Union[float, np.ndarray]: conditional value at risk
    &#34;&#34;&#34;
    if p &lt; 0 or p &gt;= 1:
      raise ValueError(&#34;p must be in the open interval (0,1)&#34;)

    return (self &gt;= self.ppf(p)).mean(return_all)

  def simulate(self, size: int) -&gt; np.ndarray:
    n_samples = np.random.multinomial(n=size, pvals = self.weights, size=1)[0]
    indices = (n_samples &gt; 0).nonzero()[0]
    samples = [gpdist.rvs(size=n_samples[i], c=self.shapes[i], scale=self.scales[i], loc=self.thresholds[i]) for i in indices]
    return np.concatenate(samples, axis=0)


  def __gt__(self, other: float) -&gt; GPTailMixture:
    exceedance_prob = 1 - self.cdf(other)
    #prob_exceedance = np.dot(self.weights, prob_cond_exceedance)
    if exceedance_prob == 0:
      raise ValueError(f&#34;There is no probability mass above {other}; conditional distribution does not exist.&#34;)

    conditional_weights = self.weights*gpdist.sf(other, c=self.shapes, scale=self.scales, loc=self.thresholds)/exceedance_prob

    indices = (conditional_weights &gt; 0).nonzero()[0] #indices of mixture components with nonzero exceedance probability

    new_weights = conditional_weights[indices]

    # disable warnings temporarily
    # with warnings.catch_warnings():
    #   warnings.simplefilter(&#34;ignore&#34;)
    #   new_thresholds = np.array([(GPTail(threshold = mu, shape = xi, scale = sigma) &gt;= other).threshold for mu, sigma, xi in zip(self.thresholds[indices], self.scales[indices], self.shapes[indices])])

    new_thresholds = np.array([max(other, threshold) for threshold in self.thresholds[indices]])
    new_shapes = self.shapes[indices]
    new_scales = self.scales[indices] + new_shapes*(new_thresholds - self.thresholds[indices])
    # new_thresholds = np.clip(self.thresholds[indices], a_min=other, a_max = np.Inf)
    # new_shapes = self.shapes[indices]
    # new_scales = self.scales[indices] + new_shapes*np.clip(other - new_thresholds, a_min = 0.0, a_max=np.Inf)

    if self.data is not None and np.all(self.data &lt; other):
      warnings.warn(f&#34;No observed data above {other}; setting data to None in conditioned model&#34;, stacklevel=2)
      new_data = None
    elif self.data is None:
      new_data = None
    else:
      new_data = self.data[self.data &gt; other]

    return type(self)(
      weights = new_weights,
      thresholds = new_thresholds,
      shapes = new_shapes,
      scales = new_scales,
      data = new_data)

  def __ge__(self, other: float) -&gt; BaseDistribution:
    return self.__gt__(other)

  def __add__(self, other: self._allowed_scalar_types):

    if type(other) not in self._allowed_scalar_types:
      raise TypeError(f&#34;+ is implemented for types {self._allowed_scalar_types}&#34;)

    new_data = None if self.data is None else self.data + other

    return type(self)(
      weights = self.weights,
      thresholds = self.thresholds + other,
      shapes = self.shapes,
      scales = self.scales,
      data = new_data)

  def __mul__(self, other: self._allowed_scalar_types):

    if type(other) not in self._allowed_scalar_types:
      raise TypeError(f&#34;* is implemented for types {self._allowed_scalar_types}&#34;)

    if other &lt;= 0:
      raise ValueError(f&#34;product supported for positive scalars only&#34;)

    new_data = None if self.data is None else other*self.data

    return type(self)(
      weights = self.weights,
      thresholds = other*self.thresholds,
      shapes = self.shapes,
      scales = other*self.scales,
      data = new_data)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="riskmodels.univariate.BaseDistribution" href="#riskmodels.univariate.BaseDistribution">BaseDistribution</a></li>
<li>pydantic.main.BaseModel</li>
<li>pydantic.utils.Representation</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="riskmodels.univariate.BayesianGPTail" href="#riskmodels.univariate.BayesianGPTail">BayesianGPTail</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="riskmodels.univariate.GPTailMixture.data"><code class="name">var <span class="ident">data</span> :Â Optional[numpy.ndarray]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="riskmodels.univariate.GPTailMixture.scales"><code class="name">var <span class="ident">scales</span> :Â numpy.ndarray</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="riskmodels.univariate.GPTailMixture.shapes"><code class="name">var <span class="ident">shapes</span> :Â numpy.ndarray</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="riskmodels.univariate.GPTailMixture.thresholds"><code class="name">var <span class="ident">thresholds</span> :Â numpy.ndarray</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="riskmodels.univariate.GPTailMixture.weights"><code class="name">var <span class="ident">weights</span> :Â numpy.ndarray</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="riskmodels.univariate.GPTailMixture.check_weigths"><code class="name flex">
<span>def <span class="ident">check_weigths</span></span>(<span>weights)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@validator(&#34;weights&#34;, allow_reuse=True)
def check_weigths(cls, weights):
  if not np.isclose(np.sum(weights),1, atol=cls._error_tol):
    raise ValueError(f&#34;Weights don&#39;t sum 1 (sum = {np.sum(weights)})&#34;)
  elif np.any(weights &lt;= 0):
    raise ValueError(&#34;Negative or null weights are present&#34;)
  else:
    return weights</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="riskmodels.univariate.GPTailMixture.cdf"><code class="name flex">
<span>def <span class="ident">cdf</span></span>(<span>self, x:Â t.Union[float,Â np.ndarray], return_all:Â boolÂ =Â False) â€‘>Â Union[float,Â numpy.ndarray]</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluates the CDF function</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>t.Union[float, np.ndarray]</code></dt>
<dd>Point at which to evaluate it</dd>
<dt><strong><code>return_all</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If <code>True</code>, all posterior samples of the value are returned; the CDF of the posterior predictive distribution is evaluated. The posterior distribution is taken to be the empirical distribution of posterior samples.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>t.Union[float, np.ndarray]</code></dt>
<dd>CDF value</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cdf(self, x: t.Union[float, np.ndarray], return_all: bool = False) -&gt; t.Union[float, np.ndarray]:
  &#34;&#34;&#34;Evaluates the CDF function
  
  Args:
      x (t.Union[float, np.ndarray]): Point at which to evaluate it
      return_all (bool, optional): If `True`, all posterior samples of the value are returned; the CDF of the posterior predictive distribution is evaluated. The posterior distribution is taken to be the empirical distribution of posterior samples.
  
  Returns:
      t.Union[float, np.ndarray]: CDF value
  &#34;&#34;&#34;

  if isinstance(x, np.ndarray):
    return np.array([self.cdf(elem, return_all) for elem in x])

  vals = gpdist.cdf(
    x,
    loc=self.thresholds, 
    c=self.shapes, 
    scale=self.scales)

  if return_all:
    return vals
  else:
    return np.dot(self.weights, vals)</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.GPTailMixture.cvar"><code class="name flex">
<span>def <span class="ident">cvar</span></span>(<span>self, p:Â float, return_all:Â boolÂ =Â False) â€‘>Â float</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the conditional value at risk for a given probability level</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>t.Union[float, np.ndarray]</code></dt>
<dd>probability level</dd>
<dt><strong><code>return_all</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If <code>True</code>, all posterior samples of the value are returned; otherwise
the cvar of the posterior predictive distribution is evaluated. The posterior distribution is taken to be the empirical distribution of posterior samples.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>t.Union[float, np.ndarray]</code></dt>
<dd>conditional value at risk</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cvar(self, p:float, return_all: bool = False) -&gt; float:
  &#34;&#34;&#34;Returns the conditional value at risk for a given probability level
  
  Args:
      x (t.Union[float, np.ndarray]): probability level
      return_all (bool, optional): If `True`, all posterior samples of the value are returned; otherwise  the cvar of the posterior predictive distribution is evaluated. The posterior distribution is taken to be the empirical distribution of posterior samples.
  
  Returns:
      t.Union[float, np.ndarray]: conditional value at risk
  &#34;&#34;&#34;
  if p &lt; 0 or p &gt;= 1:
    raise ValueError(&#34;p must be in the open interval (0,1)&#34;)

  return (self &gt;= self.ppf(p)).mean(return_all)</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.GPTailMixture.mean"><code class="name flex">
<span>def <span class="ident">mean</span></span>(<span>self, return_all:Â boolÂ =Â False) â€‘>Â float</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the mean value</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>return_all</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If <code>True</code>, all posterior samples of the value are returned; the mean of the posterior predictive distribution is evaluated. The posterior distribution is taken to be the empirical distribution of posterior samples.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>mean value</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mean(self, return_all: bool = False) -&gt; float:
  &#34;&#34;&#34;Returns the mean value
  
  Args:
      return_all (bool, optional): If `True`, all posterior samples of the value are returned; the mean of the posterior predictive distribution is evaluated. The posterior distribution is taken to be the empirical distribution of posterior samples.
  
  Returns:
      float: mean value
  &#34;&#34;&#34;
  vals = gpdist.mean(
    loc=self.thresholds, 
    c=self.shapes, 
    scale=self.scales)

  if return_all:
    return vals
  else:
    return np.dot(self.weights, vals)</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.GPTailMixture.moment"><code class="name flex">
<span>def <span class="ident">moment</span></span>(<span>self, n:Â int, return_all:Â boolÂ =Â False) â€‘>Â float</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the n-th order moment</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code></dt>
<dd>Moment's order</dd>
<dt><strong><code>return_all</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If <code>True</code>, all posterior samples of the value are returned; otherwise
the moment value of the posterior predictive distribution is returned. The posterior distribution is taken to be the empirical distribution of posterior samples.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>moment value</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def moment(self, n: int, return_all: bool = False) -&gt; float:
  &#34;&#34;&#34;Returns the n-th order moment
  
  Args:
      n (int): Moment&#39;s order
      return_all (bool, optional): If `True`, all posterior samples of the value are returned; otherwise  the moment value of the posterior predictive distribution is returned. The posterior distribution is taken to be the empirical distribution of posterior samples.
  
  Returns:
      float: moment value
  &#34;&#34;&#34;
  vals = np.array([gpdist.moment(
    n,
    loc=threshold, 
    c=shape, 
    scale=scale) for threshold, shape, scale in zip(self.thresholds, self.shapes, self.scales)])

  if return_all:
    return vals
  else:
    return np.dot(self.weights, vals)</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.GPTailMixture.pdf"><code class="name flex">
<span>def <span class="ident">pdf</span></span>(<span>self, x:Â t.Union[float,Â np.ndarray], return_all:Â boolÂ =Â False) â€‘>Â Union[float,Â numpy.ndarray]</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluates the pdf function</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>t.Union[float, np.ndarray]</code></dt>
<dd>Point at which to evaluate it</dd>
<dt><strong><code>return_all</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If <code>True</code>, all posterior samples of the value are returned; otherwise the pdf of the posterior predictive distribution is evaluated. The posterior distribution is taken to be the empirical distribution of posterior samples.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>t.Union[float, np.ndarray]</code></dt>
<dd>pdf value</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pdf(self, x: t.Union[float, np.ndarray], return_all: bool = False) -&gt; t.Union[float, np.ndarray]:
  &#34;&#34;&#34;Evaluates the pdf function
  
  Args:
      x (t.Union[float, np.ndarray]): Point at which to evaluate it
      return_all (bool, optional): If `True`, all posterior samples of the value are returned; otherwise the pdf of the posterior predictive distribution is evaluated. The posterior distribution is taken to be the empirical distribution of posterior samples.
  
  Returns:
      t.Union[float, np.ndarray]: pdf value
  &#34;&#34;&#34;
  
  if isinstance(x, np.ndarray):
    return np.array([self.pdf(elem, return_all) for elem in x])

  vals = gpdist.pdf(
    x,
    loc=self.thresholds, 
    c=self.shapes, 
    scale=self.scales)

  if return_all:
    return vals
  else:
    return np.dot(self.weights, vals)</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.GPTailMixture.ppf"><code class="name flex">
<span>def <span class="ident">ppf</span></span>(<span>self, q:Â t.Union[float,Â np.ndarray], return_all:Â boolÂ =Â False) â€‘>Â Union[float,Â numpy.ndarray]</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluates the quantile function</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>t.Union[float, np.ndarray]</code></dt>
<dd>probability level</dd>
<dt><strong><code>return_all</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If <code>True</code>, all posterior samples of the value are returned; otherwise
the quantile function of the posterior predictive distribution is evaluated. The posterior distribution is taken to be the empirical distribution of posterior samples.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>t.Union[float, np.ndarray]</code></dt>
<dd>quantile function value value</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ppf(self, q: t.Union[float, np.ndarray], return_all: bool = False) -&gt; t.Union[float, np.ndarray]:
  &#34;&#34;&#34;Evaluates the quantile function
  
  Args:
      x (t.Union[float, np.ndarray]): probability level
      return_all (bool, optional): If `True`, all posterior samples of the value are returned; otherwise  the quantile function of the posterior predictive distribution is evaluated. The posterior distribution is taken to be the empirical distribution of posterior samples.
  
  Returns:
      t.Union[float, np.ndarray]: quantile function value value
  &#34;&#34;&#34;
  if isinstance(q, np.ndarray):
    return np.array([self.ppf(elem, return_all) for elem in q])

  vals = gpdist.ppf(
    q,
    loc=self.thresholds, 
    c=self.shapes, 
    scale=self.scales)


  if return_all:
    return vals
  else:
    def target_function(x):
      return self.cdf(x) - q
    x0 = np.dot(self.weights, vals)
    return root_scalar(target_function, x0 = x0, x1 = x0 + 1, method=&#34;secant&#34;).root</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.GPTailMixture.std"><code class="name flex">
<span>def <span class="ident">std</span></span>(<span>self, return_all:Â boolÂ =Â False) â€‘>Â float</span>
</code></dt>
<dd>
<div class="desc"><p>Standard deviation value</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>return_all</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If <code>True</code>, all posterior samples of the value are returned; otherwise the standard deviation of the posterior predictive distribution is returned. The posterior distribution is taken to be the empirical distribution of posterior samples.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>standard deviation value</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def std(self, return_all: bool = False) -&gt; float:
  &#34;&#34;&#34;Standard deviation value
  
  Args:
      return_all (bool, optional): If `True`, all posterior samples of the value are returned; otherwise the standard deviation of the posterior predictive distribution is returned. The posterior distribution is taken to be the empirical distribution of posterior samples.
  
  Returns:
      float: standard deviation value
  &#34;&#34;&#34;
  var = gpdist.var(
    loc=self.thresholds, 
    c=self.shapes, 
    scale=self.scales)

  mean = gpdist.mean(
    loc=self.thresholds, 
    c=self.shapes, 
    scale=self.scales)

  if return_all:
    return np.sqrt(var)
  else:
    return np.sqrt(np.dot(self.weights, var + mean**2) - self.mean()**2)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="riskmodels.univariate.BaseDistribution" href="#riskmodels.univariate.BaseDistribution">BaseDistribution</a></b></code>:
<ul class="hlist">
<li><code><a title="riskmodels.univariate.BaseDistribution.histogram" href="#riskmodels.univariate.BaseDistribution.histogram">histogram</a></code></li>
<li><code><a title="riskmodels.univariate.BaseDistribution.plot" href="#riskmodels.univariate.BaseDistribution.plot">plot</a></code></li>
<li><code><a title="riskmodels.univariate.BaseDistribution.simulate" href="#riskmodels.univariate.BaseDistribution.simulate">simulate</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="riskmodels.univariate.Mixture"><code class="flex name class">
<span>class <span class="ident">Mixture</span></span>
<span>(</span><span>**data:Â Any)</span>
</code></dt>
<dd>
<div class="desc"><p>This class represents a probability distribution given by a mixture of weighted continuous and empirical densities; as the base continuous densities can only be of class GPTail, this class is intended to represent either a semiparametric model with a Generalised Pareto tail, or the convolution of such a model with an integer distribution, as is the case for the power surplus distribution in power system reliability modeling.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>distributions</code></strong> :&ensp;<code>t.List[<a title="riskmodels.univariate.BaseDistribution" href="#riskmodels.univariate.BaseDistribution">BaseDistribution</a>]</code></dt>
<dd>list of distributions that make up the mixture</dd>
<dt><strong><code>weights</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>weights for each of the distribution. The weights must be a distribution themselves</dd>
</dl>
<p>Create a new model by parsing and validating input data from keyword arguments.</p>
<p>Raises ValidationError if the input data cannot be parsed to form a valid model.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Mixture(BaseDistribution):

  &#34;&#34;&#34;This class represents a probability distribution given by a mixture of weighted continuous and empirical densities; as the base continuous densities can only be of class GPTail, this class is intended to represent either a semiparametric model with a Generalised Pareto tail, or the convolution of such a model with an integer distribution, as is the case for the power surplus distribution in power system reliability modeling.

  Args:
      distributions (t.List[BaseDistribution]): list of distributions that make up the mixture
      weights (np.ndarray): weights for each of the distribution. The weights must be a distribution themselves
  &#34;&#34;&#34;
  
  distributions: t.List[BaseDistribution]
  weights: np.ndarray

  def __repr__(self):
    return f&#34;Mixture with {len(self.weights)} components&#34;

  @validator(&#34;weights&#34;, allow_reuse=True)
  def check_weigths(cls, weights):
    if not np.isclose(np.sum(weights),1, atol=cls._error_tol):
      raise ValidationError(f&#34;Weights don&#39;t sum 1 (sum = {np.sum(weights)})&#34;)
    elif np.any(weights &lt;= 0):
      raise ValidationError(&#34;Negative or null weights are present&#34;)
    else:
      return weights


  def __mul__(self, factor: float):

    return Mixture(
      weights=self.weights, 
      distributions = [factor*dist for dist in self.distributions])

  def __add__(self, factor: float):

    return Mixture(
      weights=self.weights, 
      distributions = [factor + dist for dist in self.distributions])

  def __ge__(self, other:float):

    if not isinstance(other, self._allowed_scalar_types):
      raise TypeError(f&#34;&gt;= is implemented for instances of types : {self._allowed_scalar_types}&#34;)

    if self.cdf(other) == 1:
      raise ValueError(&#34;There is no probability mass above provided threshold&#34;)

    cond_weights = np.array([1 - dist.cdf(other) + (isinstance(dist,Empirical))*dist.pdf(other) for dist in self.distributions])
    new_weights = cond_weights*self.weights

    indices = (new_weights &gt; 0).nonzero()[0]

    nz_weights = new_weights[indices]
    nz_dists = [self.distributions[i] for i in indices]

    return Mixture(
      weights = nz_weights/np.sum(nz_weights), 
      distributions = [dist &gt;= other for dist in nz_dists])

  def __gt__(self, other:float):

    if not isinstance(other, self._allowed_scalar_types):
      raise TypeError(f&#34;&gt; is implemented for instances of types : {self._allowed_scalar_types}&#34;)

    if self.cdf(other) == 1:
      raise ValueError(&#34;There is no probability mass above provided threshold&#34;)

    cond_weights = np.array([1 - dist.cdf(other) + (isinstance(dist,Empirical))*dist.pdf(other) for dist in self.distributions])

    new_weights = cond_weights*self.weights

    indices = (new_weights &gt; 0).nonzero()[0]

    nz_weights = new_weights[indices]
    nz_dists = [self.distributions[i] for i in indices]

    return Mixture(
      weights = nz_weights/np.sum(nz_weights), 
      distributions = [dist &gt; other for dist in nz_dists])

    # index = self.support &gt; other

    # return type(self)(
    #   self.support[index],
    #   self.pdf_values[index]/np.sum(self.pdf_values[index]), 
    #   self.data[self.data &gt; other])


  def simulate(self, size: int) -&gt; np.ndarray:
    &#34;&#34;&#34;Simulate values from mixture distribution
    
    Args:
        size (int): Sample size
    
    Returns:
        np.ndarray: simulated sample
    &#34;&#34;&#34;
    n_samples = np.random.multinomial(n=size, pvals = self.weights, size=1)[0]
    indices = (n_samples &gt; 0).nonzero()[0]
    samples = [dist.simulate(size=k) for dist, k in zip([self.distributions[k] for k in indices], n_samples[indices])]
    return np.concatenate(samples, axis=0)

  def cdf(self, x: t.Union[float, np.ndarray], **kwargs) -&gt; t.Union[float, np.ndarray]:
    &#34;&#34;&#34;Evaluate Mixture&#39;s CDF function
    
    Args:
        x (t.Union[float, np.ndarray]): point at which to evaluate CDF
        **kwargs: Additional arguments passed to individual mixture component&#39;s CDF function
    
    Returns:
        t.Union[float, np.ndarray]: CDF value
    &#34;&#34;&#34;

    # the use of a list and a reduce is needed because mixture components might return scalars or vectors depending on their class and on the passed kwargs.
    vals = [w*dist.cdf(x,**kwargs) for w, dist in zip(self.weights, self.distributions)]
    return reduce(lambda x,y: x + y, vals)

  def pdf(self, x: t.Union[float, np.ndarray], **kwargs) -&gt; t.Union[float, np.ndarray]:
    &#34;&#34;&#34;Evaluate Mixture&#39;s pdf function
    
    Args:
        x (t.Union[float, np.ndarray]): point at which to evaluate CDF
        **kwargs: Additional arguments passed to individual mixture component&#39;s pdf function
    
    Returns:
        t.Union[float, np.ndarray]: pdf value
    &#34;&#34;&#34;

    # the use of a list and a reduce is needed because mixture components might return scalars or vectors depending on their class and on the passed kwargs.
    vals = [w*dist.pdf(x,**kwargs) for w, dist in zip(self.weights, self.distributions)]
    return reduce(lambda x,y: x + y, vals)

  def ppf(self, q: t.Union[float, np.ndarray], **kwargs) -&gt; t.Union[float, np.ndarray]:
    &#34;&#34;&#34;Evaluate Mixture&#39;s quantile function function
    
    Args:
        x (t.Union[float, np.ndarray]): point at which to evaluate quantile function
        **kwargs: Additional arguments passed to individual mixture component&#39;s quantile function
    
    Returns:
        t.Union[float, np.ndarray]: quantile value
    &#34;&#34;&#34;

    if isinstance(q, np.ndarray):
      return np.array([self.ppf(elem, **kwargs) for elem in q])

    def target_function(x):
      return self.cdf(x) - q

    vals =[w*dist.ppf(q, **kwargs) for w, dist in zip(self.weights, self.distributions)]
    x0 = reduce(lambda x,y: x + y, vals)

    # the use of a list and a reduce is needed because mixture components might return scalars or vectors depending on their class and on the passed kwargs.
    vals =[w*dist.ppf(0.5 + q/2, **kwargs) for w, dist in zip(self.weights, self.distributions)] 
    x1 = reduce(lambda x,y: x + y, vals)

    return root_scalar(target_function, x0 = x0, x1 = x1, method=&#34;secant&#34;).root

  def moment(self, n: int, **kwargs) -&gt; float:
    &#34;&#34;&#34;Evaluate Mixture&#39;s n-th moment
    
    Args:
        x (t.Union[float, np.ndarray]): Moment order
        **kwargs: Additional arguments passed to individual mixture components&#39; moment function
    
    Returns:
        t.Union[float, np.ndarray]: moment value
    &#34;&#34;&#34;
    
    # the use of a list and a reduce is needed because mixture components might return scalars or vectors depending on their class and on the passed kwargs.
    vals = [w*dist.moment(n, **kwargs) for w, dist in zip(self.weights, self.distributions)]
    return reduce(lambda x,y: x + y, vals)

  def mean(self, **kwargs) -&gt; float:
    &#34;&#34;&#34;Evaluate Mixture&#39;s mean
    
    Args:
        **kwargs: Additional arguments passed to individual mixture components&#39; mean function
    
    Returns:
        float: mean value
    &#34;&#34;&#34;

    # the use of a list and a reduce is needed because mixture components might return scalars or vectors depending on their class and on the passed kwargs.
    vals = [w*dist.mean(**kwargs) for w, dist in zip(self.weights, self.distributions)]
    return reduce(lambda x,y: x + y, vals)

  def std(self, **kwargs) -&gt; float:
    &#34;&#34;&#34;Evaluate Mixture&#39;s standard deviation
    
    Args:
        **kwargs: Additional arguments passed to individual mixture components&#39; standard deviation function
    
    Returns:
        float: standard deviation value
    &#34;&#34;&#34;

    # the use of a list and a reduce is needed because mixture components might return scalars or vectors depending on their class and on the passed kwargs.
    vals = [w*(dist.std(**kwargs)**2 + dist.mean(**kwargs)**2) for w, dist in zip(self.weights, self.distributions)]
    return np.sqrt(reduce(lambda x,y: x + y, vals) - self.mean()**2)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="riskmodels.univariate.BaseDistribution" href="#riskmodels.univariate.BaseDistribution">BaseDistribution</a></li>
<li>pydantic.main.BaseModel</li>
<li>pydantic.utils.Representation</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="riskmodels.univariate.EmpiricalWithGPTail" href="#riskmodels.univariate.EmpiricalWithGPTail">EmpiricalWithGPTail</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="riskmodels.univariate.Mixture.distributions"><code class="name">var <span class="ident">distributions</span> :Â List[<a title="riskmodels.univariate.BaseDistribution" href="#riskmodels.univariate.BaseDistribution">BaseDistribution</a>]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="riskmodels.univariate.Mixture.weights"><code class="name">var <span class="ident">weights</span> :Â numpy.ndarray</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="riskmodels.univariate.Mixture.check_weigths"><code class="name flex">
<span>def <span class="ident">check_weigths</span></span>(<span>weights)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@validator(&#34;weights&#34;, allow_reuse=True)
def check_weigths(cls, weights):
  if not np.isclose(np.sum(weights),1, atol=cls._error_tol):
    raise ValidationError(f&#34;Weights don&#39;t sum 1 (sum = {np.sum(weights)})&#34;)
  elif np.any(weights &lt;= 0):
    raise ValidationError(&#34;Negative or null weights are present&#34;)
  else:
    return weights</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="riskmodels.univariate.Mixture.cdf"><code class="name flex">
<span>def <span class="ident">cdf</span></span>(<span>self, x:Â t.Union[float,Â np.ndarray], **kwargs) â€‘>Â Union[float,Â numpy.ndarray]</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate Mixture's CDF function</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>t.Union[float, np.ndarray]</code></dt>
<dd>point at which to evaluate CDF</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional arguments passed to individual mixture component's CDF function</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>t.Union[float, np.ndarray]</code></dt>
<dd>CDF value</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cdf(self, x: t.Union[float, np.ndarray], **kwargs) -&gt; t.Union[float, np.ndarray]:
  &#34;&#34;&#34;Evaluate Mixture&#39;s CDF function
  
  Args:
      x (t.Union[float, np.ndarray]): point at which to evaluate CDF
      **kwargs: Additional arguments passed to individual mixture component&#39;s CDF function
  
  Returns:
      t.Union[float, np.ndarray]: CDF value
  &#34;&#34;&#34;

  # the use of a list and a reduce is needed because mixture components might return scalars or vectors depending on their class and on the passed kwargs.
  vals = [w*dist.cdf(x,**kwargs) for w, dist in zip(self.weights, self.distributions)]
  return reduce(lambda x,y: x + y, vals)</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.Mixture.mean"><code class="name flex">
<span>def <span class="ident">mean</span></span>(<span>self, **kwargs) â€‘>Â float</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate Mixture's mean</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional arguments passed to individual mixture components' mean function</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>mean value</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mean(self, **kwargs) -&gt; float:
  &#34;&#34;&#34;Evaluate Mixture&#39;s mean
  
  Args:
      **kwargs: Additional arguments passed to individual mixture components&#39; mean function
  
  Returns:
      float: mean value
  &#34;&#34;&#34;

  # the use of a list and a reduce is needed because mixture components might return scalars or vectors depending on their class and on the passed kwargs.
  vals = [w*dist.mean(**kwargs) for w, dist in zip(self.weights, self.distributions)]
  return reduce(lambda x,y: x + y, vals)</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.Mixture.moment"><code class="name flex">
<span>def <span class="ident">moment</span></span>(<span>self, n:Â int, **kwargs) â€‘>Â float</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate Mixture's n-th moment</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>t.Union[float, np.ndarray]</code></dt>
<dd>Moment order</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional arguments passed to individual mixture components' moment function</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>t.Union[float, np.ndarray]</code></dt>
<dd>moment value</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def moment(self, n: int, **kwargs) -&gt; float:
  &#34;&#34;&#34;Evaluate Mixture&#39;s n-th moment
  
  Args:
      x (t.Union[float, np.ndarray]): Moment order
      **kwargs: Additional arguments passed to individual mixture components&#39; moment function
  
  Returns:
      t.Union[float, np.ndarray]: moment value
  &#34;&#34;&#34;
  
  # the use of a list and a reduce is needed because mixture components might return scalars or vectors depending on their class and on the passed kwargs.
  vals = [w*dist.moment(n, **kwargs) for w, dist in zip(self.weights, self.distributions)]
  return reduce(lambda x,y: x + y, vals)</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.Mixture.pdf"><code class="name flex">
<span>def <span class="ident">pdf</span></span>(<span>self, x:Â t.Union[float,Â np.ndarray], **kwargs) â€‘>Â Union[float,Â numpy.ndarray]</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate Mixture's pdf function</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>t.Union[float, np.ndarray]</code></dt>
<dd>point at which to evaluate CDF</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional arguments passed to individual mixture component's pdf function</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>t.Union[float, np.ndarray]</code></dt>
<dd>pdf value</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pdf(self, x: t.Union[float, np.ndarray], **kwargs) -&gt; t.Union[float, np.ndarray]:
  &#34;&#34;&#34;Evaluate Mixture&#39;s pdf function
  
  Args:
      x (t.Union[float, np.ndarray]): point at which to evaluate CDF
      **kwargs: Additional arguments passed to individual mixture component&#39;s pdf function
  
  Returns:
      t.Union[float, np.ndarray]: pdf value
  &#34;&#34;&#34;

  # the use of a list and a reduce is needed because mixture components might return scalars or vectors depending on their class and on the passed kwargs.
  vals = [w*dist.pdf(x,**kwargs) for w, dist in zip(self.weights, self.distributions)]
  return reduce(lambda x,y: x + y, vals)</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.Mixture.ppf"><code class="name flex">
<span>def <span class="ident">ppf</span></span>(<span>self, q:Â t.Union[float,Â np.ndarray], **kwargs) â€‘>Â Union[float,Â numpy.ndarray]</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate Mixture's quantile function function</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>t.Union[float, np.ndarray]</code></dt>
<dd>point at which to evaluate quantile function</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional arguments passed to individual mixture component's quantile function</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>t.Union[float, np.ndarray]</code></dt>
<dd>quantile value</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ppf(self, q: t.Union[float, np.ndarray], **kwargs) -&gt; t.Union[float, np.ndarray]:
  &#34;&#34;&#34;Evaluate Mixture&#39;s quantile function function
  
  Args:
      x (t.Union[float, np.ndarray]): point at which to evaluate quantile function
      **kwargs: Additional arguments passed to individual mixture component&#39;s quantile function
  
  Returns:
      t.Union[float, np.ndarray]: quantile value
  &#34;&#34;&#34;

  if isinstance(q, np.ndarray):
    return np.array([self.ppf(elem, **kwargs) for elem in q])

  def target_function(x):
    return self.cdf(x) - q

  vals =[w*dist.ppf(q, **kwargs) for w, dist in zip(self.weights, self.distributions)]
  x0 = reduce(lambda x,y: x + y, vals)

  # the use of a list and a reduce is needed because mixture components might return scalars or vectors depending on their class and on the passed kwargs.
  vals =[w*dist.ppf(0.5 + q/2, **kwargs) for w, dist in zip(self.weights, self.distributions)] 
  x1 = reduce(lambda x,y: x + y, vals)

  return root_scalar(target_function, x0 = x0, x1 = x1, method=&#34;secant&#34;).root</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.Mixture.simulate"><code class="name flex">
<span>def <span class="ident">simulate</span></span>(<span>self, size:Â int) â€‘>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Simulate values from mixture distribution</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code></dt>
<dd>Sample size</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>simulated sample</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def simulate(self, size: int) -&gt; np.ndarray:
  &#34;&#34;&#34;Simulate values from mixture distribution
  
  Args:
      size (int): Sample size
  
  Returns:
      np.ndarray: simulated sample
  &#34;&#34;&#34;
  n_samples = np.random.multinomial(n=size, pvals = self.weights, size=1)[0]
  indices = (n_samples &gt; 0).nonzero()[0]
  samples = [dist.simulate(size=k) for dist, k in zip([self.distributions[k] for k in indices], n_samples[indices])]
  return np.concatenate(samples, axis=0)</code></pre>
</details>
</dd>
<dt id="riskmodels.univariate.Mixture.std"><code class="name flex">
<span>def <span class="ident">std</span></span>(<span>self, **kwargs) â€‘>Â float</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate Mixture's standard deviation</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional arguments passed to individual mixture components' standard deviation function</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>standard deviation value</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def std(self, **kwargs) -&gt; float:
  &#34;&#34;&#34;Evaluate Mixture&#39;s standard deviation
  
  Args:
      **kwargs: Additional arguments passed to individual mixture components&#39; standard deviation function
  
  Returns:
      float: standard deviation value
  &#34;&#34;&#34;

  # the use of a list and a reduce is needed because mixture components might return scalars or vectors depending on their class and on the passed kwargs.
  vals = [w*(dist.std(**kwargs)**2 + dist.mean(**kwargs)**2) for w, dist in zip(self.weights, self.distributions)]
  return np.sqrt(reduce(lambda x,y: x + y, vals) - self.mean()**2)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="riskmodels.univariate.BaseDistribution" href="#riskmodels.univariate.BaseDistribution">BaseDistribution</a></b></code>:
<ul class="hlist">
<li><code><a title="riskmodels.univariate.BaseDistribution.cvar" href="#riskmodels.univariate.BaseDistribution.cvar">cvar</a></code></li>
<li><code><a title="riskmodels.univariate.BaseDistribution.histogram" href="#riskmodels.univariate.BaseDistribution.histogram">histogram</a></code></li>
<li><code><a title="riskmodels.univariate.BaseDistribution.plot" href="#riskmodels.univariate.BaseDistribution.plot">plot</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="riskmodels" href="index.html">riskmodels</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="riskmodels.univariate.BaseDistribution" href="#riskmodels.univariate.BaseDistribution">BaseDistribution</a></code></h4>
<ul class="two-column">
<li><code><a title="riskmodels.univariate.BaseDistribution.Config" href="#riskmodels.univariate.BaseDistribution.Config">Config</a></code></li>
<li><code><a title="riskmodels.univariate.BaseDistribution.cdf" href="#riskmodels.univariate.BaseDistribution.cdf">cdf</a></code></li>
<li><code><a title="riskmodels.univariate.BaseDistribution.cvar" href="#riskmodels.univariate.BaseDistribution.cvar">cvar</a></code></li>
<li><code><a title="riskmodels.univariate.BaseDistribution.histogram" href="#riskmodels.univariate.BaseDistribution.histogram">histogram</a></code></li>
<li><code><a title="riskmodels.univariate.BaseDistribution.mean" href="#riskmodels.univariate.BaseDistribution.mean">mean</a></code></li>
<li><code><a title="riskmodels.univariate.BaseDistribution.moment" href="#riskmodels.univariate.BaseDistribution.moment">moment</a></code></li>
<li><code><a title="riskmodels.univariate.BaseDistribution.pdf" href="#riskmodels.univariate.BaseDistribution.pdf">pdf</a></code></li>
<li><code><a title="riskmodels.univariate.BaseDistribution.plot" href="#riskmodels.univariate.BaseDistribution.plot">plot</a></code></li>
<li><code><a title="riskmodels.univariate.BaseDistribution.ppf" href="#riskmodels.univariate.BaseDistribution.ppf">ppf</a></code></li>
<li><code><a title="riskmodels.univariate.BaseDistribution.simulate" href="#riskmodels.univariate.BaseDistribution.simulate">simulate</a></code></li>
<li><code><a title="riskmodels.univariate.BaseDistribution.std" href="#riskmodels.univariate.BaseDistribution.std">std</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="riskmodels.univariate.BayesianGPTail" href="#riskmodels.univariate.BayesianGPTail">BayesianGPTail</a></code></h4>
<ul class="two-column">
<li><code><a title="riskmodels.univariate.BayesianGPTail.data" href="#riskmodels.univariate.BayesianGPTail.data">data</a></code></li>
<li><code><a title="riskmodels.univariate.BayesianGPTail.fit" href="#riskmodels.univariate.BayesianGPTail.fit">fit</a></code></li>
<li><code><a title="riskmodels.univariate.BayesianGPTail.plot_diagnostics" href="#riskmodels.univariate.BayesianGPTail.plot_diagnostics">plot_diagnostics</a></code></li>
<li><code><a title="riskmodels.univariate.BayesianGPTail.scales" href="#riskmodels.univariate.BayesianGPTail.scales">scales</a></code></li>
<li><code><a title="riskmodels.univariate.BayesianGPTail.shapes" href="#riskmodels.univariate.BayesianGPTail.shapes">shapes</a></code></li>
<li><code><a title="riskmodels.univariate.BayesianGPTail.thresholds" href="#riskmodels.univariate.BayesianGPTail.thresholds">thresholds</a></code></li>
<li><code><a title="riskmodels.univariate.BayesianGPTail.weights" href="#riskmodels.univariate.BayesianGPTail.weights">weights</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="riskmodels.univariate.Binned" href="#riskmodels.univariate.Binned">Binned</a></code></h4>
<ul class="two-column">
<li><code><a title="riskmodels.univariate.Binned.data" href="#riskmodels.univariate.Binned.data">data</a></code></li>
<li><code><a title="riskmodels.univariate.Binned.from_data" href="#riskmodels.univariate.Binned.from_data">from_data</a></code></li>
<li><code><a title="riskmodels.univariate.Binned.from_empirical" href="#riskmodels.univariate.Binned.from_empirical">from_empirical</a></code></li>
<li><code><a title="riskmodels.univariate.Binned.integer_support" href="#riskmodels.univariate.Binned.integer_support">integer_support</a></code></li>
<li><code><a title="riskmodels.univariate.Binned.pdf_values" href="#riskmodels.univariate.Binned.pdf_values">pdf_values</a></code></li>
<li><code><a title="riskmodels.univariate.Binned.support" href="#riskmodels.univariate.Binned.support">support</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="riskmodels.univariate.Empirical" href="#riskmodels.univariate.Empirical">Empirical</a></code></h4>
<ul class="">
<li><code><a title="riskmodels.univariate.Empirical.cdf_values" href="#riskmodels.univariate.Empirical.cdf_values">cdf_values</a></code></li>
<li><code><a title="riskmodels.univariate.Empirical.check_pdf_values" href="#riskmodels.univariate.Empirical.check_pdf_values">check_pdf_values</a></code></li>
<li><code><a title="riskmodels.univariate.Empirical.data" href="#riskmodels.univariate.Empirical.data">data</a></code></li>
<li><code><a title="riskmodels.univariate.Empirical.ecdf" href="#riskmodels.univariate.Empirical.ecdf">ecdf</a></code></li>
<li><code><a title="riskmodels.univariate.Empirical.ecdf_inv" href="#riskmodels.univariate.Empirical.ecdf_inv">ecdf_inv</a></code></li>
<li><code><a title="riskmodels.univariate.Empirical.fit_tail_model" href="#riskmodels.univariate.Empirical.fit_tail_model">fit_tail_model</a></code></li>
<li><code><a title="riskmodels.univariate.Empirical.from_data" href="#riskmodels.univariate.Empirical.from_data">from_data</a></code></li>
<li><code><a title="riskmodels.univariate.Empirical.is_valid" href="#riskmodels.univariate.Empirical.is_valid">is_valid</a></code></li>
<li><code><a title="riskmodels.univariate.Empirical.map" href="#riskmodels.univariate.Empirical.map">map</a></code></li>
<li><code><a title="riskmodels.univariate.Empirical.max" href="#riskmodels.univariate.Empirical.max">max</a></code></li>
<li><code><a title="riskmodels.univariate.Empirical.min" href="#riskmodels.univariate.Empirical.min">min</a></code></li>
<li><code><a title="riskmodels.univariate.Empirical.moment" href="#riskmodels.univariate.Empirical.moment">moment</a></code></li>
<li><code><a title="riskmodels.univariate.Empirical.pdf_lookup" href="#riskmodels.univariate.Empirical.pdf_lookup">pdf_lookup</a></code></li>
<li><code><a title="riskmodels.univariate.Empirical.pdf_values" href="#riskmodels.univariate.Empirical.pdf_values">pdf_values</a></code></li>
<li><code><a title="riskmodels.univariate.Empirical.plot_mean_residual_life" href="#riskmodels.univariate.Empirical.plot_mean_residual_life">plot_mean_residual_life</a></code></li>
<li><code><a title="riskmodels.univariate.Empirical.ppf" href="#riskmodels.univariate.Empirical.ppf">ppf</a></code></li>
<li><code><a title="riskmodels.univariate.Empirical.simulate" href="#riskmodels.univariate.Empirical.simulate">simulate</a></code></li>
<li><code><a title="riskmodels.univariate.Empirical.support" href="#riskmodels.univariate.Empirical.support">support</a></code></li>
<li><code><a title="riskmodels.univariate.Empirical.to_integer" href="#riskmodels.univariate.Empirical.to_integer">to_integer</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="riskmodels.univariate.EmpiricalWithBayesianGPTail" href="#riskmodels.univariate.EmpiricalWithBayesianGPTail">EmpiricalWithBayesianGPTail</a></code></h4>
<ul class="">
<li><code><a title="riskmodels.univariate.EmpiricalWithBayesianGPTail.from_data" href="#riskmodels.univariate.EmpiricalWithBayesianGPTail.from_data">from_data</a></code></li>
<li><code><a title="riskmodels.univariate.EmpiricalWithBayesianGPTail.ppf" href="#riskmodels.univariate.EmpiricalWithBayesianGPTail.ppf">ppf</a></code></li>
<li><code><a title="riskmodels.univariate.EmpiricalWithBayesianGPTail.threshold" href="#riskmodels.univariate.EmpiricalWithBayesianGPTail.threshold">threshold</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="riskmodels.univariate.EmpiricalWithGPTail" href="#riskmodels.univariate.EmpiricalWithGPTail">EmpiricalWithGPTail</a></code></h4>
<ul class="two-column">
<li><code><a title="riskmodels.univariate.EmpiricalWithGPTail.empirical" href="#riskmodels.univariate.EmpiricalWithGPTail.empirical">empirical</a></code></li>
<li><code><a title="riskmodels.univariate.EmpiricalWithGPTail.exs_prob" href="#riskmodels.univariate.EmpiricalWithGPTail.exs_prob">exs_prob</a></code></li>
<li><code><a title="riskmodels.univariate.EmpiricalWithGPTail.from_data" href="#riskmodels.univariate.EmpiricalWithGPTail.from_data">from_data</a></code></li>
<li><code><a title="riskmodels.univariate.EmpiricalWithGPTail.plot_diagnostics" href="#riskmodels.univariate.EmpiricalWithGPTail.plot_diagnostics">plot_diagnostics</a></code></li>
<li><code><a title="riskmodels.univariate.EmpiricalWithGPTail.plot_return_levels" href="#riskmodels.univariate.EmpiricalWithGPTail.plot_return_levels">plot_return_levels</a></code></li>
<li><code><a title="riskmodels.univariate.EmpiricalWithGPTail.tail" href="#riskmodels.univariate.EmpiricalWithGPTail.tail">tail</a></code></li>
<li><code><a title="riskmodels.univariate.EmpiricalWithGPTail.threshold" href="#riskmodels.univariate.EmpiricalWithGPTail.threshold">threshold</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="riskmodels.univariate.GPTail" href="#riskmodels.univariate.GPTail">GPTail</a></code></h4>
<ul class="two-column">
<li><code><a title="riskmodels.univariate.GPTail.data" href="#riskmodels.univariate.GPTail.data">data</a></code></li>
<li><code><a title="riskmodels.univariate.GPTail.endpoint" href="#riskmodels.univariate.GPTail.endpoint">endpoint</a></code></li>
<li><code><a title="riskmodels.univariate.GPTail.fit" href="#riskmodels.univariate.GPTail.fit">fit</a></code></li>
<li><code><a title="riskmodels.univariate.GPTail.loglik" href="#riskmodels.univariate.GPTail.loglik">loglik</a></code></li>
<li><code><a title="riskmodels.univariate.GPTail.loglik_grad" href="#riskmodels.univariate.GPTail.loglik_grad">loglik_grad</a></code></li>
<li><code><a title="riskmodels.univariate.GPTail.loglik_hessian" href="#riskmodels.univariate.GPTail.loglik_hessian">loglik_hessian</a></code></li>
<li><code><a title="riskmodels.univariate.GPTail.logreg" href="#riskmodels.univariate.GPTail.logreg">logreg</a></code></li>
<li><code><a title="riskmodels.univariate.GPTail.logreg_grad" href="#riskmodels.univariate.GPTail.logreg_grad">logreg_grad</a></code></li>
<li><code><a title="riskmodels.univariate.GPTail.logreg_hessian" href="#riskmodels.univariate.GPTail.logreg_hessian">logreg_hessian</a></code></li>
<li><code><a title="riskmodels.univariate.GPTail.loss" href="#riskmodels.univariate.GPTail.loss">loss</a></code></li>
<li><code><a title="riskmodels.univariate.GPTail.loss_grad" href="#riskmodels.univariate.GPTail.loss_grad">loss_grad</a></code></li>
<li><code><a title="riskmodels.univariate.GPTail.loss_hessian" href="#riskmodels.univariate.GPTail.loss_hessian">loss_hessian</a></code></li>
<li><code><a title="riskmodels.univariate.GPTail.mle_cov" href="#riskmodels.univariate.GPTail.mle_cov">mle_cov</a></code></li>
<li><code><a title="riskmodels.univariate.GPTail.model" href="#riskmodels.univariate.GPTail.model">model</a></code></li>
<li><code><a title="riskmodels.univariate.GPTail.plot_diagnostics" href="#riskmodels.univariate.GPTail.plot_diagnostics">plot_diagnostics</a></code></li>
<li><code><a title="riskmodels.univariate.GPTail.scale" href="#riskmodels.univariate.GPTail.scale">scale</a></code></li>
<li><code><a title="riskmodels.univariate.GPTail.shape" href="#riskmodels.univariate.GPTail.shape">shape</a></code></li>
<li><code><a title="riskmodels.univariate.GPTail.threshold" href="#riskmodels.univariate.GPTail.threshold">threshold</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="riskmodels.univariate.GPTailMixture" href="#riskmodels.univariate.GPTailMixture">GPTailMixture</a></code></h4>
<ul class="two-column">
<li><code><a title="riskmodels.univariate.GPTailMixture.cdf" href="#riskmodels.univariate.GPTailMixture.cdf">cdf</a></code></li>
<li><code><a title="riskmodels.univariate.GPTailMixture.check_weigths" href="#riskmodels.univariate.GPTailMixture.check_weigths">check_weigths</a></code></li>
<li><code><a title="riskmodels.univariate.GPTailMixture.cvar" href="#riskmodels.univariate.GPTailMixture.cvar">cvar</a></code></li>
<li><code><a title="riskmodels.univariate.GPTailMixture.data" href="#riskmodels.univariate.GPTailMixture.data">data</a></code></li>
<li><code><a title="riskmodels.univariate.GPTailMixture.mean" href="#riskmodels.univariate.GPTailMixture.mean">mean</a></code></li>
<li><code><a title="riskmodels.univariate.GPTailMixture.moment" href="#riskmodels.univariate.GPTailMixture.moment">moment</a></code></li>
<li><code><a title="riskmodels.univariate.GPTailMixture.pdf" href="#riskmodels.univariate.GPTailMixture.pdf">pdf</a></code></li>
<li><code><a title="riskmodels.univariate.GPTailMixture.ppf" href="#riskmodels.univariate.GPTailMixture.ppf">ppf</a></code></li>
<li><code><a title="riskmodels.univariate.GPTailMixture.scales" href="#riskmodels.univariate.GPTailMixture.scales">scales</a></code></li>
<li><code><a title="riskmodels.univariate.GPTailMixture.shapes" href="#riskmodels.univariate.GPTailMixture.shapes">shapes</a></code></li>
<li><code><a title="riskmodels.univariate.GPTailMixture.std" href="#riskmodels.univariate.GPTailMixture.std">std</a></code></li>
<li><code><a title="riskmodels.univariate.GPTailMixture.thresholds" href="#riskmodels.univariate.GPTailMixture.thresholds">thresholds</a></code></li>
<li><code><a title="riskmodels.univariate.GPTailMixture.weights" href="#riskmodels.univariate.GPTailMixture.weights">weights</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="riskmodels.univariate.Mixture" href="#riskmodels.univariate.Mixture">Mixture</a></code></h4>
<ul class="two-column">
<li><code><a title="riskmodels.univariate.Mixture.cdf" href="#riskmodels.univariate.Mixture.cdf">cdf</a></code></li>
<li><code><a title="riskmodels.univariate.Mixture.check_weigths" href="#riskmodels.univariate.Mixture.check_weigths">check_weigths</a></code></li>
<li><code><a title="riskmodels.univariate.Mixture.distributions" href="#riskmodels.univariate.Mixture.distributions">distributions</a></code></li>
<li><code><a title="riskmodels.univariate.Mixture.mean" href="#riskmodels.univariate.Mixture.mean">mean</a></code></li>
<li><code><a title="riskmodels.univariate.Mixture.moment" href="#riskmodels.univariate.Mixture.moment">moment</a></code></li>
<li><code><a title="riskmodels.univariate.Mixture.pdf" href="#riskmodels.univariate.Mixture.pdf">pdf</a></code></li>
<li><code><a title="riskmodels.univariate.Mixture.ppf" href="#riskmodels.univariate.Mixture.ppf">ppf</a></code></li>
<li><code><a title="riskmodels.univariate.Mixture.simulate" href="#riskmodels.univariate.Mixture.simulate">simulate</a></code></li>
<li><code><a title="riskmodels.univariate.Mixture.std" href="#riskmodels.univariate.Mixture.std">std</a></code></li>
<li><code><a title="riskmodels.univariate.Mixture.weights" href="#riskmodels.univariate.Mixture.weights">weights</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>