<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>riskmodels.bivariate API documentation</title>
<meta name="description" content="This module contains bivariate risk models to analyse exceedance dependence between components. Available exceedance models are inspired in bivariate â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>riskmodels.bivariate</code></h1>
</header>
<section id="section-intro">
<p>This module contains bivariate risk models to analyse exceedance dependence between components. Available exceedance models are inspired in bivariate generalised Pareto models, whose support is an inverted L-shaped subset of Euclidean space, where at least one component takes an extreme value above a specified threshold. Available parametric models in this module include the logistic model, equivalent to a Gumbel-Hougaard copula between exceedances, and a Gaussian model, equivalent to a Gaussian copula. The former exhibits asymptotic dependence and the latter asymtptotic independence, which characterises the dependence of extremes across components.
Finally, <code><a title="riskmodels.bivariate.Empirical" href="#riskmodels.bivariate.Empirical">Empirical</a></code> instances have methods to assess asymptotic dependence vs independence through hypothesis tests and visual inspection of the Pickands dependence function.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;This module contains bivariate risk models to analyse exceedance dependence between components. Available exceedance models are inspired in bivariate generalised Pareto models, whose support is an inverted L-shaped subset of Euclidean space, where at least one component takes an extreme value above a specified threshold. Available parametric models in this module include the logistic model, equivalent to a Gumbel-Hougaard copula between exceedances, and a Gaussian model, equivalent to a Gaussian copula. The former exhibits asymptotic dependence and the latter asymtptotic independence, which characterises the dependence of extremes across components.
Finally, `Empirical` instances have methods to assess asymptotic dependence vs independence through hypothesis tests and visual inspection of the Pickands dependence function.
&#34;&#34;&#34;

from __future__ import annotations

import logging
import time
import typing as t
import traceback
import warnings
from abc import ABC, abstractmethod
from collections.abc import Iterable
import copy

import pandas as pd
import scipy as sp

import matplotlib.pyplot as plt
from matplotlib.colors import Normalize
from matplotlib import cm
import matplotlib

import numpy as np
import emcee

from scipy.optimize import LinearConstraint, minimize, root_scalar, approx_fprime
from scipy.special import lambertw
from scipy.stats import (
    genpareto as gpdist,
    expon as exponential,
    gumbel_r as gumbel,
    norm as gaussian,
    multivariate_normal as mv_gaussian,
    gaussian_kde,
    rv_continuous as continuous_dist
)

from pydantic import BaseModel, ValidationError, validator, PositiveFloat, Field
from functools import reduce

import riskmodels.univariate as univar

from riskmodels.utils.tmvn import TruncatedMVN as tmvn

class BaseDistribution(BaseModel, ABC):

    &#34;&#34;&#34;Base interface for bivariate distributions&#34;&#34;&#34;

    _allowed_scalar_types = (int, float, np.int64, np.int32, np.float32, np.float64)
    _figure_color_palette = [&#34;tab:cyan&#34;, &#34;deeppink&#34;]
    _error_tol = 1e-6

    data: t.Optional[np.ndarray] = None

    class Config:
        arbitrary_types_allowed = True

    def __repr__(self):
        return &#34;Base distribution object&#34;

    def __str__(self):
        return self.__repr__()

    @abstractmethod
    def pdf(self, x: np.ndarray) -&gt; float:
        &#34;&#34;&#34;Evaluate probability density function&#34;&#34;&#34;
        pass

    @abstractmethod
    def cdf(self, x: np.ndarray):
        &#34;&#34;&#34;Evaluate cumulative distribution function&#34;&#34;&#34;
        pass

    @abstractmethod
    def simulate(self, size: int):
        &#34;&#34;&#34;Simulate from bivariate distribution&#34;&#34;&#34;
        pass

    def plot(self, size: int = 1000) -&gt; matplotlib.figure.Figure:
        &#34;&#34;&#34;Sample distribution and produce scatterplots and histograms

        Args:
            size (int, optional): Sample size

        Returns:
            matplotlib.figure.Figure: figure
        &#34;&#34;&#34;
        sample = self.simulate(size)

        x = sample[:, 0]
        y = sample[:, 1]

        # definitions for the axes
        left, width = 0.1, 0.65
        bottom, height = 0.1, 0.65
        spacing = 0.005

        rect_scatter = [left, bottom, width, height]
        rect_histx = [left, bottom + height + spacing, width, 0.2]
        rect_histy = [left + width + spacing, bottom, 0.2, height]

        # start with a rectangular Figure
        fig = plt.figure(figsize=(8, 8))

        ax_scatter = plt.axes(rect_scatter)
        ax_scatter.tick_params(direction=&#34;in&#34;, top=True, right=True)
        ax_histx = plt.axes(rect_histx)
        ax_histx.tick_params(direction=&#34;in&#34;, labelbottom=False)
        ax_histy = plt.axes(rect_histy)
        ax_histy.tick_params(direction=&#34;in&#34;, labelleft=False)

        # the scatter plot:
        ax_scatter.scatter(x, y, color=self._figure_color_palette[0], alpha=0.35)

        # now determine nice limits by hand:
        # binwidth = 0.25
        # lim = np.ceil(np.abs([x, y]).max() / binwidth) * binwidth
        # ax_scatter.set_xlim((-lim, lim))
        # ax_scatter.set_ylim((-lim, lim))

        # bins = np.arange(-lim, lim + binwidth, binwidth)
        ax_histx.hist(
            x, bins=25, color=self._figure_color_palette[0], edgecolor=&#34;white&#34;
        )
        # plt.title(f&#34;Scatter plot from {np.round(size/1000,1)}K simulated samples&#34;)
        ax_histy.hist(
            y,
            bins=25,
            orientation=&#34;horizontal&#34;,
            color=self._figure_color_palette[0],
            edgecolor=&#34;white&#34;,
        )

        # ax_histx.set_xlim(ax_scatter.get_xlim())
        # ax_histy.set_ylim(ax_scatter.get_ylim())
        plt.tight_layout()
        return fig


class Frechet(object):
    &#34;&#34;&#34;Minimal implementation of a unit Frechet distribution&#34;&#34;&#34;
    @classmethod
    def cdf(cls, x: np.ndarray):
        return np.exp(-1/x)

    @classmethod
    def ppf(cls, p: np.ndarray):
        return -1.0/np.log(p)

class Mixture(BaseDistribution):

    &#34;&#34;&#34;Base interface for a bivariate mixture distribution&#34;&#34;&#34;

    distributions: t.List[BaseDistribution]
    weights: np.ndarray

    def __repr__(self):
        return f&#34;Mixture with {len(self.weights)} components&#34;

    def simulate(self, size: int) -&gt; np.ndarray:

        n_samples = np.random.multinomial(n=size, pvals=self.weights, size=1)[0]
        indices = (n_samples &gt; 0).nonzero()[0]
        samples = [
            dist.simulate(size=k)
            for dist, k in zip(
                [self.distributions[k] for k in indices], n_samples[indices]
            )
        ]
        samples = np.concatenate(samples, axis=0)
        np.random.shuffle(samples)
        return samples

    def cdf(self, x: np.ndarray, **kwargs) -&gt; float:
        vals = [
            w * dist.cdf(x, **kwargs)
            for w, dist in zip(self.weights, self.distributions)
        ]
        return reduce(lambda x, y: x + y, vals)

    def pdf(self, x: np.ndarray, **kwargs) -&gt; float:

        vals = [
            w * dist.pdf(x, **kwargs)
            for w, dist in zip(self.weights, self.distributions)
        ]
        return reduce(lambda x, y: x + y, vals)


class ExceedanceModel(Mixture):

    &#34;&#34;&#34;Interface for exceedance models. This is a mixture of an empirical distribution with support below the exceedance threshold and an exceedance distribution (see `ExceedanceDistribution`) above it.&#34;&#34;&#34;

    def __repr__(self):
        return f&#34;Sempirametric model with {self.tail.__class__.__name__} exceedance dependence&#34;

    def plot_diagnostics(self):

        return self.distributions[1].plot_diagnostics()

    @property
    def tail(self):
        return self.distributions[1]

    @property
    def empirical(self):
        return self.distributions[0]


class Independent(BaseDistribution):

    &#34;&#34;&#34;Bivariate distribution with independent components&#34;&#34;&#34;

    x: univar.BaseDistribution
    y: univar.BaseDistribution

    def __repr__(self):
        return f&#34;Independent bivariate distribution with marginals:\nx:{self.x.__repr__()}\ny:{self.y.__repr__()}&#34;

    def pdf(self, x: np.ndarray):
        x1, x2 = x
        return self.x.pdf(x1) * self.y.pdf(x2)

    def cdf(self, x: np.ndarray):
        x1, x2 = x
        return self.x.cdf(x1) * self.y.cdf(x2)

    def simulate(self, size: int):
        return np.concatenate(
            [
                self.x.simulate(size).reshape((-1, 1)),
                self.y.simulate(size).reshape((-1, 1)),
            ],
            axis=1,
        )


class ExceedanceDistribution(BaseDistribution):

    &#34;&#34;&#34;Main interface for exceedance distributions, which are defined on a region of the form \\( U \\nleq u \\), or equivalently \\( \\max\\{U_1,U_2\\} &gt; u \\).&#34;&#34;&#34;


    quantile_threshold: float

    margin1: t.Union[univar.BaseDistribution, continuous_dist]
    margin2: t.Union[univar.BaseDistribution, continuous_dist]

    # default method variables below are the same as for the logistic model
    # this does not matter as this class should not be instantiated directly
    params: t.Optional[np.ndarray] = Field(default_factory=lambda: np.zeros((1,), dtype=np.float32))
    _param_names = {0:&#34;alpha&#34;} #mapping from params array indices to names for diagnostic plots
    _model_marginal_dist = gumbel
    _plotting_dist_name = &#34;Gumbel&#34;
    _default_x0 = np.array([0.0])

    @validator(&#34;data&#34;, allow_reuse=True)
    def validate_data(cls, data):
        if data is not None and (len(data.shape) != 2 or data.shape[1] != 2 or np.any(np.isnan(data))):
            raise ValueError(&#34;Data is not an n x 2 numpy array&#34;)
        else:
            return data

    @classmethod
    @abstractmethod
    def unconditioned_cdf(cls, params: np.ndarray, data: np.ndarray):
        &#34;&#34;&#34;Computes the raw model cdf, without conditioning to the exceedance region.
        
        Args:
            params (np.ndarray): array with dependence parameter as only element
            data (t.Union[np.ndarray, t.Iterable]): Observed data in model scale
        &#34;&#34;&#34;
        pass

    @classmethod
    @abstractmethod
    def logpdf(
        cls, params: np.ndarray, threshold: float, data: t.Union[np.ndarray, t.Iterable]
    ):
        &#34;&#34;&#34;Calculates logpdf function for exceedance distribution
        
        
        Args:
            params (np.ndarray): array with model parameters
            threshold (float): Exceedance threshold in model scale
            data (t.Union[np.ndarray, t.Iterable]): Observed data in model scale
        &#34;&#34;&#34;
        pass

    @property
    def model_scale_threshold(self):
        return self._model_marginal_dist.ppf(self.quantile_threshold)

    @property
    def data_scale_threshold(self):
        return self.model_to_data_dist(
            self.bundle(self.model_scale_threshold, self.model_scale_threshold)
        )

    @property
    def mle_cov(self):
        return -np.linalg.inv(
            self.hessian(
                self.params, 
                self.model_scale_threshold, 
                self.data_to_model_dist(self.data)))
    
    @classmethod
    def unbundle(
        cls, data: t.Union[np.ndarray, t.Iterable]
    ) -&gt; t.Tuple[t.Union[np.ndarray, float], t.Union[np.ndarray, float]]:
        &#34;&#34;&#34;Unbundles matrix or iterables into separate components

        Args:
            data (t.Union[np.ndarray, t.Iterable]): dara

        &#34;&#34;&#34;
        if isinstance(data, np.ndarray) and len(data.shape) == 2 and data.shape[1] == 2:
            x = data[:, 0]
            y = data[:, 1]
        elif isinstance(data, Iterable):
            # if iterable, unroll
            x, y = data
        else:
            raise TypeError(
                &#34;data must be an n x 2 numpy array or an iterable of length 2.&#34;
            )
        return x, y

    @classmethod
    def bundle(
        cls, x: t.Union[np.ndarray, float, int], y: t.Union[np.ndarray, float, int]
    ) -&gt; t.Tuple[t.Union[np.ndarray, float], t.Union[np.ndarray, float]]:
        &#34;&#34;&#34;bundle a pair of arrays or primitives into n x 2 matrix

        Args:
            data (t.Union[np.ndarray, t.Iterable])

        &#34;&#34;&#34;
        if isinstance(x, np.ndarray) and isinstance(y, np.ndarray) and len(x) == len(y):
            z = np.concatenate([x.reshape((-1, 1)), y.reshape((-1, 1))], axis=1)
        elif issubclass(type(x), (float, int)) and issubclass(type(y), (float, int)):
            z = np.array([x, y]).reshape((1, 2))
        else:
            raise TypeError(
                &#34;x, y must be 1-dimensional arrays or inherit from float or int.&#34;
            )
        return z

    def data_to_model_dist(self, data: t.Union[np.ndarray, t.Iterable]) -&gt; np.ndarray:
        &#34;&#34;&#34;Transforms original data scale to standard Gumbel scale

        Args:
            data (t.Union[np.ndarray, t.Iterable]): observations in original scale

        &#34;&#34;&#34;
        x, y = self.unbundle(data)

        ## to copula scale
        x = self.margin1.cdf(x)
        y = self.margin2.cdf(y)

        # pass to Gumbel scale
        x = self._model_marginal_dist.ppf(x)
        y = self._model_marginal_dist.ppf(y)

        return self.bundle(x, y)

    def model_to_data_dist(self, data: t.Union[np.ndarray, t.Iterable]) -&gt; np.ndarray:
        &#34;&#34;&#34;Transforms data in standard Gumbel scale to original data scale

        Args:
            x (np.ndarray): data from first component
            y (np.ndarray): data from second component

        Returns:
            np.ndarray
        &#34;&#34;&#34;

        # copula scale
        x, y = self.unbundle(data)

        u = self._model_marginal_dist.cdf(x)
        w = self._model_marginal_dist.cdf(y)

        # data scale
        u = self.margin1.ppf(u)
        w = self.margin2.ppf(w)

        return self.bundle(u, w)

    def simulate(self, size: int):

        return self.model_to_data_dist(self.simulate_model(size, self.params, self.quantile_threshold))

    @classmethod
    def loglik(
        cls, params: np.ndarray, threshold: float, data: t.Union[np.ndarray, t.Iterable]
    ):
        &#34;&#34;&#34;Computes log-likelihood for threshold exceedances&#34;&#34;&#34;
        return np.sum(cls.logpdf(params, threshold, data))

    def cdf(self, x: np.ndarray):
        mapped_data = self.data_to_model_dist(x)
        model_threshold = self.model_scale_threshold
        u = np.minimum(mapped_data, model_threshold)
        norm_factor = float(
            1
            - self.unconditioned_cdf(
                self.params, self.bundle(model_threshold, model_threshold)
            )
        )

        return (
            self.unconditioned_cdf(self.params, mapped_data)
            - self.unconditioned_cdf(self.params, u)
        ) / norm_factor

    def pdf(self, x: np.ndarray, eps = 1e-5) -&gt; np.ndarray:
        &#34;&#34;&#34;Numerical approximation to the model&#39;s pdf in the original data scale. This is only non-zero when both marginal distributions are continuous on x
        
        Args:
            x (np.ndarray): Points to evaluate
            eps (float, optional): Numeric delta
        
        Returns:
            np.ndarray: pdf approximation
        &#34;&#34;&#34;
        x1, x2 = self.unbundle(x)
        model_scale_data = self.data_to_model_dist(x)
        n = len(model_scale_data)

        e1, e2 = (np.stack([np.ones((n,), dtype=np.float32), np.zeros((n,), dtype=np.float32)], axis=1),
            np.stack([np.zeros((n,), dtype=np.float32), np.ones((n,), dtype=np.float32)], axis=1))

        dz1_dx1 = (self.data_to_model_dist(x + eps*e1)[:,0] - self.data_to_model_dist(x - eps*e1)[:,0])/(2*eps)
        dz2_dx2 = (self.data_to_model_dist(x + eps*e2)[:,1] - self.data_to_model_dist(x - eps*e2)[:,1])/(2*eps)

        return (
            np.exp(self.logpdf(self.params, self.quantile_threshold, model_scale_data))
            * dz1_dx1 * dz2_dx2
        )

    @classmethod
    def fit(
        cls,
        data: t.Union[np.ndarray, t.Iterable],
        quantile_threshold: float,
        margin1: univar.BaseDistribution = None,
        margin2: univar.BaseDistribution = None,
        return_opt_results=False,
        x0: float = None,
    ) -&gt; Logistic:
        &#34;&#34;&#34;Fits the model from provided data, threshold and marginal distributons

        Args:
            data (t.Union[np.ndarray, t.Iterable]): input data
            quantile_threshold (float): Description: quantile threshold over which observations are classified as extreme
            margin1 (univar.BaseDistribution, optional): Marginal distribution for first component
            margin2 (univar.BaseDistribution, optional): Marginal distribution for second component
            return_opt_results (bool, optional): If True, the object from the optimization result is returned
            x0 (float, optional): Initial point for the optimisation algorithm. Defaults to 0.5

        Returns:
            Logistic: Fitted model


        &#34;&#34;&#34;
        if margin1 is None:
            margin1 = univar.empirical.from_data(data[:, 0])
            warnings.warn(
                &#34;margin1 is None; using an empirical distribution&#34;, stacklevel=2
            )

        if margin2 is None:
            margin1 = univar.empirical.from_data(data[:, 1])
            warnings.warn(
                &#34;margin1 is None; using an empirical distribution&#34;, stacklevel=2
            )

        if (
            not isinstance(quantile_threshold, float)
            or quantile_threshold &lt;= 0
            or quantile_threshold &gt;= 1
        ):
            raise ValueError(&#34;quantile_threshold must be in the open interval (0,1)&#34;)

        mapped_data = cls(
            margin1=margin1,
            margin2=margin2,
            quantile_threshold=quantile_threshold,
        ).data_to_model_dist(data)

        x, y = cls.unbundle(mapped_data)

        # get threshold exceedances
        model_scale_threshold = cls._model_marginal_dist.ppf(quantile_threshold)

        exs_idx = np.logical_or(x &gt; model_scale_threshold, y &gt; model_scale_threshold)
        x = x[exs_idx]
        y = y[exs_idx]

        x0 = cls._default_x0 if x0 is None else x0

        mapped_exceedances = cls.bundle(x, y)
        n = len(mapped_exceedances)

        def logistic(x):
            return 1.0 / (1 + np.exp(-x))

        def loss(x, data):
            params = logistic(x)
            # optimise normalised log-likelihood
            return -cls.loglik(params, model_scale_threshold, data)/n

        res = minimize(fun=loss, x0=x0, method=&#34;BFGS&#34;, args=(mapped_exceedances,))

        if return_opt_results:
            return res

        return cls(
            quantile_threshold=quantile_threshold,
            params=logistic(res.x),
            data=data[exs_idx, :],
            margin1=margin1,
            margin2=margin2,
        )

    @classmethod
    def hessian(
        cls, 
        params: np.ndarray, 
        threshold: float, 
        data: t.Union[np.ndarray, t.Iterable],
        eps = 1e-6) -&gt;np.ndarray:
        &#34;&#34;&#34;Numerical approximation for the loglikelihood function&#39;s Hessian
        
        Args:
            params (np.ndarray): parameter array
            threshold (float): Threshold in standard scale (i.e. Gumbel or Gaussian)
            data (t.Union[np.ndarray, t.Iterable]): Data in standard scale (i.e. Gumbel or Gaussian)
            eps (float, optional): Numerical delta in each component
        
        Returns:
            np.ndarray: Hessian matrix
        
        &#34;&#34;&#34;
        x0 = params
        grad0 = approx_fprime(x0, cls.loglik, eps, threshold, data) 
        n = len(x0)
        hessian = np.zeros((n,n), dtype=np.float32)
        # The next loop fill in the matrix
        xx = x0
        for j in range( n ):
            xx0 = xx[j] # Store old value
            xx[j] = xx0 + eps # Perturb with finite difference
            # Recalculate the partial derivatives for this new point
            current_grad = approx_fprime(xx, cls.loglik, eps, threshold, data) 
            hessian[:, j] = (current_grad - grad0)/eps # scale...
            xx[j] = xx0 # Restore initial value of x0        
        return hessian

    def plot_diagnostics(self, eps=1e-6):
        &#34;&#34;&#34;Returns a figure with the fitted exceedance model&#39;s profile log-likelihoods and fitted density in model scale.
        
        Args:
            eps (float, optional): numeric delta when calculating the Hessian matrix numerically; this is used to plot profile loglikelihoods.
        
        &#34;&#34;&#34;
        # unbundle data
        if self.data is None:
            raise ValueError(&#34;Diagnostics cannot be shown for models with no underlying data.&#34;)

        x, y = self.unbundle(self.data)
        z1, z2 = self.unbundle(self.data_to_model_dist(self.data))
        n = len(z1)
        params = self.params

        # create plot mosaic
        n_params = len(params)
        r, c = int(np.ceil(n_params/2 + 1/2)), 2
        # this function is needed to handle mosaics with a varying number of rows
        def get_subplot(k):
            if r == 1:
                return axs[k]
            else:
                return axs[k//2,k%2]
        fig, axs = plt.subplots(r,c)

        # compute mle sdev
        model_scale_threshold = self.model_scale_threshold
        try:
            hessian = self.hessian(params, model_scale_threshold, self.bundle(z1, z2), eps)
            sdevs = np.sqrt(-np.linalg.inv(hessian))
        except Exception as e:
            raise Exception(f&#34;Error when estimating fitted parameter variances; if fitted parameters are at the edge of their support, passing a lower eps value might help. Full trace: {traceback.format_exc()}&#34;)
        # create profile log-likelihood plots
        for k in range(n_params):
            param = params[k]
            sdev = sdevs[k,k]
            grid = np.linspace(param - 1.96*sdev, param + 1.96*sdev, 100)
            feasible_grid = grid[np.logical_and(grid &gt; 0, grid &lt; 1)]
            perturbed_params = np.copy(params)
            profile_ll = []
            for x in feasible_grid:
                perturbed_params[k] = x
                profile_ll.append(self.loglik(perturbed_params, model_scale_threshold, self.bundle(z1, z2)))
            # filter to almost optimal values
            profile_ll = np.array(profile_ll)
            max_ll = max(profile_ll)
            almost_optimal = np.abs(profile_ll - max_ll) &lt; np.abs(2 * max_ll)
            profile_ll = profile_ll[almost_optimal]
            feasible_grid = feasible_grid[almost_optimal]
            get_subplot(k).plot(feasible_grid, profile_ll, color=self._figure_color_palette[0])
            get_subplot(k).vlines(
                x=param,
                ymin=min(profile_ll),
                ymax=max(profile_ll),
                linestyle=&#34;dashed&#34;,
                colors=self._figure_color_palette[1],
            )
            get_subplot(k).title.set_text(&#34;Profile log-likelihood&#34;)
            get_subplot(k).set_xlabel(self._param_names.get(k, f&#34;params[{k}]&#34;))
            get_subplot(k).set_ylabel(&#34;&#34;)
            get_subplot(k).grid()

        ### last subplot contains the fitted density in model scale
        # avoid plotting in Frechet scale as it makes it difficult to see anything
        if isinstance(self._model_marginal_dist, Frechet):
            # convert Frechet data to Gumbel
            z1 = np.log(z1)
            z2 = np.log(z2)
            dist_name = &#34;Gumbel&#34;
            logpdf = Logistic.logpdf
            contour_dist_threshold = np.log(model_scale_threshold)
        else:
            dist_name = self._plotting_dist_name
            logpdf = self.logpdf
            contour_dist_threshold = model_scale_threshold

        x_range = np.linspace(np.min(z1), np.max(z1), 50)
        y_range = np.linspace(np.min(z2), np.max(z2), 50)

        X, Y = np.meshgrid(x_range, y_range)
        bundled_grid = self.bundle(X.reshape((-1, 1)), Y.reshape((-1, 1)))
        Z = logpdf(
            data=bundled_grid, threshold=contour_dist_threshold, params=params
        ).reshape(X.shape)
        get_subplot(-1).contourf(X, Y, Z)
        get_subplot(-1).scatter(z1, z2, color=self._figure_color_palette[1], s=0.9)
        get_subplot(-1).title.set_text(f&#34;Model density ({dist_name} scale)&#34;)
        get_subplot(-1).set_xlabel(&#34;x&#34;)
        get_subplot(-1).set_ylabel(&#34;y&#34;)

        plt.tight_layout()
        return fig



class Logistic(ExceedanceDistribution):

    &#34;&#34;&#34;This model assumes association between exceedances at different components follow a Gumbel-Hougaard copula; in Gumbel scale, this is given by
    
    $$ \\mathbb{P}(\\textbf{X} \\leq \\mathbf{x}) = \\exp \\left(- \\left(  \\exp \\left( - \\frac{\\mathbf{x}_1}{\\alpha} \\right)+ \\left(  - \\frac{\\mathbf{x}_2}{\\alpha}  \\right) \\right)^\\alpha \\right), \\, 0 \\leq \\alpha\\leq 1, \\, \\mathbf{X} \\in \\mathbb{R}^2$$

    Exceedances in each component are defined as observations above a fixed quantile threshold \\( \\textbf{q}\\) for a high probability level \\(p \\approx 1\\), and so bivariate exceedances \\(\\textbf{Z}\\) are defined in an inverted-L-shaped region of space, \\( \\textbf{Z} \\nleq \\mathbf{q} \\): that in which there is an exceedance in at least one component. Consequently the model implemented here is only defined in the corresponding inverted-L-shaped region; the functional form of the dependence is the same as a Gumbel-Hougaard copula, but the normalisation constant is different because of this constraint.

    If the underlying marginal distributions follow a generalised Pareto above the quantile thresholds, this model can be seen as a pre-limit version of a bivariate generalised Pareto distribution with a logistic dependence model (see Rootzen and Tajvidi, 2006), to which this model converges as the quantile thresholds grow.
    &#34;&#34;&#34;
    params: t.Optional[np.ndarray] = Field(default_factory=lambda: np.zeros((1,), dtype=np.float32))

    _param_names = {0:&#34;alpha&#34;} #mapping from params array indices to names for diagnostic plots

    _model_marginal_dist = gumbel
    #_plotting_dist = gumbel
    _plotting_dist_name = &#34;Gumbel&#34;
    _default_x0 = np.array([0.0])

    def __repr__(self):
        return f&#34;{self.__class__.__name__} exceedance dependence model with alpha = {self.alpha} and quantile threshold {self.quantile_threshold}&#34;

    @validator(&#34;params&#34;)
    def validate_params(cls, params):
        alpha = params[0]
        if alpha &lt;= 0 or alpha &gt; 1:
            raise TypeError(f&#34;alpha must be in the interval (0,1]&#34;)
        else:
            return params

    @property
    def alpha(self):
        return self.params[0]


    @classmethod
    def logpdf(
        cls, params: np.ndarray, threshold: float, data: t.Union[np.ndarray, t.Iterable]
    ):
        &#34;&#34;&#34;Calculates logpdf function for Gumbel exceedances
        
        
        Args:
            params (np.ndarray): array with dependence parameter as only element
            threshold (float): Exceedance threshold in Gumbel scale
            data (t.Union[np.ndarray, t.Iterable]): Observed data in Gumbel scale
        
        &#34;&#34;&#34;
        alpha = params[0]

        x, y = cls.unbundle(data)

        nlogp = (np.exp(-x / alpha) + np.exp(-y / alpha)) ** alpha
        lognlogp = alpha * np.log(np.exp(-x / alpha) + np.exp(-y / alpha))
        rescaler = 1 - cls.unconditioned_cdf(params, cls.bundle(threshold, threshold))

        # a = np.exp((x + y - nlogp*alpha)/alpha)
        log_a = (x + y) / alpha - nlogp

        # b = nlogp
        log_b = lognlogp

        # c = 1 + alpha*(nlogp - 1)
        log_c = np.log(1 + alpha * (nlogp - 1))

        # d = 1.0/(alpha*(np.exp(x/alpha) + np.exp(y/alpha))**2)
        log_d = -(np.log(alpha) + 2 * np.log(np.exp(x / alpha) + np.exp(y / alpha)))

        log_density = log_a + log_b + log_c + log_d - np.log(rescaler)

        # density is 0 when both coordinates are below the threshold
        nil_density_idx = np.logical_and(x &lt;= threshold, y &lt;= threshold)
        log_density[nil_density_idx] = -np.Inf

        return log_density

    @classmethod
    def unconditioned_cdf(cls, params: np.ndarray, data: t.Union[np.ndarray, t.Iterable]):
        &#34;&#34;&#34;Calculates unconstrained standard Gumbel CDF&#34;&#34;&#34;
        alpha = params[0]
        x, y = cls.unbundle(data)
        return np.exp(-((np.exp(-x / alpha) + np.exp(-y / alpha)) ** (alpha)))           

    @classmethod
    def simulate_model(cls, size: int, params: np.ndarray, quantile_threshold: float) -&gt; np.ndarray:
        &#34;&#34;&#34;Simulates logistic exceedance model in Gumbel scale
        
        Args:
            size (int): Simulated sample size
            params (np.ndarray): Array with dependence parameter
            threshold (float): Exceedance threshold in both components
        
        Returns:
            np.ndarray: Simulated sample
        &#34;&#34;&#34;
        ### simulate in Gumbel scale maximum component: z = max(x1, x2) ~ Gumbel(loc=alpha*np.log(2)) using inverse function method
        threshold = gumbel.ppf(quantile_threshold)
        alpha = params[0]

        q0 = gumbel.cdf(threshold, loc=alpha * np.log(2))  # quantile of model&#39;s threshold in the maximum&#39;s distribution
        u = np.random.uniform(size=size, low=q0)
        maxima = gumbel.ppf(q=u, loc=alpha * np.log(2))

        if alpha == 0:
            r = 0
        elif alpha == 1:
            r = np.log(exponential.rvs(size=size, loc=1, scale = np.exp(maxima)))
        else:
            ###simulate difference between maxima and minima r = max(x,y) - min(x,y) using inverse function method
            u = np.random.uniform(size=size)
            r = (
                alpha
                * np.log(
                    (
                        -(
                            (alpha - 1)
                            * np.exp(maxima)
                            * lambertw(
                                -(
                                    np.exp(
                                        -maxima
                                        - (2 ** alpha * np.exp(-maxima) * alpha)
                                        / (alpha - 1)
                                    )
                                    * (-(2 ** (alpha - 1)) * (u - 1))
                                    ** (alpha / (alpha - 1))
                                    * alpha
                                )
                                / (alpha - 1)
                            )
                        )
                        / alpha
                    )
                    ** (1 / alpha)
                    - 1
                )
            ).real

        minima = maxima - r

        # allocate maxima randomly between components
        max_indices = np.random.binomial(1, 0.5, size)

        x = np.concatenate(
            [
                maxima[max_indices == 0].reshape((-1, 1)),
                minima[max_indices == 1].reshape((-1, 1)),
            ],
            axis=0,
        )

        y = np.concatenate(
            [
                minima[max_indices == 0].reshape((-1, 1)),
                maxima[max_indices == 1].reshape((-1, 1)),
            ],
            axis=0,
        )

        return cls.bundle(x,y)


class Gaussian(ExceedanceDistribution):

    &#34;&#34;&#34;This model assumes association between exceedances at different components follow a Gaussian copula. Exceedances in each component are defined as observations above a fixed quantile threshold \\( \\textbf{q}\\) for a high probability level \\(p \\approx 1\\), and so bivariate exceedances \\(\\textbf{Z}\\) are defined in an inverted-L-shaped region of space, \\( \\textbf{Z} \\nleq \\mathbf{q} \\): that in which there is an exceedance in at least one component. Consequently this copula model is only defined in the corresponding inverted-L-shaped region in \\( [\\textbf{0}, \\textbf{1}]\\); the functional form is the same as a Gaussian copula, but the normalisation constant is different.

    If the underlying marginal distributions follow a generalised Pareto above the quantile thresholds, this model can be seen as a pre-limit version of a bivariate generalised Pareto distribution (see Rootzen and Tajvidi, 2006) with a Gaussian dependence model. Because Gaussian copulas are asymptotically independent (this is, dependence  weakens at progressively more extreme levels regardless of the correlation parameter, and disappears in the limit), said limiting model is degenerate, with probability mass at \\(-\\infty\\). This pre-limit model on the other hand is non-degenerate and can be used to model asymptotically independent data.
    &#34;&#34;&#34;

    _model_marginal_dist = gaussian
    #_plotting_dist = gaussian
    _plotting_dist_name = &#34;Gaussian&#34;
    _param_names = {0:&#34;rho&#34;} #mapping from params array indices to names for diagnostic plots

    def __repr__(self):
        return f&#34;{self.__class__.__name__} exceedance dependence model with rho = {self.params} and quantile threshold {self.quantile_threshold}&#34;

    @validator(&#34;params&#34;)
    def validate_params(cls, params):
        rho = params[0]
        if rho &lt; 0 or rho &gt;= 1:
            raise TypeError(f&#34;rho must be in the interval [0,1)&#34;)
        else:
            return params

    @property
    def cov(self):
        rho = self.params[0]
        return np.array([[1, rho], [rho, 1]])

    @property
    def rho(self):
        return self.params[0]
    
    @classmethod
    def unconditioned_cdf(cls, params: np.ndarray, data: np.ndarray):
        rho = params[0]
        &#34;&#34;&#34;Calculates unconstrained standard Gaussian CDF&#34;&#34;&#34;
        return mv_gaussian.cdf(data, cov=np.array([[1, rho], [rho, 1]]))

    @classmethod
    def logpdf(
        cls, params: np.ndarray, threshold: float, data: t.Union[np.ndarray, t.Iterable]
    ):
        &#34;&#34;&#34;Calculates logpdf for Gaussian exceedances&#34;&#34;&#34;
        rho = params[0]
        x, y = cls.unbundle(data)
        if isinstance(rho, (list, np.ndarray)):
            rho = rho[0]
        norm_factor = 1 - mv_gaussian.cdf(
            cls.bundle(threshold, threshold), cov=np.array([[1, rho], [rho, 1]])
        )
        density = mv_gaussian.logpdf(
            data, cov=np.array([[1, rho], [rho, 1]])
        ) - np.log(norm_factor)

        # density is 0 when both coordinates are below the threshold
        nil_density_idx = np.logical_and(x &lt;= threshold, y &lt;= threshold)
        density[nil_density_idx] = -np.Inf

        return density

    @classmethod
    def simulate_model(cls, size: int, params: np.ndarray, quantile_threshold: float) -&gt; np.ndarray:
        &#34;&#34;&#34;Simulate Gaussian exceedance model in Gaussian scale
        
        Args:
            size (int): Simulated sample size
            params (np.ndarray): Array with dependence parameter
            threshold (float): Exceedance threshold in both components
        
        Returns:
            np.ndarray: Simulated sample
        &#34;&#34;&#34;

        # exceedance subregions:
        # r1 =&gt; exceedance in second component only, r2 =&gt; exceedance in both components, r3 =&gt; exceedance in first component only
        rho = params[0]
        cov = np.array([[1, rho], [rho, 1]])

        if quantile_threshold == 0:
            samples = mv_gaussian.rvs(size=size, cov=cov)
        else:
            threshold = gaussian.ppf(quantile_threshold)

            th = cls.bundle(threshold, threshold)
            th_cdf = mv_gaussian.cdf(th, cov=cov) 

            p1 = quantile_threshold - th_cdf
            p2 = 1 - 2 * quantile_threshold + th_cdf
            p3 = 1 - th_cdf - (p1 + p2)

            p = np.array([p1, p2, p3])
            p = p / np.sum(p)

            # compute number of samples per subregion
            n1, n2, n3 = np.random.multinomial(n=size, pvals=p, size=1)[0].astype(np.int32)
            n1, n2, n3 = int(n1), int(n2), int(n3)

            r1_samples = (
                tmvn(
                    mu=np.zeros((2,)),
                    cov=cov,
                    lb=np.array([-np.Inf, threshold]),
                    ub=np.array([threshold, np.Inf]),
                )
                .sample(n1)
                .T
            )

            r2_samples = (
                tmvn(
                    mu=np.zeros((2,)),
                    cov=cov,
                    lb=np.array([threshold, threshold]),
                    ub=np.array([np.Inf, np.Inf]),
                )
                .sample(n2)
                .T
            )

            r3_samples = (
                tmvn(
                    mu=np.zeros((2,)),
                    cov=cov,
                    lb=np.array([threshold, -np.Inf]),
                    ub=np.array([np.Inf, threshold]),
                )
                .sample(n3)
                .T
            )

            samples = np.concatenate([r1_samples, r2_samples, r3_samples], axis=0)

        return samples


class AsymmetricLogistic(ExceedanceDistribution):

    &#34;&#34;&#34;This model assumes association between exceedances at different components follow a copula induced by an asymmetric logistic model of extremal dependence; this model in unit Frechet scale is given by 
    
    $$ \\mathbb{P}(\\textbf{X} \\leq \\mathbf{x}) = \\exp \\left( - \\frac{1 - \\beta}{\\mathbf{x}_1} - \\frac{1 - \\gamma}{\\mathbf{x}_2} - \\left(  \\left( \\frac{\\beta}{\\mathbf{x}_1} \\right)^{1/\\alpha} + \\left( \\frac{\\gamma}{\\mathbf{x}_2} \\right)^{1/\\alpha}\\right)^\\alpha \\right), \\, 0 \\leq \\alpha, \\beta, \\gamma \\leq 1, \\, \\mathbf{X} &gt; 0$$

    Exceedances in each component are defined as observations above a fixed quantile threshold \\( \\textbf{q}\\) for a high probability level \\(p \\approx 1\\), and so bivariate exceedances \\(\\textbf{Z}\\) are defined in an inverted-L-shaped region of space, \\( \\textbf{Z} \\nleq \\mathbf{q} \\): that in which there is an exceedance in at least one component. Consequently the model implemented here is only defined in the corresponding inverted-L-shaped region.

    If the underlying marginal distributions follow a generalised Pareto above the quantile thresholds, this model can be seen as a pre-limit version of a bivariate generalised Pareto distribution (see Rootzen and Tajvidi, 2006) with an asymmetric logistic dependence model, to which this model converges as the quantile threshold grows.
    &#34;&#34;&#34;

    params: t.Optional[np.ndarray] = Field(default_factory=lambda: np.zeros((3,), dtype=np.float32))

    _param_names = {0:&#34;alpha&#34;, 1:&#34;beta&#34;, 2:&#34;gamma&#34;} #mapping from params array indices to names for diagnostic plots
    _default_x0 = np.array([0,0,0])

    _model_marginal_dist = Frechet()
    
    #_plotting_dist = Frechet
    _plotting_dist_name = &#34;Frechet&#34;

    def __repr__(self):
        return f&#34;{self.__class__.__name__} exceedance dependence model with (alpha, beta, gamma) = {self.params} and quantile threshold {self.quantile_threshold}&#34;

    @validator(&#34;params&#34;)
    def validate_params(cls, params):
        alpha, beta, gamma = params
        if alpha &lt;= 0 or alpha &gt; 1:
            raise TypeError(f&#34;alpha must be in the interval (0,1]&#34;)
        if (beta &lt; 0 or beta &gt; 1) or (gamma &lt; 0 or gamma &gt; 1):
            raise TypeError(f&#34;beta, gamma must be in the interval [0,1]&#34;)
        else:
            return params

    @classmethod
    def logpdf(
        cls, params: np.ndarray, threshold: float, data: t.Union[np.ndarray, t.Iterable]
    ):
        &#34;&#34;&#34;Calculates logpdf function for asymmetric logistic Frechet exceedances
        
        
        Args:
            params (np.ndarray): array with dependence and asymmetry parameters
            threshold (float): Exceedance threshold in Gumbel scale
            data (t.Union[np.ndarray, t.Iterable]): Observed data in Frechet scale
        
        &#34;&#34;&#34;
        alpha, beta, gamma = params

        x, y = cls.unbundle(data)
        rescaler = 1 - cls.unconditioned_cdf(params, cls.bundle(threshold, threshold))

        alpha_prenorm = (beta/x)**(1/alpha) + (gamma/y)**(1/alpha)
        alpha_norm = (alpha_prenorm)**alpha

        log_a = (-1+beta)/x + (-1+gamma-y*alpha_norm)/y
        log_b = -(2*np.log(x) + 2*np.log(y) + np.log(alpha))

        c_1 = -x*y*(-1+alpha)*(beta*gamma/(x*y))**(1/alpha)*alpha_prenorm**(-2+alpha)
        c_2 = alpha * (1 - beta + x*(beta/x)**(1/alpha)*alpha_prenorm**(-1+alpha))*(1 - gamma + y*(gamma/y)**(1/alpha)*alpha_prenorm**(-1+alpha))
        log_c = np.log(c_1+c_2)

        log_density = log_a + log_b + log_c - np.log(rescaler)

        # density is 0 when both coordinates are below the threshold
        nil_density_idx = np.logical_and(x &lt;= threshold, y &lt;= threshold)
        log_density[nil_density_idx] = -np.Inf

        return log_density

    @classmethod
    def unconditioned_cdf(cls, params: np.ndarray, data: t.Union[np.ndarray, t.Iterable]) -&gt; np.ndarray:
        &#34;&#34;&#34;Calculates unconstrained standard Frechet cdf with asymmetric logistic dependence
        
        Args:
            params (np.ndarray): array with dependence and asymmetry parameters
            data (t.Union[np.ndarray, t.Iterable]): Observed data in Frechet scale
        
        Returns:
            np.ndarray: cdf values
        &#34;&#34;&#34;
        x, y = cls.unbundle(data)
        alpha, beta, gamma = params
        return np.exp(-(1-beta)/x - (1-gamma)/y - ((beta/x)**(1/alpha) + (gamma/y)**(1/alpha))**alpha)

    def simulate_model(self, size: int, params: np.ndarray, quantile_threshold: float) -&gt; np.ndarray:
        &#34;&#34;&#34;Simulate asymmetric logistic exceedance model variates in Frechet scale using a method inspired by the one outlined in &#39;Simulating Multivariate Extreme Value Distributions of Logistic Type&#39; by Stephenson (2003).
        
        Args:
            size (int): Sample size
            params (np.ndarray): array with dependence and asymmetry parameters
            quantile_threshold (float): Quantile threshold for simulated exceedances
        
        Returns:
            np.ndarray: Simulated sample
        &#34;&#34;&#34;

        # Gumbel scales are used instead of Frechet in this method, and a the existing logistic model is used as a baseline sampler
        # in Gumbel scales asymmetry constants are offsets instead of rescaling factors
        gumbel_logistic = Logistic.simulate_model
        threshold = gumbel.ppf(quantile_threshold)
        #threshold from which we want to sample
        target_th = np.array([threshold, threshold]).reshape((1,2))
        # unpack params
        alpha, beta, gamma = params

        def logistic_exs(size: int, alpha: float, a: float, b: float) -&gt; np.ndarray:
            &#34;&#34;&#34;
            Samples a non-deterministic number of exceedances from a logistic exceedance model. A smaller , symmetric proxy threshold is used to simulate variates that are later offset to match the target threshold. Because the proxy threshold is symmetric but the target threshold may not be, some samples may be discarded. The expected sample size is the passed size parameter. The target threshold value comes from outer scope.
            
            Args:
                size (int): Sample size
                alpha (float): Dependence parameter
                a (float): asymmetry parameter for first component
                b (float): Asymmetry parameter for second component
            
            Returns:
                np.ndarray: Simulated sample
            
            &#34;&#34;&#34;
            alpha_param = np.array([alpha])
            ## offset from asymmetry parameters
            offset = np.array([np.log(a), np.log(b)]).reshape((1,2))
            offset_th = target_th - offset 
            #compute lower symmetric threshold to use as a proxy when sampling (some samples may be dropped)
            min_offset = np.min(offset_th)
            effective_th = min_offset * np.ones((2,)).reshape((1,2)) 
            effective_q = gumbel.cdf(min_offset)
            # compute average sample drop rate
            s_effective = 1 - Logistic.unconditioned_cdf(alpha_param, effective_th)
            s_offset = 1 - Logistic.unconditioned_cdf(alpha_param, offset_th)
            drop_rate = (s_effective - s_offset)/s_effective        
            if drop_rate &gt;= 1 or drop_rate &lt; 0:
                raise Exception(f&#34;Invalid drop rate ({drop_rate})&#34;)
            effective_size = int(size/(1-drop_rate))
            # sample from logistic model
            b2 = gumbel_logistic(effective_size, alpha_param, effective_q)
            # skew to symmetric logistic parameters
            b2[:,0] += np.log(a)
            b2[:,1] += np.log(b)
            # drop samples outside target region
            target_th1, target_th2 = target_th.reshape(-1)
            exs_idx = np.logical_or(b2[:,0] &gt; target_th1, b2[:,1] &gt; target_th2)
            b2 = b2[exs_idx,:]
            return b2

        def logistic_non_exs(size: int, alpha: float, a: float, b: float) -&gt; np.ndarray:
            &#34;&#34;&#34;
            same as above for non-exceedances
            
            Args:
                size (int): Sample size
                alpha (float): Dependence parameter
                a (float): asymmetry parameter for first component
                b (float): Asymmetry parameter for second component
            
            Returns:
                np.ndarray: Simulated sample
            
            &#34;&#34;&#34;
            alpha_param = np.array([alpha])
            ## offset from asymmetry parameters
            offset = np.array([np.log(a), np.log(b)]).reshape((1,2))
            offset_th = target_th - offset 
            #compute lower symmetric threshold to use as a proxy when sampling (some samples may be dropped)
            min_offset = np.min(offset_th)
            effective_th = min_offset * np.ones((2,)).reshape((1,2)) 
            # compute average sample drop rate
            drop_rate = 1 - Logistic.unconditioned_cdf(alpha_param, effective_th)    
            if drop_rate &gt;= 1 or drop_rate &lt; 0:
                raise Exception(f&#34;Invalid drop rate ({drop_rate})&#34;)
            effective_size = int(size/(1-drop_rate))
            # sample from logistic model
            b2 = gumbel_logistic(effective_size, alpha_param, 0)
            # skew to symmetric logistic parameters
            b2[:,0] += np.log(a)
            b2[:,1] += np.log(b)
            # drop samples outside target region
            target_th1, target_th2 = target_th.reshape(-1)
            exs_idx = np.logical_and(b2[:,0] &lt;= target_th1, b2[:,1] &lt;= target_th2)
            b2 = b2[exs_idx,:]
            return b2

        def sample(
            sampler: t.Callable,
            size: int, 
            alpha: float, 
            a: float, 
            b: float) -&gt; np.ndarray:
            &#34;&#34;&#34;Wrapper for the functions above that makes the sample size deterministic
            
            Args:
                size (int): Sample size
                alpha (float): dependence parameter
                a (float): asymmetry parameter for first component
                b (float): asymmetry parameter for second component
            
            Returns:
                np.ndarray: Sample
            &#34;&#34;&#34;
            # get bivariate samples ($B_2$ in the referenced paper)
            #
            b2 = sampler(size, alpha, a, b)
            while len(b2) &lt; size:
                k = size - len(b2)
                updated_size = max(100,2*k)
                b2 = np.concatenate([b2, sampler(updated_size, alpha, a, b)], axis=0)
            b2 = b2[0:size,:]
            return b2

        # The paper referenced above takes the maximum from logistic model samples and rescales them (or offsets them, in Gumbel scale) in order to sample from an asymmetric logistic.
        # B_1 = bivariate independent offset Gumbel samples
        # B_2 = bivariate logistic offset Gumbel
        # B_1 is independent from B_2. Let M = max(B_1, B_2), then M ~ asym. logistic Gumbel

        alpha_param = np.array([alpha])

        b2_offset = np.log(np.array([beta,gamma]))
        b1_offset = np.log(np.array([1-beta,1-gamma]))
        if quantile_threshold == 0:
            # if simulated region is unconditioned (i.e. the entire plane) use method from cited paper directly
            # independent gumbel variates (alpha = 1)
            b1 = gumbel_logistic(size=size, params=np.array([1]), quantile_threshold=0) + b1_offset
            # logistic gumbel variates
            b2 = gumbel_logistic(size=size, params=alpha_param, quantile_threshold=0) + b2_offset
            z = np.maximum(b1,b2)
        else:
            # Sampling exceedances requires some additional care:
            # exceedance in M &lt;=&gt; exceedance in B_1 or exceedance in B_2
            # exceedances can then be split in three cases: exceedance in B_1 alone, in B_2 alone or in both.
            # below the 3 cases are simulated separately and then concatenated 

            # work out sample size proportions for the three cases
            p_exs_b1 = float(1 - Logistic.unconditioned_cdf(np.array([1]), target_th - b1_offset))
            p_exs_b2 = float(1 - Logistic.unconditioned_cdf(alpha_param, target_th - b2_offset))
            p_exs_any = p_exs_b1 + p_exs_b2 - p_exs_b1*p_exs_b2
            p_exs = np.array([
                p_exs_b1*(1-p_exs_b2), 
                p_exs_b2*(1-p_exs_b1), 
                p_exs_b1*p_exs_b2])/p_exs_any #relative sample sizes for all three cases

            b1_exs_size, b2_exs_size, both_exs_size = (
                np.random.multinomial(n=size, pvals=p_exs, size=1)[0].astype(np.int32)
                )

            # sample 6 cases: (case 1, 2 or 3) * (exceedance or non-exceedance)
            b1_exs = sample(
                sampler=logistic_exs,
                size=b1_exs_size, 
                alpha=1, 
                a=1-beta, 
                b=1-gamma)

            b2_not_exs = sample(
                sampler=logistic_non_exs,
                size=b1_exs_size, 
                alpha=alpha, 
                a=beta, 
                b=gamma)

            b2_exs = sample(
                sampler=logistic_exs,
                size=b2_exs_size, 
                alpha=alpha, 
                a=beta, 
                b=gamma)

            b1_not_exs = sample(
                sampler = logistic_non_exs,
                size=b2_exs_size, 
                alpha=1, 
                a=1-beta, 
                b=1-gamma)

            b1_both_exs = sample(
                sampler=logistic_exs,
                size=both_exs_size, 
                alpha=1, 
                a=1-beta, 
                b=1-gamma)

            b2_both_exs = sample(
                sampler = logistic_exs,
                size=both_exs_size, 
                alpha=alpha, 
                a=beta, 
                b=gamma)

            # take elementwise maximum and concatenate
            z = np.concatenate([
                np.maximum(b1_exs, b2_not_exs),
                np.maximum(b2_exs, b1_not_exs),
                np.maximum(b1_both_exs, b2_both_exs)],
                axis=0)

        # return in canonical model scale, i.e. unit Frechet
        return np.exp(z)


class LogisticGP(Logistic):

    &#34;&#34;&#34;This model is an implementation of a bivariate generalised Pareto distribution with logistic dependence. In Gumbel scale this is given by 
    
    $$ \\mathbb{P}(\\textbf{X} \\leq \\mathbf{x}) = \\frac{\\Psi(\\mathbf{x}) - \\Psi(\\min \\{\\mathbf{x},\\mathbf{0}\\})}{-\\Psi(\\mathbf{0})}, \\mathbf{X} \\nleq \\mathbf{0}$$

    where

    $$ \\Psi(\\mathbf{x}) = - \\left(  \\exp \\left( - \\frac{\\mathbf{x}_1}{\\alpha} \\right)+ \\left(  - \\frac{\\mathbf{x}_2}{\\alpha}  \\right) \\right)^\\alpha, \\, 0 \\leq \\alpha\\leq 1, \\$$

    &#34;&#34;&#34;

    @validator(&#34;quantile_threshold&#34;)
    def val_qt(cls, quantile_threshold):
        q = Logistic._model_marginal_dist.cdf(0)
        if quantile_threshold &lt;= q:
            raise ValueError(f&#34;This model does not support quantile thresholds lower than {np.round(q,2)}&#34;)
        else:
            return quantile_threshold

    @classmethod
    def logpdf(
        cls, params: np.ndarray, threshold: float, data: t.Union[np.ndarray, t.Iterable]
    ):
        &#34;&#34;&#34;Calculates the logpdf function
        
        
        Args:
            params (np.ndarray): array with dependence parameter as only element
            threshold (float): Exceedance threshold in Gumbel scale
            data (t.Union[np.ndarray, t.Iterable]): Observed data in Gumbel scale
        
        &#34;&#34;&#34;
        alpha = params[0]

        x, y = cls.unbundle(data)

        rescaler = 1 - Logistic.unconditioned_cdf(params, cls.bundle(threshold, threshold))

        log_a = (-x/alpha-y/alpha)
        log_b = (-2+alpha)*np.log(np.exp(-x/alpha)+np.exp(-y/alpha))
        log_c = np.log(1-alpha)
        log_d = -np.log(alpha)

        log_density = log_a + log_b + log_c + log_d - np.log(rescaler)

        # density is 0 when both coordinates are below the threshold
        nil_density_idx = np.logical_and(x &lt;= threshold, y &lt;= threshold)
        log_density[nil_density_idx] = -np.Inf

        return log_density

    @classmethod
    def unconditioned_cdf(cls, params: np.ndarray, data: t.Union[np.ndarray, t.Iterable]):
        &#34;&#34;&#34;Calculates cdf function with an exceedance threshold of zero&#34;&#34;&#34;
        alpha = params[0]
        x, y = cls.unbundle(data)
        G0 = Logistic.unconditioned_cdf(params, cls.bundle(0,0))
        Ga = Logistic.unconditioned_cdf(params, data)
        Gb = Logistic.unconditioned_cdf(params, np.minimum(data,0))
        return -1/np.log(G0)*(np.log(Ga)-np.log(Gb))

    @classmethod
    def simulate_model(cls, size: int, params: np.ndarray, quantile_threshold: float) -&gt; np.ndarray:
        &#34;&#34;&#34;Simulates exceedances from bivariate GP model
        
        Args:
            size (int): Simulated sample size
            params (np.ndarray): Array with dependence parameter
            threshold (float): Exceedance threshold in both components
        
        Returns:
            np.ndarray: Simulated sample
        &#34;&#34;&#34;
        threshold = gumbel.ppf(quantile_threshold)
        alpha = params[0]

        maxima = exponential.rvs(size=size, loc = threshold)

        if alpha == 0:
            r = 0
        elif alpha == 1:
            r = np.Inf
        else:
            ###simulate difference between maxima and minima r = max(x,y) - min(x,y) using inverse function method
            u = np.random.uniform(size=size)
            r = alpha * np.log(((1-u)/2**(1-alpha))**(1/(alpha-1))-1)

        minima = maxima - r

        # allocate maxima randomly between components
        max_indices = np.random.binomial(1, 0.5, size)

        x = np.concatenate(
            [
                maxima[max_indices == 0].reshape((-1, 1)),
                minima[max_indices == 1].reshape((-1, 1)),
            ],
            axis=0,
        )

        y = np.concatenate(
            [
                minima[max_indices == 0].reshape((-1, 1)),
                maxima[max_indices == 1].reshape((-1, 1)),
            ],
            axis=0,
        )

        return cls.bundle(x,y)


class Empirical(BaseDistribution):

    &#34;&#34;&#34;Bivariate empirical distribution induced by a sample of observed data&#34;&#34;&#34;

    data: np.ndarray
    pdf_values: np.ndarray

    _exceedance_models = {
    &#34;logistic&#34;: Logistic, 
    &#34;gaussian&#34;: Gaussian, 
    &#34;asymmetric logistic&#34;: AsymmetricLogistic,
    &#34;logistic gp&#34;: LogisticGP
    }

    def __repr__(self):
        return f&#34;Bivariate empirical distribution with {len(self.data)} points&#34;

    @validator(&#34;pdf_values&#34;, allow_reuse=True)
    def check_pdf_values(cls, pdf_values):
        if np.any(pdf_values &lt; -cls._error_tol):
            raise ValueError(&#34;There are negative pdf values&#34;)
        if not np.isclose(np.sum(pdf_values), 1, atol=cls._error_tol):
            print(f&#34;sum: {np.sum(pdf_values)}, pdf vals: {pdf_values}&#34;)
            raise ValueError(&#34;pdf values don&#39;t sum 1&#34;)
        # pdf_values = np.clip(pdf_values, a_min = 0.0, a_max = 1.0)
        # # normalise
        # pdf_values = pdf_values/np.sum(pdf_values)

        return pdf_values

    @classmethod
    def from_data(cls, data: np.ndarray):
        &#34;&#34;&#34;Instantiate an empirical distribution from an n x 2 data matrix

        Args:
            data (np.ndarray): observed data


        &#34;&#34;&#34;
        if (
            not isinstance(data, np.ndarray)
            or len(data.shape) != 2
            or data.shape[1] != 2
        ):
            raise ValueError(&#34;data must be an n x 2 numpy array&#34;)

        n = len(data)
        return Empirical(
            data=data, pdf_values=1.0 / n * np.ones((n,), dtype=np.float64)
        )

    def pdf(self, x: np.ndarray):

        return np.mean(self.data == x.reshape((1, 2)))

    def cdf(self, x: np.ndarray):
        if len(x.shape) &gt; 1:
            return np.array([self.cdf(elem) for elem in x])

        u = self.data &lt;= x.reshape((1, 2))  # componentwise comparison
        v = (
            u.dot(np.ones((2, 1))) &gt;= 2
        )  # equals 1 if and only if both components are below x
        return np.mean(v)

    def simulate(self, size: int):
        n = len(self.data)
        idx = np.random.choice(n, size=size)
        return self.data[idx]

    def get_marginals(self):

        return univar.Empirical.from_data(self.data[:, 0]), univar.Empirical.from_data(
            self.data[:, 1]
        )

    def fit_tail_model(
        self,
        model: str,
        quantile_threshold: float,
        margin1: univar.BaseDistribution = None,
        margin2: univar.BaseDistribution = None,
    ):
        &#34;&#34;&#34;Fits a parametric model for threshold exceedances in the data. For a given threshold \\( u \\), exceedances are defined as vectors \\(Z\\) such that \\( \\max\\{Z_1,Z_2\\} &gt; u \\), this is, an exceedance in at least one component, and encompasses an inverted L-shaped subset of Euclidean space.
        Currently, logistic and Gaussian models are available, with the former exhibiting asymptotic dependence, a strong type of dependence between extreme occurrences across components, and the latter exhibiting asymptotic independence, in which extremes occur relatively independently across components.

        Args:
            model (str): name of selected model, currently one of &#39;gaussian&#39; or &#39;logistic&#39; or &#39;asymmetric logistic&#39;. For more information, see `Gaussian`,  `Logistic` and `AsymmetricLogistic` classes.
            margin1 (univar.BaseDistribution, optional): Marginal distribution for first component. If not provided, a semiparametric model with a fitted Generalised Pareto upper tail is used.
            margin2 (univar.BaseDistribution, optional): Marginal distribution for second component. If not provided, a semiparametric model with a fitted Generalised Pareto upper tail is used.
            quantile_threshold (float): Quantile threshold to use for the definition of exceedances

        Returns:
            ExceedanceModel

        &#34;&#34;&#34;
        if model not in self._exceedance_models:
            raise ValueError(f&#34;model must be one of {self._exceedance_models}&#34;)

        if margin1 is None:

            margin1, _ = self.get_marginals()
            margin1 = margin1.fit_tail_model(threshold=margin1.ppf(quantile_threshold))
            warnings.warn(
                f&#34;First marginal not provided. Fitting tail model using provided quantile threshold ({quantile_threshold} =&gt; {margin1.ppf(quantile_threshold)})&#34;,
                stacklevel=2,
            )

        if margin2 is None:

            _, margin2 = self.get_marginals()
            margin2 = margin2.fit_tail_model(threshold=margin2.ppf(quantile_threshold))
            warnings.warn(
                f&#34;Second marginal not provided. Fitting tail model using provided quantile threshold ({quantile_threshold} =&gt; {margin2.ppf(quantile_threshold)})&#34;,
                stacklevel=2,
            )

        data = self.data

        x = data[:, 0]
        y = data[:, 1]

        exceedance_idx = np.logical_or(
            x &gt; margin1.ppf(quantile_threshold), y &gt; margin2.ppf(quantile_threshold)
        )

        exceedances = data[exceedance_idx]

        exceedance_model = self._exceedance_models[model].fit(
            data=exceedances,
            quantile_threshold=quantile_threshold,
            margin1=margin1,
            margin2=margin2,
        )

        empirical_model = Empirical.from_data(self.data[np.logical_not(exceedance_idx)])

        p = np.mean(np.logical_not(exceedance_idx))

        return ExceedanceModel(
            distributions=[empirical_model, exceedance_model],
            weights=np.array([p, 1 - p]),
        )

    def test_asymptotic_dependence(
        self, 
        quantile_threshold: float = 0.95, 
        prior: t.Optional[str] = &#34;jeffreys&#34;) -&gt; float:
        &#34;&#34;&#34;Computes a Savage-Dickey ratio for the coefficient of tail dependence \\(\\eta\\) to test the hypothesis of asymptotic dependence (See &#39;Statistics of Extremes&#39; by Beirlant, page 345-346). The hypothesis space is \\(\\eta \\in [0,1]\\) with \\(\\eta = 1\\) corresponding to asymptotic dependence. The posterior density is approximated through Gaussian Kernel density estimation.

        Args:
            quantile_threshold (float, optional): Quantile threshold over which the coefficient of tail dependence is to be estimated.
            log_prior (str, optional): Name of prior distribution to use for Bayesian inference. must be one of &#39;flat&#39;, which uses a flat prior on the support, or &#39;jeffreys&#39; which uses an uninformative Jeffreys prior; defaults to &#39;jeffreys&#39;.

        Returns:
            float: Savage-Dickey ratio. If larger than 1, this favors the asymptotic dependence hypothesis and vice versa.
        &#34;&#34;&#34;
        def flat_prior(theta):
            scale, shape = theta
            if scale &gt; 0 and shape &gt;= 0 and shape &lt;= 1:
                return 0.0
            else:
                return -np.Inf

        def jeffreys_prior(theta):
            scale, shape = theta
            if scale &gt; 0 and shape &gt;= 0 and shape &lt;= 1:
                return -np.log(scale) - np.log(1+shape) - 0.5*np.log(1+2*shape)
            else:
                return -np.Inf

        log_priors = {
            &#34;flat&#34;: flat_prior,
            &#34;jeffreys&#34;: jeffreys_prior
        }
        # Savage-Dickey ratios use the prior marginal density for eta = 1. Compute it for both priors
        prior_density = {
            &#34;flat&#34;: 1, # prior is uniform in [0,1] for eta
            &#34;jeffreys&#34;: np.exp(log_priors[&#34;jeffreys&#34;]([1,1]))/(np.pi/6) #constant factor is pi/6 (integral in [0,1])
        }

        try:
            log_prior = log_priors[prior]
        except KeyError as e:
            raise ValueError(f&#34;Prior name not recognised. Must be one of {log_priors.keys()}&#34;)

        ### compute savage-dickey density ratio
        # Use generalised pareto to fit tails
        x, y = self.data.T
        x_dist, y_dist = univar.Empirical.from_data(x), univar.Empirical.from_data(y)
        x_dist, y_dist = x_dist.fit_tail_model(
            x_dist.ppf(quantile_threshold)
        ), y_dist.fit_tail_model(y_dist.ppf(quantile_threshold))

        # transform to approximate standard Frechet margins and map to test data t
        u1, u2 = x_dist.cdf(x), y_dist.cdf(y)
        z1, z2 = -1 / np.log(u1), -1 / np.log(u2)
        t = np.minimum(z1, z2)

        t_dist = univar.Empirical.from_data(t)
        # Pass initial point that enforces theoretical constraints of 0 &lt;= eta &lt;= 1. Approximation inaccuracies from the transformation to Frechet margins and MLE estimation can violate the bounds.
        mle_tail = t_dist.fit_tail_model(t_dist.ppf(quantile_threshold))
        x0 = np.array([mle_tail.tail.scale, max(0, min(0.99, mle_tail.tail.shape))])
        # sample posterior
        t_dist = t_dist.fit_tail_model(
            t_dist.ppf(quantile_threshold), bayesian=True, log_prior=log_prior, x0=x0
        )

        # approximate posterior distribution through Kernel density estimation. Evaluating it on 1 gives us the savage-dickey ratio
        #return gaussian_kde(t_dist.tail.shapes).evaluate(1)[0]

        return gaussian_kde(t_dist.tail.shapes).evaluate(1)[0]/prior_density[prior]

    @classmethod
    def pickands(self, p: np.ndarray, data: np.ndarray, quantile_threshold: float = 0.95) -&gt; np.ndarray:
        &#34;&#34;&#34;Non-parametric Pickands dependence function approximation based on Hall and Tajvidi (2000).

        Args:
            p (np.ndarray): Pickands dependence function arguments
            data: (np.ndarray): data matrix of n x 2
            quantile_threshold (float, optional): Quantile threshold over which Pickands dependence will be approximated.

        Returns:
            np.ndarray: Nonparametric estimate of the Pickands dependence function
        &#34;&#34;&#34;
        # find joint extremes
        if np.any(p &gt; 1) or np.any(p &lt; 0):
            raise ValueError(&#34;All argument values must be in [0,1]&#34;)

        x, y = data.T
        joint_extremes_idx = np.logical_and(
            x &gt; np.quantile(x, quantile_threshold),
            y &gt; np.quantile(y, quantile_threshold),
        )
        n = np.sum(joint_extremes_idx)

        # compute nonparametric scores
        x_exs, y_exs = x[joint_extremes_idx], y[joint_extremes_idx]
        x_exs_model, y_exs_model = (
            univar.Empirical.from_data(x_exs),
            univar.Empirical.from_data(y_exs),
        )

        # normalise to avoid copula values on the border of the unit square
        u1, u2 = n / (n + 1) * x_exs_model.cdf(x_exs), n / (n + 1) * y_exs_model.cdf(
            y_exs
        )
        # map to exponential
        s, t = -np.log(u1), -np.log(u2)

        pk = []
        for p_ in p:
            a = (s / np.mean(s)) / (1 - p_)
            b = (t / np.mean(t)) / p_
            pk.append(1.0 / np.mean(np.minimum(a, b)))
            
        return np.array(pk)

    def plot_pickands(self, quantile_threshold: float = 0.95)  -&gt; matplotlib.figure.Figure:
        &#34;&#34;&#34;Returns a plot of the empirical Pickands dependence function induced by joint exceedances above the specified quantile threshold. 
        The Pickands dependence function \\(A: [0,1] \\to [1/2,1]\\) is convex and bounded by \\(\\max\\{t,1-t\\} \\leq A(t) \\leq 1\\); it can be used to assess extremal dependence, as there is a one-to-one correspondence between \\(A(t)\\) and extremal copulas; the closer it is to its lower bound, the stronger the extremal dependence. Conversely, for asymptotically independent data \\(A(t) = 1\\).
        The non-parametric approximation used here is based on Hall and Tajvidi (2000).

        Args:
            quantile_threshold (float, optional): Quantile threshold over which Pickands dependence will be approximated.
        
        Returns:
            matplotlib.figure.Figure: figure
        &#34;&#34;&#34;
        fig = plt.figure(figsize=(5, 5))

        x = np.linspace(0, 1, 101)
        pk = self.pickands(x, self.data)

        plt.plot(x, pk, color=&#34;darkorange&#34;, label=&#34;Empirical&#34;)
        plt.plot(x, np.maximum(1 - x, x), linestyle=&#34;dashed&#34;, color=&#34;black&#34;)
        plt.plot(x, np.ones((len(x),)), linestyle=&#34;dashed&#34;, color=&#34;black&#34;)
        plt.xlabel(&#34;t&#34;)
        plt.ylabel(&#34;A(t)&#34;)
        plt.title(&#34;Empirical Pickands dependence of joint exceedances&#34;)
        plt.grid()
        return fig

    def plot_chi(self, direct_estimate: bool = True)  -&gt; matplotlib.figure.Figure:
        &#34;&#34;&#34;Returns a plot of the empirical estimate of the coefficient of asymptotic dependence , defined as \\( \\chi = \\ \\lim_{p \\to 1} \\mathbb{P}(Y &gt; q_y(p) \\,|\\, X &gt; q_x (p)) \\), where \\( q_x, q_y\\) are the corresponding quantiles for a given probability level \\(p\\). There is asymptotic dependence when \\(\\chi &gt; 0\\) and vice versa.

        Args:
            direct_estimate (optional, bool): if True, use the empirical copula&#39;s probability estimates directly. Otherwise use the approximation given by \\(\\chi = \\lim_{u \\to 1} 2 - \\log C(u,u) / \\log(u) \\) where \\(C(u,u)\\) is the empirical copula.
        
        Returns:
            matplotlib.figure.Figure: figure
        &#34;&#34;&#34;
        def chi(p: np.ndarray) -&gt; t.Tuple[np.ndarray, np.ndarray]:
            &#34;&#34;&#34;Returns chi estimates and the corresponding estimate&#39;s standard deviation
            
            Args:
                t (np.ndarray): input probability levels
            
            Returns:
                t.Tuple[np.ndarray, np.ndarray]: estimate and standard deviation arrays
            &#34;&#34;&#34;
            x, y = self.get_marginals()            
            n = len(x.data)
            # map to rescaled copula values in the open set (0,1)
            u1, u2 = x.cdf(x.data) * n/(n+1), y.cdf(y.data) * n/(n+1)
            empirical_copula = Empirical.from_data(np.stack([u1,u2], axis=1))
            if direct_estimate:
                joint_p = 1 - 2*p + empirical_copula.cdf(np.stack([p,p], axis=1))
                estimate = joint_p/(1-p)
                n_obs = len(x.data) * (1-p)
                std = np.sqrt(estimate*(1-estimate)/n_obs)
            else:
                vals = empirical_copula.cdf(np.stack([p,p], axis=1))
                estimate = 2 - np.log(vals)/np.log(p)
                # standard error approximation using delta method
                std = np.sqrt(vals*(1-vals)/n * 1/np.log(p)**2 * 1/vals**2)
            return estimate, std


        fig = plt.figure(figsize=(5, 5))
        p = np.linspace(0.01, 0.99, 99)
        chi_central, chi_std = chi(p)
        ci_l, ci_u = chi_central - 1.96*chi_std, chi_central + 1.96*chi_std

        plt.plot(p, chi_central, color=self._figure_color_palette[0])
        plt.scatter(p, chi_central, color=self._figure_color_palette[0])
        plt.fill_between(p, ci_l, ci_u, linestyle=&#34;dashed&#34;, color=self._figure_color_palette[1], alpha = 0.2)
        plt.xlabel(&#34;quantiles&#34;)
        plt.ylabel(&#34;Chi(u)&#34;)
        plt.grid()
        return fig</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="riskmodels.bivariate.AsymmetricLogistic"><code class="flex name class">
<span>class <span class="ident">AsymmetricLogistic</span></span>
<span>(</span><span>**data:Â Any)</span>
</code></dt>
<dd>
<div class="desc"><p>This model assumes association between exceedances at different components follow a copula induced by an asymmetric logistic model of extremal dependence; this model in unit Frechet scale is given by </p>
<p><span><span class="MathJax_Preview"> \mathbb{P}(\textbf{X} \leq \mathbf{x}) = \exp \left( - \frac{1 - \beta}{\mathbf{x}_1} - \frac{1 - \gamma}{\mathbf{x}_2} - \left(
\left( \frac{\beta}{\mathbf{x}_1} \right)^{1/\alpha} + \left( \frac{\gamma}{\mathbf{x}_2} \right)^{1/\alpha}\right)^\alpha \right), \, 0 \leq \alpha, \beta, \gamma \leq 1, \, \mathbf{X} &gt; 0</span><script type="math/tex; mode=display"> \mathbb{P}(\textbf{X} \leq \mathbf{x}) = \exp \left( - \frac{1 - \beta}{\mathbf{x}_1} - \frac{1 - \gamma}{\mathbf{x}_2} - \left(
\left( \frac{\beta}{\mathbf{x}_1} \right)^{1/\alpha} + \left( \frac{\gamma}{\mathbf{x}_2} \right)^{1/\alpha}\right)^\alpha \right), \, 0 \leq \alpha, \beta, \gamma \leq 1, \, \mathbf{X} > 0</script></span></p>
<p>Exceedances in each component are defined as observations above a fixed quantile threshold <span><span class="MathJax_Preview"> \textbf{q}</span><script type="math/tex"> \textbf{q}</script></span> for a high probability level <span><span class="MathJax_Preview">p \approx 1</span><script type="math/tex">p \approx 1</script></span>, and so bivariate exceedances <span><span class="MathJax_Preview">\textbf{Z}</span><script type="math/tex">\textbf{Z}</script></span> are defined in an inverted-L-shaped region of space, <span><span class="MathJax_Preview"> \textbf{Z} \nleq \mathbf{q} </span><script type="math/tex"> \textbf{Z} \nleq \mathbf{q} </script></span>: that in which there is an exceedance in at least one component. Consequently the model implemented here is only defined in the corresponding inverted-L-shaped region.</p>
<p>If the underlying marginal distributions follow a generalised Pareto above the quantile thresholds, this model can be seen as a pre-limit version of a bivariate generalised Pareto distribution (see Rootzen and Tajvidi, 2006) with an asymmetric logistic dependence model, to which this model converges as the quantile threshold grows.</p>
<p>Create a new model by parsing and validating input data from keyword arguments.</p>
<p>Raises ValidationError if the input data cannot be parsed to form a valid model.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AsymmetricLogistic(ExceedanceDistribution):

    &#34;&#34;&#34;This model assumes association between exceedances at different components follow a copula induced by an asymmetric logistic model of extremal dependence; this model in unit Frechet scale is given by 
    
    $$ \\mathbb{P}(\\textbf{X} \\leq \\mathbf{x}) = \\exp \\left( - \\frac{1 - \\beta}{\\mathbf{x}_1} - \\frac{1 - \\gamma}{\\mathbf{x}_2} - \\left(  \\left( \\frac{\\beta}{\\mathbf{x}_1} \\right)^{1/\\alpha} + \\left( \\frac{\\gamma}{\\mathbf{x}_2} \\right)^{1/\\alpha}\\right)^\\alpha \\right), \\, 0 \\leq \\alpha, \\beta, \\gamma \\leq 1, \\, \\mathbf{X} &gt; 0$$

    Exceedances in each component are defined as observations above a fixed quantile threshold \\( \\textbf{q}\\) for a high probability level \\(p \\approx 1\\), and so bivariate exceedances \\(\\textbf{Z}\\) are defined in an inverted-L-shaped region of space, \\( \\textbf{Z} \\nleq \\mathbf{q} \\): that in which there is an exceedance in at least one component. Consequently the model implemented here is only defined in the corresponding inverted-L-shaped region.

    If the underlying marginal distributions follow a generalised Pareto above the quantile thresholds, this model can be seen as a pre-limit version of a bivariate generalised Pareto distribution (see Rootzen and Tajvidi, 2006) with an asymmetric logistic dependence model, to which this model converges as the quantile threshold grows.
    &#34;&#34;&#34;

    params: t.Optional[np.ndarray] = Field(default_factory=lambda: np.zeros((3,), dtype=np.float32))

    _param_names = {0:&#34;alpha&#34;, 1:&#34;beta&#34;, 2:&#34;gamma&#34;} #mapping from params array indices to names for diagnostic plots
    _default_x0 = np.array([0,0,0])

    _model_marginal_dist = Frechet()
    
    #_plotting_dist = Frechet
    _plotting_dist_name = &#34;Frechet&#34;

    def __repr__(self):
        return f&#34;{self.__class__.__name__} exceedance dependence model with (alpha, beta, gamma) = {self.params} and quantile threshold {self.quantile_threshold}&#34;

    @validator(&#34;params&#34;)
    def validate_params(cls, params):
        alpha, beta, gamma = params
        if alpha &lt;= 0 or alpha &gt; 1:
            raise TypeError(f&#34;alpha must be in the interval (0,1]&#34;)
        if (beta &lt; 0 or beta &gt; 1) or (gamma &lt; 0 or gamma &gt; 1):
            raise TypeError(f&#34;beta, gamma must be in the interval [0,1]&#34;)
        else:
            return params

    @classmethod
    def logpdf(
        cls, params: np.ndarray, threshold: float, data: t.Union[np.ndarray, t.Iterable]
    ):
        &#34;&#34;&#34;Calculates logpdf function for asymmetric logistic Frechet exceedances
        
        
        Args:
            params (np.ndarray): array with dependence and asymmetry parameters
            threshold (float): Exceedance threshold in Gumbel scale
            data (t.Union[np.ndarray, t.Iterable]): Observed data in Frechet scale
        
        &#34;&#34;&#34;
        alpha, beta, gamma = params

        x, y = cls.unbundle(data)
        rescaler = 1 - cls.unconditioned_cdf(params, cls.bundle(threshold, threshold))

        alpha_prenorm = (beta/x)**(1/alpha) + (gamma/y)**(1/alpha)
        alpha_norm = (alpha_prenorm)**alpha

        log_a = (-1+beta)/x + (-1+gamma-y*alpha_norm)/y
        log_b = -(2*np.log(x) + 2*np.log(y) + np.log(alpha))

        c_1 = -x*y*(-1+alpha)*(beta*gamma/(x*y))**(1/alpha)*alpha_prenorm**(-2+alpha)
        c_2 = alpha * (1 - beta + x*(beta/x)**(1/alpha)*alpha_prenorm**(-1+alpha))*(1 - gamma + y*(gamma/y)**(1/alpha)*alpha_prenorm**(-1+alpha))
        log_c = np.log(c_1+c_2)

        log_density = log_a + log_b + log_c - np.log(rescaler)

        # density is 0 when both coordinates are below the threshold
        nil_density_idx = np.logical_and(x &lt;= threshold, y &lt;= threshold)
        log_density[nil_density_idx] = -np.Inf

        return log_density

    @classmethod
    def unconditioned_cdf(cls, params: np.ndarray, data: t.Union[np.ndarray, t.Iterable]) -&gt; np.ndarray:
        &#34;&#34;&#34;Calculates unconstrained standard Frechet cdf with asymmetric logistic dependence
        
        Args:
            params (np.ndarray): array with dependence and asymmetry parameters
            data (t.Union[np.ndarray, t.Iterable]): Observed data in Frechet scale
        
        Returns:
            np.ndarray: cdf values
        &#34;&#34;&#34;
        x, y = cls.unbundle(data)
        alpha, beta, gamma = params
        return np.exp(-(1-beta)/x - (1-gamma)/y - ((beta/x)**(1/alpha) + (gamma/y)**(1/alpha))**alpha)

    def simulate_model(self, size: int, params: np.ndarray, quantile_threshold: float) -&gt; np.ndarray:
        &#34;&#34;&#34;Simulate asymmetric logistic exceedance model variates in Frechet scale using a method inspired by the one outlined in &#39;Simulating Multivariate Extreme Value Distributions of Logistic Type&#39; by Stephenson (2003).
        
        Args:
            size (int): Sample size
            params (np.ndarray): array with dependence and asymmetry parameters
            quantile_threshold (float): Quantile threshold for simulated exceedances
        
        Returns:
            np.ndarray: Simulated sample
        &#34;&#34;&#34;

        # Gumbel scales are used instead of Frechet in this method, and a the existing logistic model is used as a baseline sampler
        # in Gumbel scales asymmetry constants are offsets instead of rescaling factors
        gumbel_logistic = Logistic.simulate_model
        threshold = gumbel.ppf(quantile_threshold)
        #threshold from which we want to sample
        target_th = np.array([threshold, threshold]).reshape((1,2))
        # unpack params
        alpha, beta, gamma = params

        def logistic_exs(size: int, alpha: float, a: float, b: float) -&gt; np.ndarray:
            &#34;&#34;&#34;
            Samples a non-deterministic number of exceedances from a logistic exceedance model. A smaller , symmetric proxy threshold is used to simulate variates that are later offset to match the target threshold. Because the proxy threshold is symmetric but the target threshold may not be, some samples may be discarded. The expected sample size is the passed size parameter. The target threshold value comes from outer scope.
            
            Args:
                size (int): Sample size
                alpha (float): Dependence parameter
                a (float): asymmetry parameter for first component
                b (float): Asymmetry parameter for second component
            
            Returns:
                np.ndarray: Simulated sample
            
            &#34;&#34;&#34;
            alpha_param = np.array([alpha])
            ## offset from asymmetry parameters
            offset = np.array([np.log(a), np.log(b)]).reshape((1,2))
            offset_th = target_th - offset 
            #compute lower symmetric threshold to use as a proxy when sampling (some samples may be dropped)
            min_offset = np.min(offset_th)
            effective_th = min_offset * np.ones((2,)).reshape((1,2)) 
            effective_q = gumbel.cdf(min_offset)
            # compute average sample drop rate
            s_effective = 1 - Logistic.unconditioned_cdf(alpha_param, effective_th)
            s_offset = 1 - Logistic.unconditioned_cdf(alpha_param, offset_th)
            drop_rate = (s_effective - s_offset)/s_effective        
            if drop_rate &gt;= 1 or drop_rate &lt; 0:
                raise Exception(f&#34;Invalid drop rate ({drop_rate})&#34;)
            effective_size = int(size/(1-drop_rate))
            # sample from logistic model
            b2 = gumbel_logistic(effective_size, alpha_param, effective_q)
            # skew to symmetric logistic parameters
            b2[:,0] += np.log(a)
            b2[:,1] += np.log(b)
            # drop samples outside target region
            target_th1, target_th2 = target_th.reshape(-1)
            exs_idx = np.logical_or(b2[:,0] &gt; target_th1, b2[:,1] &gt; target_th2)
            b2 = b2[exs_idx,:]
            return b2

        def logistic_non_exs(size: int, alpha: float, a: float, b: float) -&gt; np.ndarray:
            &#34;&#34;&#34;
            same as above for non-exceedances
            
            Args:
                size (int): Sample size
                alpha (float): Dependence parameter
                a (float): asymmetry parameter for first component
                b (float): Asymmetry parameter for second component
            
            Returns:
                np.ndarray: Simulated sample
            
            &#34;&#34;&#34;
            alpha_param = np.array([alpha])
            ## offset from asymmetry parameters
            offset = np.array([np.log(a), np.log(b)]).reshape((1,2))
            offset_th = target_th - offset 
            #compute lower symmetric threshold to use as a proxy when sampling (some samples may be dropped)
            min_offset = np.min(offset_th)
            effective_th = min_offset * np.ones((2,)).reshape((1,2)) 
            # compute average sample drop rate
            drop_rate = 1 - Logistic.unconditioned_cdf(alpha_param, effective_th)    
            if drop_rate &gt;= 1 or drop_rate &lt; 0:
                raise Exception(f&#34;Invalid drop rate ({drop_rate})&#34;)
            effective_size = int(size/(1-drop_rate))
            # sample from logistic model
            b2 = gumbel_logistic(effective_size, alpha_param, 0)
            # skew to symmetric logistic parameters
            b2[:,0] += np.log(a)
            b2[:,1] += np.log(b)
            # drop samples outside target region
            target_th1, target_th2 = target_th.reshape(-1)
            exs_idx = np.logical_and(b2[:,0] &lt;= target_th1, b2[:,1] &lt;= target_th2)
            b2 = b2[exs_idx,:]
            return b2

        def sample(
            sampler: t.Callable,
            size: int, 
            alpha: float, 
            a: float, 
            b: float) -&gt; np.ndarray:
            &#34;&#34;&#34;Wrapper for the functions above that makes the sample size deterministic
            
            Args:
                size (int): Sample size
                alpha (float): dependence parameter
                a (float): asymmetry parameter for first component
                b (float): asymmetry parameter for second component
            
            Returns:
                np.ndarray: Sample
            &#34;&#34;&#34;
            # get bivariate samples ($B_2$ in the referenced paper)
            #
            b2 = sampler(size, alpha, a, b)
            while len(b2) &lt; size:
                k = size - len(b2)
                updated_size = max(100,2*k)
                b2 = np.concatenate([b2, sampler(updated_size, alpha, a, b)], axis=0)
            b2 = b2[0:size,:]
            return b2

        # The paper referenced above takes the maximum from logistic model samples and rescales them (or offsets them, in Gumbel scale) in order to sample from an asymmetric logistic.
        # B_1 = bivariate independent offset Gumbel samples
        # B_2 = bivariate logistic offset Gumbel
        # B_1 is independent from B_2. Let M = max(B_1, B_2), then M ~ asym. logistic Gumbel

        alpha_param = np.array([alpha])

        b2_offset = np.log(np.array([beta,gamma]))
        b1_offset = np.log(np.array([1-beta,1-gamma]))
        if quantile_threshold == 0:
            # if simulated region is unconditioned (i.e. the entire plane) use method from cited paper directly
            # independent gumbel variates (alpha = 1)
            b1 = gumbel_logistic(size=size, params=np.array([1]), quantile_threshold=0) + b1_offset
            # logistic gumbel variates
            b2 = gumbel_logistic(size=size, params=alpha_param, quantile_threshold=0) + b2_offset
            z = np.maximum(b1,b2)
        else:
            # Sampling exceedances requires some additional care:
            # exceedance in M &lt;=&gt; exceedance in B_1 or exceedance in B_2
            # exceedances can then be split in three cases: exceedance in B_1 alone, in B_2 alone or in both.
            # below the 3 cases are simulated separately and then concatenated 

            # work out sample size proportions for the three cases
            p_exs_b1 = float(1 - Logistic.unconditioned_cdf(np.array([1]), target_th - b1_offset))
            p_exs_b2 = float(1 - Logistic.unconditioned_cdf(alpha_param, target_th - b2_offset))
            p_exs_any = p_exs_b1 + p_exs_b2 - p_exs_b1*p_exs_b2
            p_exs = np.array([
                p_exs_b1*(1-p_exs_b2), 
                p_exs_b2*(1-p_exs_b1), 
                p_exs_b1*p_exs_b2])/p_exs_any #relative sample sizes for all three cases

            b1_exs_size, b2_exs_size, both_exs_size = (
                np.random.multinomial(n=size, pvals=p_exs, size=1)[0].astype(np.int32)
                )

            # sample 6 cases: (case 1, 2 or 3) * (exceedance or non-exceedance)
            b1_exs = sample(
                sampler=logistic_exs,
                size=b1_exs_size, 
                alpha=1, 
                a=1-beta, 
                b=1-gamma)

            b2_not_exs = sample(
                sampler=logistic_non_exs,
                size=b1_exs_size, 
                alpha=alpha, 
                a=beta, 
                b=gamma)

            b2_exs = sample(
                sampler=logistic_exs,
                size=b2_exs_size, 
                alpha=alpha, 
                a=beta, 
                b=gamma)

            b1_not_exs = sample(
                sampler = logistic_non_exs,
                size=b2_exs_size, 
                alpha=1, 
                a=1-beta, 
                b=1-gamma)

            b1_both_exs = sample(
                sampler=logistic_exs,
                size=both_exs_size, 
                alpha=1, 
                a=1-beta, 
                b=1-gamma)

            b2_both_exs = sample(
                sampler = logistic_exs,
                size=both_exs_size, 
                alpha=alpha, 
                a=beta, 
                b=gamma)

            # take elementwise maximum and concatenate
            z = np.concatenate([
                np.maximum(b1_exs, b2_not_exs),
                np.maximum(b2_exs, b1_not_exs),
                np.maximum(b1_both_exs, b2_both_exs)],
                axis=0)

        # return in canonical model scale, i.e. unit Frechet
        return np.exp(z)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="riskmodels.bivariate.ExceedanceDistribution" href="#riskmodels.bivariate.ExceedanceDistribution">ExceedanceDistribution</a></li>
<li><a title="riskmodels.bivariate.BaseDistribution" href="#riskmodels.bivariate.BaseDistribution">BaseDistribution</a></li>
<li>pydantic.main.BaseModel</li>
<li>pydantic.utils.Representation</li>
<li>abc.ABC</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="riskmodels.bivariate.AsymmetricLogistic.params"><code class="name">var <span class="ident">params</span> :Â Optional[numpy.ndarray]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="riskmodels.bivariate.AsymmetricLogistic.logpdf"><code class="name flex">
<span>def <span class="ident">logpdf</span></span>(<span>params:Â np.ndarray, threshold:Â float, data:Â t.Union[np.ndarray,Â t.Iterable])</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates logpdf function for asymmetric logistic Frechet exceedances</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>params</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>array with dependence and asymmetry parameters</dd>
<dt><strong><code>threshold</code></strong> :&ensp;<code>float</code></dt>
<dd>Exceedance threshold in Gumbel scale</dd>
<dt><strong><code>data</code></strong> :&ensp;<code>t.Union[np.ndarray, t.Iterable]</code></dt>
<dd>Observed data in Frechet scale</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def logpdf(
    cls, params: np.ndarray, threshold: float, data: t.Union[np.ndarray, t.Iterable]
):
    &#34;&#34;&#34;Calculates logpdf function for asymmetric logistic Frechet exceedances
    
    
    Args:
        params (np.ndarray): array with dependence and asymmetry parameters
        threshold (float): Exceedance threshold in Gumbel scale
        data (t.Union[np.ndarray, t.Iterable]): Observed data in Frechet scale
    
    &#34;&#34;&#34;
    alpha, beta, gamma = params

    x, y = cls.unbundle(data)
    rescaler = 1 - cls.unconditioned_cdf(params, cls.bundle(threshold, threshold))

    alpha_prenorm = (beta/x)**(1/alpha) + (gamma/y)**(1/alpha)
    alpha_norm = (alpha_prenorm)**alpha

    log_a = (-1+beta)/x + (-1+gamma-y*alpha_norm)/y
    log_b = -(2*np.log(x) + 2*np.log(y) + np.log(alpha))

    c_1 = -x*y*(-1+alpha)*(beta*gamma/(x*y))**(1/alpha)*alpha_prenorm**(-2+alpha)
    c_2 = alpha * (1 - beta + x*(beta/x)**(1/alpha)*alpha_prenorm**(-1+alpha))*(1 - gamma + y*(gamma/y)**(1/alpha)*alpha_prenorm**(-1+alpha))
    log_c = np.log(c_1+c_2)

    log_density = log_a + log_b + log_c - np.log(rescaler)

    # density is 0 when both coordinates are below the threshold
    nil_density_idx = np.logical_and(x &lt;= threshold, y &lt;= threshold)
    log_density[nil_density_idx] = -np.Inf

    return log_density</code></pre>
</details>
</dd>
<dt id="riskmodels.bivariate.AsymmetricLogistic.unconditioned_cdf"><code class="name flex">
<span>def <span class="ident">unconditioned_cdf</span></span>(<span>params:Â np.ndarray, data:Â t.Union[np.ndarray,Â t.Iterable]) â€‘>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates unconstrained standard Frechet cdf with asymmetric logistic dependence</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>params</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>array with dependence and asymmetry parameters</dd>
<dt><strong><code>data</code></strong> :&ensp;<code>t.Union[np.ndarray, t.Iterable]</code></dt>
<dd>Observed data in Frechet scale</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>cdf values</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def unconditioned_cdf(cls, params: np.ndarray, data: t.Union[np.ndarray, t.Iterable]) -&gt; np.ndarray:
    &#34;&#34;&#34;Calculates unconstrained standard Frechet cdf with asymmetric logistic dependence
    
    Args:
        params (np.ndarray): array with dependence and asymmetry parameters
        data (t.Union[np.ndarray, t.Iterable]): Observed data in Frechet scale
    
    Returns:
        np.ndarray: cdf values
    &#34;&#34;&#34;
    x, y = cls.unbundle(data)
    alpha, beta, gamma = params
    return np.exp(-(1-beta)/x - (1-gamma)/y - ((beta/x)**(1/alpha) + (gamma/y)**(1/alpha))**alpha)</code></pre>
</details>
</dd>
<dt id="riskmodels.bivariate.AsymmetricLogistic.validate_params"><code class="name flex">
<span>def <span class="ident">validate_params</span></span>(<span>params)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@validator(&#34;params&#34;)
def validate_params(cls, params):
    alpha, beta, gamma = params
    if alpha &lt;= 0 or alpha &gt; 1:
        raise TypeError(f&#34;alpha must be in the interval (0,1]&#34;)
    if (beta &lt; 0 or beta &gt; 1) or (gamma &lt; 0 or gamma &gt; 1):
        raise TypeError(f&#34;beta, gamma must be in the interval [0,1]&#34;)
    else:
        return params</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="riskmodels.bivariate.AsymmetricLogistic.simulate_model"><code class="name flex">
<span>def <span class="ident">simulate_model</span></span>(<span>self, size:Â int, params:Â np.ndarray, quantile_threshold:Â float) â€‘>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Simulate asymmetric logistic exceedance model variates in Frechet scale using a method inspired by the one outlined in 'Simulating Multivariate Extreme Value Distributions of Logistic Type' by Stephenson (2003).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code></dt>
<dd>Sample size</dd>
<dt><strong><code>params</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>array with dependence and asymmetry parameters</dd>
<dt><strong><code>quantile_threshold</code></strong> :&ensp;<code>float</code></dt>
<dd>Quantile threshold for simulated exceedances</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>Simulated sample</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def simulate_model(self, size: int, params: np.ndarray, quantile_threshold: float) -&gt; np.ndarray:
    &#34;&#34;&#34;Simulate asymmetric logistic exceedance model variates in Frechet scale using a method inspired by the one outlined in &#39;Simulating Multivariate Extreme Value Distributions of Logistic Type&#39; by Stephenson (2003).
    
    Args:
        size (int): Sample size
        params (np.ndarray): array with dependence and asymmetry parameters
        quantile_threshold (float): Quantile threshold for simulated exceedances
    
    Returns:
        np.ndarray: Simulated sample
    &#34;&#34;&#34;

    # Gumbel scales are used instead of Frechet in this method, and a the existing logistic model is used as a baseline sampler
    # in Gumbel scales asymmetry constants are offsets instead of rescaling factors
    gumbel_logistic = Logistic.simulate_model
    threshold = gumbel.ppf(quantile_threshold)
    #threshold from which we want to sample
    target_th = np.array([threshold, threshold]).reshape((1,2))
    # unpack params
    alpha, beta, gamma = params

    def logistic_exs(size: int, alpha: float, a: float, b: float) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Samples a non-deterministic number of exceedances from a logistic exceedance model. A smaller , symmetric proxy threshold is used to simulate variates that are later offset to match the target threshold. Because the proxy threshold is symmetric but the target threshold may not be, some samples may be discarded. The expected sample size is the passed size parameter. The target threshold value comes from outer scope.
        
        Args:
            size (int): Sample size
            alpha (float): Dependence parameter
            a (float): asymmetry parameter for first component
            b (float): Asymmetry parameter for second component
        
        Returns:
            np.ndarray: Simulated sample
        
        &#34;&#34;&#34;
        alpha_param = np.array([alpha])
        ## offset from asymmetry parameters
        offset = np.array([np.log(a), np.log(b)]).reshape((1,2))
        offset_th = target_th - offset 
        #compute lower symmetric threshold to use as a proxy when sampling (some samples may be dropped)
        min_offset = np.min(offset_th)
        effective_th = min_offset * np.ones((2,)).reshape((1,2)) 
        effective_q = gumbel.cdf(min_offset)
        # compute average sample drop rate
        s_effective = 1 - Logistic.unconditioned_cdf(alpha_param, effective_th)
        s_offset = 1 - Logistic.unconditioned_cdf(alpha_param, offset_th)
        drop_rate = (s_effective - s_offset)/s_effective        
        if drop_rate &gt;= 1 or drop_rate &lt; 0:
            raise Exception(f&#34;Invalid drop rate ({drop_rate})&#34;)
        effective_size = int(size/(1-drop_rate))
        # sample from logistic model
        b2 = gumbel_logistic(effective_size, alpha_param, effective_q)
        # skew to symmetric logistic parameters
        b2[:,0] += np.log(a)
        b2[:,1] += np.log(b)
        # drop samples outside target region
        target_th1, target_th2 = target_th.reshape(-1)
        exs_idx = np.logical_or(b2[:,0] &gt; target_th1, b2[:,1] &gt; target_th2)
        b2 = b2[exs_idx,:]
        return b2

    def logistic_non_exs(size: int, alpha: float, a: float, b: float) -&gt; np.ndarray:
        &#34;&#34;&#34;
        same as above for non-exceedances
        
        Args:
            size (int): Sample size
            alpha (float): Dependence parameter
            a (float): asymmetry parameter for first component
            b (float): Asymmetry parameter for second component
        
        Returns:
            np.ndarray: Simulated sample
        
        &#34;&#34;&#34;
        alpha_param = np.array([alpha])
        ## offset from asymmetry parameters
        offset = np.array([np.log(a), np.log(b)]).reshape((1,2))
        offset_th = target_th - offset 
        #compute lower symmetric threshold to use as a proxy when sampling (some samples may be dropped)
        min_offset = np.min(offset_th)
        effective_th = min_offset * np.ones((2,)).reshape((1,2)) 
        # compute average sample drop rate
        drop_rate = 1 - Logistic.unconditioned_cdf(alpha_param, effective_th)    
        if drop_rate &gt;= 1 or drop_rate &lt; 0:
            raise Exception(f&#34;Invalid drop rate ({drop_rate})&#34;)
        effective_size = int(size/(1-drop_rate))
        # sample from logistic model
        b2 = gumbel_logistic(effective_size, alpha_param, 0)
        # skew to symmetric logistic parameters
        b2[:,0] += np.log(a)
        b2[:,1] += np.log(b)
        # drop samples outside target region
        target_th1, target_th2 = target_th.reshape(-1)
        exs_idx = np.logical_and(b2[:,0] &lt;= target_th1, b2[:,1] &lt;= target_th2)
        b2 = b2[exs_idx,:]
        return b2

    def sample(
        sampler: t.Callable,
        size: int, 
        alpha: float, 
        a: float, 
        b: float) -&gt; np.ndarray:
        &#34;&#34;&#34;Wrapper for the functions above that makes the sample size deterministic
        
        Args:
            size (int): Sample size
            alpha (float): dependence parameter
            a (float): asymmetry parameter for first component
            b (float): asymmetry parameter for second component
        
        Returns:
            np.ndarray: Sample
        &#34;&#34;&#34;
        # get bivariate samples ($B_2$ in the referenced paper)
        #
        b2 = sampler(size, alpha, a, b)
        while len(b2) &lt; size:
            k = size - len(b2)
            updated_size = max(100,2*k)
            b2 = np.concatenate([b2, sampler(updated_size, alpha, a, b)], axis=0)
        b2 = b2[0:size,:]
        return b2

    # The paper referenced above takes the maximum from logistic model samples and rescales them (or offsets them, in Gumbel scale) in order to sample from an asymmetric logistic.
    # B_1 = bivariate independent offset Gumbel samples
    # B_2 = bivariate logistic offset Gumbel
    # B_1 is independent from B_2. Let M = max(B_1, B_2), then M ~ asym. logistic Gumbel

    alpha_param = np.array([alpha])

    b2_offset = np.log(np.array([beta,gamma]))
    b1_offset = np.log(np.array([1-beta,1-gamma]))
    if quantile_threshold == 0:
        # if simulated region is unconditioned (i.e. the entire plane) use method from cited paper directly
        # independent gumbel variates (alpha = 1)
        b1 = gumbel_logistic(size=size, params=np.array([1]), quantile_threshold=0) + b1_offset
        # logistic gumbel variates
        b2 = gumbel_logistic(size=size, params=alpha_param, quantile_threshold=0) + b2_offset
        z = np.maximum(b1,b2)
    else:
        # Sampling exceedances requires some additional care:
        # exceedance in M &lt;=&gt; exceedance in B_1 or exceedance in B_2
        # exceedances can then be split in three cases: exceedance in B_1 alone, in B_2 alone or in both.
        # below the 3 cases are simulated separately and then concatenated 

        # work out sample size proportions for the three cases
        p_exs_b1 = float(1 - Logistic.unconditioned_cdf(np.array([1]), target_th - b1_offset))
        p_exs_b2 = float(1 - Logistic.unconditioned_cdf(alpha_param, target_th - b2_offset))
        p_exs_any = p_exs_b1 + p_exs_b2 - p_exs_b1*p_exs_b2
        p_exs = np.array([
            p_exs_b1*(1-p_exs_b2), 
            p_exs_b2*(1-p_exs_b1), 
            p_exs_b1*p_exs_b2])/p_exs_any #relative sample sizes for all three cases

        b1_exs_size, b2_exs_size, both_exs_size = (
            np.random.multinomial(n=size, pvals=p_exs, size=1)[0].astype(np.int32)
            )

        # sample 6 cases: (case 1, 2 or 3) * (exceedance or non-exceedance)
        b1_exs = sample(
            sampler=logistic_exs,
            size=b1_exs_size, 
            alpha=1, 
            a=1-beta, 
            b=1-gamma)

        b2_not_exs = sample(
            sampler=logistic_non_exs,
            size=b1_exs_size, 
            alpha=alpha, 
            a=beta, 
            b=gamma)

        b2_exs = sample(
            sampler=logistic_exs,
            size=b2_exs_size, 
            alpha=alpha, 
            a=beta, 
            b=gamma)

        b1_not_exs = sample(
            sampler = logistic_non_exs,
            size=b2_exs_size, 
            alpha=1, 
            a=1-beta, 
            b=1-gamma)

        b1_both_exs = sample(
            sampler=logistic_exs,
            size=both_exs_size, 
            alpha=1, 
            a=1-beta, 
            b=1-gamma)

        b2_both_exs = sample(
            sampler = logistic_exs,
            size=both_exs_size, 
            alpha=alpha, 
            a=beta, 
            b=gamma)

        # take elementwise maximum and concatenate
        z = np.concatenate([
            np.maximum(b1_exs, b2_not_exs),
            np.maximum(b2_exs, b1_not_exs),
            np.maximum(b1_both_exs, b2_both_exs)],
            axis=0)

    # return in canonical model scale, i.e. unit Frechet
    return np.exp(z)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="riskmodels.bivariate.ExceedanceDistribution" href="#riskmodels.bivariate.ExceedanceDistribution">ExceedanceDistribution</a></b></code>:
<ul class="hlist">
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.bundle" href="#riskmodels.bivariate.ExceedanceDistribution.bundle">bundle</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.cdf" href="#riskmodels.bivariate.BaseDistribution.cdf">cdf</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.data_to_model_dist" href="#riskmodels.bivariate.ExceedanceDistribution.data_to_model_dist">data_to_model_dist</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.fit" href="#riskmodels.bivariate.ExceedanceDistribution.fit">fit</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.hessian" href="#riskmodels.bivariate.ExceedanceDistribution.hessian">hessian</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.loglik" href="#riskmodels.bivariate.ExceedanceDistribution.loglik">loglik</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.model_to_data_dist" href="#riskmodels.bivariate.ExceedanceDistribution.model_to_data_dist">model_to_data_dist</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.pdf" href="#riskmodels.bivariate.ExceedanceDistribution.pdf">pdf</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.plot" href="#riskmodels.bivariate.BaseDistribution.plot">plot</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.plot_diagnostics" href="#riskmodels.bivariate.ExceedanceDistribution.plot_diagnostics">plot_diagnostics</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.simulate" href="#riskmodels.bivariate.BaseDistribution.simulate">simulate</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.unbundle" href="#riskmodels.bivariate.ExceedanceDistribution.unbundle">unbundle</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="riskmodels.bivariate.BaseDistribution"><code class="flex name class">
<span>class <span class="ident">BaseDistribution</span></span>
<span>(</span><span>**data:Â Any)</span>
</code></dt>
<dd>
<div class="desc"><p>Base interface for bivariate distributions</p>
<p>Create a new model by parsing and validating input data from keyword arguments.</p>
<p>Raises ValidationError if the input data cannot be parsed to form a valid model.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BaseDistribution(BaseModel, ABC):

    &#34;&#34;&#34;Base interface for bivariate distributions&#34;&#34;&#34;

    _allowed_scalar_types = (int, float, np.int64, np.int32, np.float32, np.float64)
    _figure_color_palette = [&#34;tab:cyan&#34;, &#34;deeppink&#34;]
    _error_tol = 1e-6

    data: t.Optional[np.ndarray] = None

    class Config:
        arbitrary_types_allowed = True

    def __repr__(self):
        return &#34;Base distribution object&#34;

    def __str__(self):
        return self.__repr__()

    @abstractmethod
    def pdf(self, x: np.ndarray) -&gt; float:
        &#34;&#34;&#34;Evaluate probability density function&#34;&#34;&#34;
        pass

    @abstractmethod
    def cdf(self, x: np.ndarray):
        &#34;&#34;&#34;Evaluate cumulative distribution function&#34;&#34;&#34;
        pass

    @abstractmethod
    def simulate(self, size: int):
        &#34;&#34;&#34;Simulate from bivariate distribution&#34;&#34;&#34;
        pass

    def plot(self, size: int = 1000) -&gt; matplotlib.figure.Figure:
        &#34;&#34;&#34;Sample distribution and produce scatterplots and histograms

        Args:
            size (int, optional): Sample size

        Returns:
            matplotlib.figure.Figure: figure
        &#34;&#34;&#34;
        sample = self.simulate(size)

        x = sample[:, 0]
        y = sample[:, 1]

        # definitions for the axes
        left, width = 0.1, 0.65
        bottom, height = 0.1, 0.65
        spacing = 0.005

        rect_scatter = [left, bottom, width, height]
        rect_histx = [left, bottom + height + spacing, width, 0.2]
        rect_histy = [left + width + spacing, bottom, 0.2, height]

        # start with a rectangular Figure
        fig = plt.figure(figsize=(8, 8))

        ax_scatter = plt.axes(rect_scatter)
        ax_scatter.tick_params(direction=&#34;in&#34;, top=True, right=True)
        ax_histx = plt.axes(rect_histx)
        ax_histx.tick_params(direction=&#34;in&#34;, labelbottom=False)
        ax_histy = plt.axes(rect_histy)
        ax_histy.tick_params(direction=&#34;in&#34;, labelleft=False)

        # the scatter plot:
        ax_scatter.scatter(x, y, color=self._figure_color_palette[0], alpha=0.35)

        # now determine nice limits by hand:
        # binwidth = 0.25
        # lim = np.ceil(np.abs([x, y]).max() / binwidth) * binwidth
        # ax_scatter.set_xlim((-lim, lim))
        # ax_scatter.set_ylim((-lim, lim))

        # bins = np.arange(-lim, lim + binwidth, binwidth)
        ax_histx.hist(
            x, bins=25, color=self._figure_color_palette[0], edgecolor=&#34;white&#34;
        )
        # plt.title(f&#34;Scatter plot from {np.round(size/1000,1)}K simulated samples&#34;)
        ax_histy.hist(
            y,
            bins=25,
            orientation=&#34;horizontal&#34;,
            color=self._figure_color_palette[0],
            edgecolor=&#34;white&#34;,
        )

        # ax_histx.set_xlim(ax_scatter.get_xlim())
        # ax_histy.set_ylim(ax_scatter.get_ylim())
        plt.tight_layout()
        return fig</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>pydantic.main.BaseModel</li>
<li>pydantic.utils.Representation</li>
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="riskmodels.bivariate.Empirical" href="#riskmodels.bivariate.Empirical">Empirical</a></li>
<li><a title="riskmodels.bivariate.ExceedanceDistribution" href="#riskmodels.bivariate.ExceedanceDistribution">ExceedanceDistribution</a></li>
<li><a title="riskmodels.bivariate.Independent" href="#riskmodels.bivariate.Independent">Independent</a></li>
<li><a title="riskmodels.bivariate.Mixture" href="#riskmodels.bivariate.Mixture">Mixture</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="riskmodels.bivariate.BaseDistribution.Config"><code class="name">var <span class="ident">Config</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="riskmodels.bivariate.BaseDistribution.data"><code class="name">var <span class="ident">data</span> :Â Optional[numpy.ndarray]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="riskmodels.bivariate.BaseDistribution.cdf"><code class="name flex">
<span>def <span class="ident">cdf</span></span>(<span>self, x:Â np.ndarray)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate cumulative distribution function</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def cdf(self, x: np.ndarray):
    &#34;&#34;&#34;Evaluate cumulative distribution function&#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="riskmodels.bivariate.BaseDistribution.pdf"><code class="name flex">
<span>def <span class="ident">pdf</span></span>(<span>self, x:Â np.ndarray) â€‘>Â float</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate probability density function</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def pdf(self, x: np.ndarray) -&gt; float:
    &#34;&#34;&#34;Evaluate probability density function&#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="riskmodels.bivariate.BaseDistribution.plot"><code class="name flex">
<span>def <span class="ident">plot</span></span>(<span>self, size:Â intÂ =Â 1000) â€‘>Â matplotlib.figure.Figure</span>
</code></dt>
<dd>
<div class="desc"><p>Sample distribution and produce scatterplots and histograms</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Sample size</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>matplotlib.figure.Figure</code></dt>
<dd>figure</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot(self, size: int = 1000) -&gt; matplotlib.figure.Figure:
    &#34;&#34;&#34;Sample distribution and produce scatterplots and histograms

    Args:
        size (int, optional): Sample size

    Returns:
        matplotlib.figure.Figure: figure
    &#34;&#34;&#34;
    sample = self.simulate(size)

    x = sample[:, 0]
    y = sample[:, 1]

    # definitions for the axes
    left, width = 0.1, 0.65
    bottom, height = 0.1, 0.65
    spacing = 0.005

    rect_scatter = [left, bottom, width, height]
    rect_histx = [left, bottom + height + spacing, width, 0.2]
    rect_histy = [left + width + spacing, bottom, 0.2, height]

    # start with a rectangular Figure
    fig = plt.figure(figsize=(8, 8))

    ax_scatter = plt.axes(rect_scatter)
    ax_scatter.tick_params(direction=&#34;in&#34;, top=True, right=True)
    ax_histx = plt.axes(rect_histx)
    ax_histx.tick_params(direction=&#34;in&#34;, labelbottom=False)
    ax_histy = plt.axes(rect_histy)
    ax_histy.tick_params(direction=&#34;in&#34;, labelleft=False)

    # the scatter plot:
    ax_scatter.scatter(x, y, color=self._figure_color_palette[0], alpha=0.35)

    # now determine nice limits by hand:
    # binwidth = 0.25
    # lim = np.ceil(np.abs([x, y]).max() / binwidth) * binwidth
    # ax_scatter.set_xlim((-lim, lim))
    # ax_scatter.set_ylim((-lim, lim))

    # bins = np.arange(-lim, lim + binwidth, binwidth)
    ax_histx.hist(
        x, bins=25, color=self._figure_color_palette[0], edgecolor=&#34;white&#34;
    )
    # plt.title(f&#34;Scatter plot from {np.round(size/1000,1)}K simulated samples&#34;)
    ax_histy.hist(
        y,
        bins=25,
        orientation=&#34;horizontal&#34;,
        color=self._figure_color_palette[0],
        edgecolor=&#34;white&#34;,
    )

    # ax_histx.set_xlim(ax_scatter.get_xlim())
    # ax_histy.set_ylim(ax_scatter.get_ylim())
    plt.tight_layout()
    return fig</code></pre>
</details>
</dd>
<dt id="riskmodels.bivariate.BaseDistribution.simulate"><code class="name flex">
<span>def <span class="ident">simulate</span></span>(<span>self, size:Â int)</span>
</code></dt>
<dd>
<div class="desc"><p>Simulate from bivariate distribution</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def simulate(self, size: int):
    &#34;&#34;&#34;Simulate from bivariate distribution&#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="riskmodels.bivariate.Empirical"><code class="flex name class">
<span>class <span class="ident">Empirical</span></span>
<span>(</span><span>**data:Â Any)</span>
</code></dt>
<dd>
<div class="desc"><p>Bivariate empirical distribution induced by a sample of observed data</p>
<p>Create a new model by parsing and validating input data from keyword arguments.</p>
<p>Raises ValidationError if the input data cannot be parsed to form a valid model.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Empirical(BaseDistribution):

    &#34;&#34;&#34;Bivariate empirical distribution induced by a sample of observed data&#34;&#34;&#34;

    data: np.ndarray
    pdf_values: np.ndarray

    _exceedance_models = {
    &#34;logistic&#34;: Logistic, 
    &#34;gaussian&#34;: Gaussian, 
    &#34;asymmetric logistic&#34;: AsymmetricLogistic,
    &#34;logistic gp&#34;: LogisticGP
    }

    def __repr__(self):
        return f&#34;Bivariate empirical distribution with {len(self.data)} points&#34;

    @validator(&#34;pdf_values&#34;, allow_reuse=True)
    def check_pdf_values(cls, pdf_values):
        if np.any(pdf_values &lt; -cls._error_tol):
            raise ValueError(&#34;There are negative pdf values&#34;)
        if not np.isclose(np.sum(pdf_values), 1, atol=cls._error_tol):
            print(f&#34;sum: {np.sum(pdf_values)}, pdf vals: {pdf_values}&#34;)
            raise ValueError(&#34;pdf values don&#39;t sum 1&#34;)
        # pdf_values = np.clip(pdf_values, a_min = 0.0, a_max = 1.0)
        # # normalise
        # pdf_values = pdf_values/np.sum(pdf_values)

        return pdf_values

    @classmethod
    def from_data(cls, data: np.ndarray):
        &#34;&#34;&#34;Instantiate an empirical distribution from an n x 2 data matrix

        Args:
            data (np.ndarray): observed data


        &#34;&#34;&#34;
        if (
            not isinstance(data, np.ndarray)
            or len(data.shape) != 2
            or data.shape[1] != 2
        ):
            raise ValueError(&#34;data must be an n x 2 numpy array&#34;)

        n = len(data)
        return Empirical(
            data=data, pdf_values=1.0 / n * np.ones((n,), dtype=np.float64)
        )

    def pdf(self, x: np.ndarray):

        return np.mean(self.data == x.reshape((1, 2)))

    def cdf(self, x: np.ndarray):
        if len(x.shape) &gt; 1:
            return np.array([self.cdf(elem) for elem in x])

        u = self.data &lt;= x.reshape((1, 2))  # componentwise comparison
        v = (
            u.dot(np.ones((2, 1))) &gt;= 2
        )  # equals 1 if and only if both components are below x
        return np.mean(v)

    def simulate(self, size: int):
        n = len(self.data)
        idx = np.random.choice(n, size=size)
        return self.data[idx]

    def get_marginals(self):

        return univar.Empirical.from_data(self.data[:, 0]), univar.Empirical.from_data(
            self.data[:, 1]
        )

    def fit_tail_model(
        self,
        model: str,
        quantile_threshold: float,
        margin1: univar.BaseDistribution = None,
        margin2: univar.BaseDistribution = None,
    ):
        &#34;&#34;&#34;Fits a parametric model for threshold exceedances in the data. For a given threshold \\( u \\), exceedances are defined as vectors \\(Z\\) such that \\( \\max\\{Z_1,Z_2\\} &gt; u \\), this is, an exceedance in at least one component, and encompasses an inverted L-shaped subset of Euclidean space.
        Currently, logistic and Gaussian models are available, with the former exhibiting asymptotic dependence, a strong type of dependence between extreme occurrences across components, and the latter exhibiting asymptotic independence, in which extremes occur relatively independently across components.

        Args:
            model (str): name of selected model, currently one of &#39;gaussian&#39; or &#39;logistic&#39; or &#39;asymmetric logistic&#39;. For more information, see `Gaussian`,  `Logistic` and `AsymmetricLogistic` classes.
            margin1 (univar.BaseDistribution, optional): Marginal distribution for first component. If not provided, a semiparametric model with a fitted Generalised Pareto upper tail is used.
            margin2 (univar.BaseDistribution, optional): Marginal distribution for second component. If not provided, a semiparametric model with a fitted Generalised Pareto upper tail is used.
            quantile_threshold (float): Quantile threshold to use for the definition of exceedances

        Returns:
            ExceedanceModel

        &#34;&#34;&#34;
        if model not in self._exceedance_models:
            raise ValueError(f&#34;model must be one of {self._exceedance_models}&#34;)

        if margin1 is None:

            margin1, _ = self.get_marginals()
            margin1 = margin1.fit_tail_model(threshold=margin1.ppf(quantile_threshold))
            warnings.warn(
                f&#34;First marginal not provided. Fitting tail model using provided quantile threshold ({quantile_threshold} =&gt; {margin1.ppf(quantile_threshold)})&#34;,
                stacklevel=2,
            )

        if margin2 is None:

            _, margin2 = self.get_marginals()
            margin2 = margin2.fit_tail_model(threshold=margin2.ppf(quantile_threshold))
            warnings.warn(
                f&#34;Second marginal not provided. Fitting tail model using provided quantile threshold ({quantile_threshold} =&gt; {margin2.ppf(quantile_threshold)})&#34;,
                stacklevel=2,
            )

        data = self.data

        x = data[:, 0]
        y = data[:, 1]

        exceedance_idx = np.logical_or(
            x &gt; margin1.ppf(quantile_threshold), y &gt; margin2.ppf(quantile_threshold)
        )

        exceedances = data[exceedance_idx]

        exceedance_model = self._exceedance_models[model].fit(
            data=exceedances,
            quantile_threshold=quantile_threshold,
            margin1=margin1,
            margin2=margin2,
        )

        empirical_model = Empirical.from_data(self.data[np.logical_not(exceedance_idx)])

        p = np.mean(np.logical_not(exceedance_idx))

        return ExceedanceModel(
            distributions=[empirical_model, exceedance_model],
            weights=np.array([p, 1 - p]),
        )

    def test_asymptotic_dependence(
        self, 
        quantile_threshold: float = 0.95, 
        prior: t.Optional[str] = &#34;jeffreys&#34;) -&gt; float:
        &#34;&#34;&#34;Computes a Savage-Dickey ratio for the coefficient of tail dependence \\(\\eta\\) to test the hypothesis of asymptotic dependence (See &#39;Statistics of Extremes&#39; by Beirlant, page 345-346). The hypothesis space is \\(\\eta \\in [0,1]\\) with \\(\\eta = 1\\) corresponding to asymptotic dependence. The posterior density is approximated through Gaussian Kernel density estimation.

        Args:
            quantile_threshold (float, optional): Quantile threshold over which the coefficient of tail dependence is to be estimated.
            log_prior (str, optional): Name of prior distribution to use for Bayesian inference. must be one of &#39;flat&#39;, which uses a flat prior on the support, or &#39;jeffreys&#39; which uses an uninformative Jeffreys prior; defaults to &#39;jeffreys&#39;.

        Returns:
            float: Savage-Dickey ratio. If larger than 1, this favors the asymptotic dependence hypothesis and vice versa.
        &#34;&#34;&#34;
        def flat_prior(theta):
            scale, shape = theta
            if scale &gt; 0 and shape &gt;= 0 and shape &lt;= 1:
                return 0.0
            else:
                return -np.Inf

        def jeffreys_prior(theta):
            scale, shape = theta
            if scale &gt; 0 and shape &gt;= 0 and shape &lt;= 1:
                return -np.log(scale) - np.log(1+shape) - 0.5*np.log(1+2*shape)
            else:
                return -np.Inf

        log_priors = {
            &#34;flat&#34;: flat_prior,
            &#34;jeffreys&#34;: jeffreys_prior
        }
        # Savage-Dickey ratios use the prior marginal density for eta = 1. Compute it for both priors
        prior_density = {
            &#34;flat&#34;: 1, # prior is uniform in [0,1] for eta
            &#34;jeffreys&#34;: np.exp(log_priors[&#34;jeffreys&#34;]([1,1]))/(np.pi/6) #constant factor is pi/6 (integral in [0,1])
        }

        try:
            log_prior = log_priors[prior]
        except KeyError as e:
            raise ValueError(f&#34;Prior name not recognised. Must be one of {log_priors.keys()}&#34;)

        ### compute savage-dickey density ratio
        # Use generalised pareto to fit tails
        x, y = self.data.T
        x_dist, y_dist = univar.Empirical.from_data(x), univar.Empirical.from_data(y)
        x_dist, y_dist = x_dist.fit_tail_model(
            x_dist.ppf(quantile_threshold)
        ), y_dist.fit_tail_model(y_dist.ppf(quantile_threshold))

        # transform to approximate standard Frechet margins and map to test data t
        u1, u2 = x_dist.cdf(x), y_dist.cdf(y)
        z1, z2 = -1 / np.log(u1), -1 / np.log(u2)
        t = np.minimum(z1, z2)

        t_dist = univar.Empirical.from_data(t)
        # Pass initial point that enforces theoretical constraints of 0 &lt;= eta &lt;= 1. Approximation inaccuracies from the transformation to Frechet margins and MLE estimation can violate the bounds.
        mle_tail = t_dist.fit_tail_model(t_dist.ppf(quantile_threshold))
        x0 = np.array([mle_tail.tail.scale, max(0, min(0.99, mle_tail.tail.shape))])
        # sample posterior
        t_dist = t_dist.fit_tail_model(
            t_dist.ppf(quantile_threshold), bayesian=True, log_prior=log_prior, x0=x0
        )

        # approximate posterior distribution through Kernel density estimation. Evaluating it on 1 gives us the savage-dickey ratio
        #return gaussian_kde(t_dist.tail.shapes).evaluate(1)[0]

        return gaussian_kde(t_dist.tail.shapes).evaluate(1)[0]/prior_density[prior]

    @classmethod
    def pickands(self, p: np.ndarray, data: np.ndarray, quantile_threshold: float = 0.95) -&gt; np.ndarray:
        &#34;&#34;&#34;Non-parametric Pickands dependence function approximation based on Hall and Tajvidi (2000).

        Args:
            p (np.ndarray): Pickands dependence function arguments
            data: (np.ndarray): data matrix of n x 2
            quantile_threshold (float, optional): Quantile threshold over which Pickands dependence will be approximated.

        Returns:
            np.ndarray: Nonparametric estimate of the Pickands dependence function
        &#34;&#34;&#34;
        # find joint extremes
        if np.any(p &gt; 1) or np.any(p &lt; 0):
            raise ValueError(&#34;All argument values must be in [0,1]&#34;)

        x, y = data.T
        joint_extremes_idx = np.logical_and(
            x &gt; np.quantile(x, quantile_threshold),
            y &gt; np.quantile(y, quantile_threshold),
        )
        n = np.sum(joint_extremes_idx)

        # compute nonparametric scores
        x_exs, y_exs = x[joint_extremes_idx], y[joint_extremes_idx]
        x_exs_model, y_exs_model = (
            univar.Empirical.from_data(x_exs),
            univar.Empirical.from_data(y_exs),
        )

        # normalise to avoid copula values on the border of the unit square
        u1, u2 = n / (n + 1) * x_exs_model.cdf(x_exs), n / (n + 1) * y_exs_model.cdf(
            y_exs
        )
        # map to exponential
        s, t = -np.log(u1), -np.log(u2)

        pk = []
        for p_ in p:
            a = (s / np.mean(s)) / (1 - p_)
            b = (t / np.mean(t)) / p_
            pk.append(1.0 / np.mean(np.minimum(a, b)))
            
        return np.array(pk)

    def plot_pickands(self, quantile_threshold: float = 0.95)  -&gt; matplotlib.figure.Figure:
        &#34;&#34;&#34;Returns a plot of the empirical Pickands dependence function induced by joint exceedances above the specified quantile threshold. 
        The Pickands dependence function \\(A: [0,1] \\to [1/2,1]\\) is convex and bounded by \\(\\max\\{t,1-t\\} \\leq A(t) \\leq 1\\); it can be used to assess extremal dependence, as there is a one-to-one correspondence between \\(A(t)\\) and extremal copulas; the closer it is to its lower bound, the stronger the extremal dependence. Conversely, for asymptotically independent data \\(A(t) = 1\\).
        The non-parametric approximation used here is based on Hall and Tajvidi (2000).

        Args:
            quantile_threshold (float, optional): Quantile threshold over which Pickands dependence will be approximated.
        
        Returns:
            matplotlib.figure.Figure: figure
        &#34;&#34;&#34;
        fig = plt.figure(figsize=(5, 5))

        x = np.linspace(0, 1, 101)
        pk = self.pickands(x, self.data)

        plt.plot(x, pk, color=&#34;darkorange&#34;, label=&#34;Empirical&#34;)
        plt.plot(x, np.maximum(1 - x, x), linestyle=&#34;dashed&#34;, color=&#34;black&#34;)
        plt.plot(x, np.ones((len(x),)), linestyle=&#34;dashed&#34;, color=&#34;black&#34;)
        plt.xlabel(&#34;t&#34;)
        plt.ylabel(&#34;A(t)&#34;)
        plt.title(&#34;Empirical Pickands dependence of joint exceedances&#34;)
        plt.grid()
        return fig

    def plot_chi(self, direct_estimate: bool = True)  -&gt; matplotlib.figure.Figure:
        &#34;&#34;&#34;Returns a plot of the empirical estimate of the coefficient of asymptotic dependence , defined as \\( \\chi = \\ \\lim_{p \\to 1} \\mathbb{P}(Y &gt; q_y(p) \\,|\\, X &gt; q_x (p)) \\), where \\( q_x, q_y\\) are the corresponding quantiles for a given probability level \\(p\\). There is asymptotic dependence when \\(\\chi &gt; 0\\) and vice versa.

        Args:
            direct_estimate (optional, bool): if True, use the empirical copula&#39;s probability estimates directly. Otherwise use the approximation given by \\(\\chi = \\lim_{u \\to 1} 2 - \\log C(u,u) / \\log(u) \\) where \\(C(u,u)\\) is the empirical copula.
        
        Returns:
            matplotlib.figure.Figure: figure
        &#34;&#34;&#34;
        def chi(p: np.ndarray) -&gt; t.Tuple[np.ndarray, np.ndarray]:
            &#34;&#34;&#34;Returns chi estimates and the corresponding estimate&#39;s standard deviation
            
            Args:
                t (np.ndarray): input probability levels
            
            Returns:
                t.Tuple[np.ndarray, np.ndarray]: estimate and standard deviation arrays
            &#34;&#34;&#34;
            x, y = self.get_marginals()            
            n = len(x.data)
            # map to rescaled copula values in the open set (0,1)
            u1, u2 = x.cdf(x.data) * n/(n+1), y.cdf(y.data) * n/(n+1)
            empirical_copula = Empirical.from_data(np.stack([u1,u2], axis=1))
            if direct_estimate:
                joint_p = 1 - 2*p + empirical_copula.cdf(np.stack([p,p], axis=1))
                estimate = joint_p/(1-p)
                n_obs = len(x.data) * (1-p)
                std = np.sqrt(estimate*(1-estimate)/n_obs)
            else:
                vals = empirical_copula.cdf(np.stack([p,p], axis=1))
                estimate = 2 - np.log(vals)/np.log(p)
                # standard error approximation using delta method
                std = np.sqrt(vals*(1-vals)/n * 1/np.log(p)**2 * 1/vals**2)
            return estimate, std


        fig = plt.figure(figsize=(5, 5))
        p = np.linspace(0.01, 0.99, 99)
        chi_central, chi_std = chi(p)
        ci_l, ci_u = chi_central - 1.96*chi_std, chi_central + 1.96*chi_std

        plt.plot(p, chi_central, color=self._figure_color_palette[0])
        plt.scatter(p, chi_central, color=self._figure_color_palette[0])
        plt.fill_between(p, ci_l, ci_u, linestyle=&#34;dashed&#34;, color=self._figure_color_palette[1], alpha = 0.2)
        plt.xlabel(&#34;quantiles&#34;)
        plt.ylabel(&#34;Chi(u)&#34;)
        plt.grid()
        return fig</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="riskmodels.bivariate.BaseDistribution" href="#riskmodels.bivariate.BaseDistribution">BaseDistribution</a></li>
<li>pydantic.main.BaseModel</li>
<li>pydantic.utils.Representation</li>
<li>abc.ABC</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="riskmodels.bivariate.Empirical.data"><code class="name">var <span class="ident">data</span> :Â numpy.ndarray</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="riskmodels.bivariate.Empirical.pdf_values"><code class="name">var <span class="ident">pdf_values</span> :Â numpy.ndarray</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="riskmodels.bivariate.Empirical.check_pdf_values"><code class="name flex">
<span>def <span class="ident">check_pdf_values</span></span>(<span>pdf_values)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@validator(&#34;pdf_values&#34;, allow_reuse=True)
def check_pdf_values(cls, pdf_values):
    if np.any(pdf_values &lt; -cls._error_tol):
        raise ValueError(&#34;There are negative pdf values&#34;)
    if not np.isclose(np.sum(pdf_values), 1, atol=cls._error_tol):
        print(f&#34;sum: {np.sum(pdf_values)}, pdf vals: {pdf_values}&#34;)
        raise ValueError(&#34;pdf values don&#39;t sum 1&#34;)
    # pdf_values = np.clip(pdf_values, a_min = 0.0, a_max = 1.0)
    # # normalise
    # pdf_values = pdf_values/np.sum(pdf_values)

    return pdf_values</code></pre>
</details>
</dd>
<dt id="riskmodels.bivariate.Empirical.from_data"><code class="name flex">
<span>def <span class="ident">from_data</span></span>(<span>data:Â np.ndarray)</span>
</code></dt>
<dd>
<div class="desc"><p>Instantiate an empirical distribution from an n x 2 data matrix</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>observed data</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def from_data(cls, data: np.ndarray):
    &#34;&#34;&#34;Instantiate an empirical distribution from an n x 2 data matrix

    Args:
        data (np.ndarray): observed data


    &#34;&#34;&#34;
    if (
        not isinstance(data, np.ndarray)
        or len(data.shape) != 2
        or data.shape[1] != 2
    ):
        raise ValueError(&#34;data must be an n x 2 numpy array&#34;)

    n = len(data)
    return Empirical(
        data=data, pdf_values=1.0 / n * np.ones((n,), dtype=np.float64)
    )</code></pre>
</details>
</dd>
<dt id="riskmodels.bivariate.Empirical.pickands"><code class="name flex">
<span>def <span class="ident">pickands</span></span>(<span>p:Â np.ndarray, data:Â np.ndarray, quantile_threshold:Â floatÂ =Â 0.95) â€‘>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Non-parametric Pickands dependence function approximation based on Hall and Tajvidi (2000).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>p</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Pickands dependence function arguments</dd>
<dt><strong><code>data</code></strong></dt>
<dd>(np.ndarray): data matrix of n x 2</dd>
<dt><strong><code>quantile_threshold</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Quantile threshold over which Pickands dependence will be approximated.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>Nonparametric estimate of the Pickands dependence function</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def pickands(self, p: np.ndarray, data: np.ndarray, quantile_threshold: float = 0.95) -&gt; np.ndarray:
    &#34;&#34;&#34;Non-parametric Pickands dependence function approximation based on Hall and Tajvidi (2000).

    Args:
        p (np.ndarray): Pickands dependence function arguments
        data: (np.ndarray): data matrix of n x 2
        quantile_threshold (float, optional): Quantile threshold over which Pickands dependence will be approximated.

    Returns:
        np.ndarray: Nonparametric estimate of the Pickands dependence function
    &#34;&#34;&#34;
    # find joint extremes
    if np.any(p &gt; 1) or np.any(p &lt; 0):
        raise ValueError(&#34;All argument values must be in [0,1]&#34;)

    x, y = data.T
    joint_extremes_idx = np.logical_and(
        x &gt; np.quantile(x, quantile_threshold),
        y &gt; np.quantile(y, quantile_threshold),
    )
    n = np.sum(joint_extremes_idx)

    # compute nonparametric scores
    x_exs, y_exs = x[joint_extremes_idx], y[joint_extremes_idx]
    x_exs_model, y_exs_model = (
        univar.Empirical.from_data(x_exs),
        univar.Empirical.from_data(y_exs),
    )

    # normalise to avoid copula values on the border of the unit square
    u1, u2 = n / (n + 1) * x_exs_model.cdf(x_exs), n / (n + 1) * y_exs_model.cdf(
        y_exs
    )
    # map to exponential
    s, t = -np.log(u1), -np.log(u2)

    pk = []
    for p_ in p:
        a = (s / np.mean(s)) / (1 - p_)
        b = (t / np.mean(t)) / p_
        pk.append(1.0 / np.mean(np.minimum(a, b)))
        
    return np.array(pk)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="riskmodels.bivariate.Empirical.fit_tail_model"><code class="name flex">
<span>def <span class="ident">fit_tail_model</span></span>(<span>self, model:Â str, quantile_threshold:Â float, margin1:Â univar.BaseDistributionÂ =Â None, margin2:Â univar.BaseDistributionÂ =Â None)</span>
</code></dt>
<dd>
<div class="desc"><p>Fits a parametric model for threshold exceedances in the data. For a given threshold <span><span class="MathJax_Preview"> u </span><script type="math/tex"> u </script></span>, exceedances are defined as vectors <span><span class="MathJax_Preview">Z</span><script type="math/tex">Z</script></span> such that <span><span class="MathJax_Preview"> \max\{Z_1,Z_2\} &gt; u </span><script type="math/tex"> \max\{Z_1,Z_2\} > u </script></span>, this is, an exceedance in at least one component, and encompasses an inverted L-shaped subset of Euclidean space.
Currently, logistic and Gaussian models are available, with the former exhibiting asymptotic dependence, a strong type of dependence between extreme occurrences across components, and the latter exhibiting asymptotic independence, in which extremes occur relatively independently across components.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>str</code></dt>
<dd>name of selected model, currently one of 'gaussian' or 'logistic' or 'asymmetric logistic'. For more information, see <code><a title="riskmodels.bivariate.Gaussian" href="#riskmodels.bivariate.Gaussian">Gaussian</a></code>,
<code><a title="riskmodels.bivariate.Logistic" href="#riskmodels.bivariate.Logistic">Logistic</a></code> and <code><a title="riskmodels.bivariate.AsymmetricLogistic" href="#riskmodels.bivariate.AsymmetricLogistic">AsymmetricLogistic</a></code> classes.</dd>
<dt><strong><code>margin1</code></strong> :&ensp;<code>univar.BaseDistribution</code>, optional</dt>
<dd>Marginal distribution for first component. If not provided, a semiparametric model with a fitted Generalised Pareto upper tail is used.</dd>
<dt><strong><code>margin2</code></strong> :&ensp;<code>univar.BaseDistribution</code>, optional</dt>
<dd>Marginal distribution for second component. If not provided, a semiparametric model with a fitted Generalised Pareto upper tail is used.</dd>
<dt><strong><code>quantile_threshold</code></strong> :&ensp;<code>float</code></dt>
<dd>Quantile threshold to use for the definition of exceedances</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>ExceedanceModel</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit_tail_model(
    self,
    model: str,
    quantile_threshold: float,
    margin1: univar.BaseDistribution = None,
    margin2: univar.BaseDistribution = None,
):
    &#34;&#34;&#34;Fits a parametric model for threshold exceedances in the data. For a given threshold \\( u \\), exceedances are defined as vectors \\(Z\\) such that \\( \\max\\{Z_1,Z_2\\} &gt; u \\), this is, an exceedance in at least one component, and encompasses an inverted L-shaped subset of Euclidean space.
    Currently, logistic and Gaussian models are available, with the former exhibiting asymptotic dependence, a strong type of dependence between extreme occurrences across components, and the latter exhibiting asymptotic independence, in which extremes occur relatively independently across components.

    Args:
        model (str): name of selected model, currently one of &#39;gaussian&#39; or &#39;logistic&#39; or &#39;asymmetric logistic&#39;. For more information, see `Gaussian`,  `Logistic` and `AsymmetricLogistic` classes.
        margin1 (univar.BaseDistribution, optional): Marginal distribution for first component. If not provided, a semiparametric model with a fitted Generalised Pareto upper tail is used.
        margin2 (univar.BaseDistribution, optional): Marginal distribution for second component. If not provided, a semiparametric model with a fitted Generalised Pareto upper tail is used.
        quantile_threshold (float): Quantile threshold to use for the definition of exceedances

    Returns:
        ExceedanceModel

    &#34;&#34;&#34;
    if model not in self._exceedance_models:
        raise ValueError(f&#34;model must be one of {self._exceedance_models}&#34;)

    if margin1 is None:

        margin1, _ = self.get_marginals()
        margin1 = margin1.fit_tail_model(threshold=margin1.ppf(quantile_threshold))
        warnings.warn(
            f&#34;First marginal not provided. Fitting tail model using provided quantile threshold ({quantile_threshold} =&gt; {margin1.ppf(quantile_threshold)})&#34;,
            stacklevel=2,
        )

    if margin2 is None:

        _, margin2 = self.get_marginals()
        margin2 = margin2.fit_tail_model(threshold=margin2.ppf(quantile_threshold))
        warnings.warn(
            f&#34;Second marginal not provided. Fitting tail model using provided quantile threshold ({quantile_threshold} =&gt; {margin2.ppf(quantile_threshold)})&#34;,
            stacklevel=2,
        )

    data = self.data

    x = data[:, 0]
    y = data[:, 1]

    exceedance_idx = np.logical_or(
        x &gt; margin1.ppf(quantile_threshold), y &gt; margin2.ppf(quantile_threshold)
    )

    exceedances = data[exceedance_idx]

    exceedance_model = self._exceedance_models[model].fit(
        data=exceedances,
        quantile_threshold=quantile_threshold,
        margin1=margin1,
        margin2=margin2,
    )

    empirical_model = Empirical.from_data(self.data[np.logical_not(exceedance_idx)])

    p = np.mean(np.logical_not(exceedance_idx))

    return ExceedanceModel(
        distributions=[empirical_model, exceedance_model],
        weights=np.array([p, 1 - p]),
    )</code></pre>
</details>
</dd>
<dt id="riskmodels.bivariate.Empirical.get_marginals"><code class="name flex">
<span>def <span class="ident">get_marginals</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_marginals(self):

    return univar.Empirical.from_data(self.data[:, 0]), univar.Empirical.from_data(
        self.data[:, 1]
    )</code></pre>
</details>
</dd>
<dt id="riskmodels.bivariate.Empirical.plot_chi"><code class="name flex">
<span>def <span class="ident">plot_chi</span></span>(<span>self, direct_estimate:Â boolÂ =Â True) â€‘>Â matplotlib.figure.Figure</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a plot of the empirical estimate of the coefficient of asymptotic dependence , defined as <span><span class="MathJax_Preview"> \chi = \ \lim_{p \to 1} \mathbb{P}(Y &gt; q_y(p) \,|\, X &gt; q_x (p)) </span><script type="math/tex"> \chi = \ \lim_{p \to 1} \mathbb{P}(Y > q_y(p) \,|\, X > q_x (p)) </script></span>, where <span><span class="MathJax_Preview"> q_x, q_y</span><script type="math/tex"> q_x, q_y</script></span> are the corresponding quantiles for a given probability level <span><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span>. There is asymptotic dependence when <span><span class="MathJax_Preview">\chi &gt; 0</span><script type="math/tex">\chi > 0</script></span> and vice versa.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>direct_estimate</code></strong> :&ensp;<code>optional, bool</code></dt>
<dd>if True, use the empirical copula's probability estimates directly. Otherwise use the approximation given by <span><span class="MathJax_Preview">\chi = \lim_{u \to 1} 2 - \log C(u,u) / \log(u) </span><script type="math/tex">\chi = \lim_{u \to 1} 2 - \log C(u,u) / \log(u) </script></span> where <span><span class="MathJax_Preview">C(u,u)</span><script type="math/tex">C(u,u)</script></span> is the empirical copula.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>matplotlib.figure.Figure</code></dt>
<dd>figure</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_chi(self, direct_estimate: bool = True)  -&gt; matplotlib.figure.Figure:
    &#34;&#34;&#34;Returns a plot of the empirical estimate of the coefficient of asymptotic dependence , defined as \\( \\chi = \\ \\lim_{p \\to 1} \\mathbb{P}(Y &gt; q_y(p) \\,|\\, X &gt; q_x (p)) \\), where \\( q_x, q_y\\) are the corresponding quantiles for a given probability level \\(p\\). There is asymptotic dependence when \\(\\chi &gt; 0\\) and vice versa.

    Args:
        direct_estimate (optional, bool): if True, use the empirical copula&#39;s probability estimates directly. Otherwise use the approximation given by \\(\\chi = \\lim_{u \\to 1} 2 - \\log C(u,u) / \\log(u) \\) where \\(C(u,u)\\) is the empirical copula.
    
    Returns:
        matplotlib.figure.Figure: figure
    &#34;&#34;&#34;
    def chi(p: np.ndarray) -&gt; t.Tuple[np.ndarray, np.ndarray]:
        &#34;&#34;&#34;Returns chi estimates and the corresponding estimate&#39;s standard deviation
        
        Args:
            t (np.ndarray): input probability levels
        
        Returns:
            t.Tuple[np.ndarray, np.ndarray]: estimate and standard deviation arrays
        &#34;&#34;&#34;
        x, y = self.get_marginals()            
        n = len(x.data)
        # map to rescaled copula values in the open set (0,1)
        u1, u2 = x.cdf(x.data) * n/(n+1), y.cdf(y.data) * n/(n+1)
        empirical_copula = Empirical.from_data(np.stack([u1,u2], axis=1))
        if direct_estimate:
            joint_p = 1 - 2*p + empirical_copula.cdf(np.stack([p,p], axis=1))
            estimate = joint_p/(1-p)
            n_obs = len(x.data) * (1-p)
            std = np.sqrt(estimate*(1-estimate)/n_obs)
        else:
            vals = empirical_copula.cdf(np.stack([p,p], axis=1))
            estimate = 2 - np.log(vals)/np.log(p)
            # standard error approximation using delta method
            std = np.sqrt(vals*(1-vals)/n * 1/np.log(p)**2 * 1/vals**2)
        return estimate, std


    fig = plt.figure(figsize=(5, 5))
    p = np.linspace(0.01, 0.99, 99)
    chi_central, chi_std = chi(p)
    ci_l, ci_u = chi_central - 1.96*chi_std, chi_central + 1.96*chi_std

    plt.plot(p, chi_central, color=self._figure_color_palette[0])
    plt.scatter(p, chi_central, color=self._figure_color_palette[0])
    plt.fill_between(p, ci_l, ci_u, linestyle=&#34;dashed&#34;, color=self._figure_color_palette[1], alpha = 0.2)
    plt.xlabel(&#34;quantiles&#34;)
    plt.ylabel(&#34;Chi(u)&#34;)
    plt.grid()
    return fig</code></pre>
</details>
</dd>
<dt id="riskmodels.bivariate.Empirical.plot_pickands"><code class="name flex">
<span>def <span class="ident">plot_pickands</span></span>(<span>self, quantile_threshold:Â floatÂ =Â 0.95) â€‘>Â matplotlib.figure.Figure</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a plot of the empirical Pickands dependence function induced by joint exceedances above the specified quantile threshold.
The Pickands dependence function <span><span class="MathJax_Preview">A: [0,1] \to [1/2,1]</span><script type="math/tex">A: [0,1] \to [1/2,1]</script></span> is convex and bounded by <span><span class="MathJax_Preview">\max\{t,1-t\} \leq A(t) \leq 1</span><script type="math/tex">\max\{t,1-t\} \leq A(t) \leq 1</script></span>; it can be used to assess extremal dependence, as there is a one-to-one correspondence between <span><span class="MathJax_Preview">A(t)</span><script type="math/tex">A(t)</script></span> and extremal copulas; the closer it is to its lower bound, the stronger the extremal dependence. Conversely, for asymptotically independent data <span><span class="MathJax_Preview">A(t) = 1</span><script type="math/tex">A(t) = 1</script></span>.
The non-parametric approximation used here is based on Hall and Tajvidi (2000).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>quantile_threshold</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Quantile threshold over which Pickands dependence will be approximated.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>matplotlib.figure.Figure</code></dt>
<dd>figure</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_pickands(self, quantile_threshold: float = 0.95)  -&gt; matplotlib.figure.Figure:
    &#34;&#34;&#34;Returns a plot of the empirical Pickands dependence function induced by joint exceedances above the specified quantile threshold. 
    The Pickands dependence function \\(A: [0,1] \\to [1/2,1]\\) is convex and bounded by \\(\\max\\{t,1-t\\} \\leq A(t) \\leq 1\\); it can be used to assess extremal dependence, as there is a one-to-one correspondence between \\(A(t)\\) and extremal copulas; the closer it is to its lower bound, the stronger the extremal dependence. Conversely, for asymptotically independent data \\(A(t) = 1\\).
    The non-parametric approximation used here is based on Hall and Tajvidi (2000).

    Args:
        quantile_threshold (float, optional): Quantile threshold over which Pickands dependence will be approximated.
    
    Returns:
        matplotlib.figure.Figure: figure
    &#34;&#34;&#34;
    fig = plt.figure(figsize=(5, 5))

    x = np.linspace(0, 1, 101)
    pk = self.pickands(x, self.data)

    plt.plot(x, pk, color=&#34;darkorange&#34;, label=&#34;Empirical&#34;)
    plt.plot(x, np.maximum(1 - x, x), linestyle=&#34;dashed&#34;, color=&#34;black&#34;)
    plt.plot(x, np.ones((len(x),)), linestyle=&#34;dashed&#34;, color=&#34;black&#34;)
    plt.xlabel(&#34;t&#34;)
    plt.ylabel(&#34;A(t)&#34;)
    plt.title(&#34;Empirical Pickands dependence of joint exceedances&#34;)
    plt.grid()
    return fig</code></pre>
</details>
</dd>
<dt id="riskmodels.bivariate.Empirical.test_asymptotic_dependence"><code class="name flex">
<span>def <span class="ident">test_asymptotic_dependence</span></span>(<span>self, quantile_threshold:Â floatÂ =Â 0.95, prior:Â t.Optional[str]Â =Â 'jeffreys') â€‘>Â float</span>
</code></dt>
<dd>
<div class="desc"><p>Computes a Savage-Dickey ratio for the coefficient of tail dependence <span><span class="MathJax_Preview">\eta</span><script type="math/tex">\eta</script></span> to test the hypothesis of asymptotic dependence (See 'Statistics of Extremes' by Beirlant, page 345-346). The hypothesis space is <span><span class="MathJax_Preview">\eta \in [0,1]</span><script type="math/tex">\eta \in [0,1]</script></span> with <span><span class="MathJax_Preview">\eta = 1</span><script type="math/tex">\eta = 1</script></span> corresponding to asymptotic dependence. The posterior density is approximated through Gaussian Kernel density estimation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>quantile_threshold</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Quantile threshold over which the coefficient of tail dependence is to be estimated.</dd>
<dt><strong><code>log_prior</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Name of prior distribution to use for Bayesian inference. must be one of 'flat', which uses a flat prior on the support, or 'jeffreys' which uses an uninformative Jeffreys prior; defaults to 'jeffreys'.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>Savage-Dickey ratio. If larger than 1, this favors the asymptotic dependence hypothesis and vice versa.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_asymptotic_dependence(
    self, 
    quantile_threshold: float = 0.95, 
    prior: t.Optional[str] = &#34;jeffreys&#34;) -&gt; float:
    &#34;&#34;&#34;Computes a Savage-Dickey ratio for the coefficient of tail dependence \\(\\eta\\) to test the hypothesis of asymptotic dependence (See &#39;Statistics of Extremes&#39; by Beirlant, page 345-346). The hypothesis space is \\(\\eta \\in [0,1]\\) with \\(\\eta = 1\\) corresponding to asymptotic dependence. The posterior density is approximated through Gaussian Kernel density estimation.

    Args:
        quantile_threshold (float, optional): Quantile threshold over which the coefficient of tail dependence is to be estimated.
        log_prior (str, optional): Name of prior distribution to use for Bayesian inference. must be one of &#39;flat&#39;, which uses a flat prior on the support, or &#39;jeffreys&#39; which uses an uninformative Jeffreys prior; defaults to &#39;jeffreys&#39;.

    Returns:
        float: Savage-Dickey ratio. If larger than 1, this favors the asymptotic dependence hypothesis and vice versa.
    &#34;&#34;&#34;
    def flat_prior(theta):
        scale, shape = theta
        if scale &gt; 0 and shape &gt;= 0 and shape &lt;= 1:
            return 0.0
        else:
            return -np.Inf

    def jeffreys_prior(theta):
        scale, shape = theta
        if scale &gt; 0 and shape &gt;= 0 and shape &lt;= 1:
            return -np.log(scale) - np.log(1+shape) - 0.5*np.log(1+2*shape)
        else:
            return -np.Inf

    log_priors = {
        &#34;flat&#34;: flat_prior,
        &#34;jeffreys&#34;: jeffreys_prior
    }
    # Savage-Dickey ratios use the prior marginal density for eta = 1. Compute it for both priors
    prior_density = {
        &#34;flat&#34;: 1, # prior is uniform in [0,1] for eta
        &#34;jeffreys&#34;: np.exp(log_priors[&#34;jeffreys&#34;]([1,1]))/(np.pi/6) #constant factor is pi/6 (integral in [0,1])
    }

    try:
        log_prior = log_priors[prior]
    except KeyError as e:
        raise ValueError(f&#34;Prior name not recognised. Must be one of {log_priors.keys()}&#34;)

    ### compute savage-dickey density ratio
    # Use generalised pareto to fit tails
    x, y = self.data.T
    x_dist, y_dist = univar.Empirical.from_data(x), univar.Empirical.from_data(y)
    x_dist, y_dist = x_dist.fit_tail_model(
        x_dist.ppf(quantile_threshold)
    ), y_dist.fit_tail_model(y_dist.ppf(quantile_threshold))

    # transform to approximate standard Frechet margins and map to test data t
    u1, u2 = x_dist.cdf(x), y_dist.cdf(y)
    z1, z2 = -1 / np.log(u1), -1 / np.log(u2)
    t = np.minimum(z1, z2)

    t_dist = univar.Empirical.from_data(t)
    # Pass initial point that enforces theoretical constraints of 0 &lt;= eta &lt;= 1. Approximation inaccuracies from the transformation to Frechet margins and MLE estimation can violate the bounds.
    mle_tail = t_dist.fit_tail_model(t_dist.ppf(quantile_threshold))
    x0 = np.array([mle_tail.tail.scale, max(0, min(0.99, mle_tail.tail.shape))])
    # sample posterior
    t_dist = t_dist.fit_tail_model(
        t_dist.ppf(quantile_threshold), bayesian=True, log_prior=log_prior, x0=x0
    )

    # approximate posterior distribution through Kernel density estimation. Evaluating it on 1 gives us the savage-dickey ratio
    #return gaussian_kde(t_dist.tail.shapes).evaluate(1)[0]

    return gaussian_kde(t_dist.tail.shapes).evaluate(1)[0]/prior_density[prior]</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="riskmodels.bivariate.BaseDistribution" href="#riskmodels.bivariate.BaseDistribution">BaseDistribution</a></b></code>:
<ul class="hlist">
<li><code><a title="riskmodels.bivariate.BaseDistribution.cdf" href="#riskmodels.bivariate.BaseDistribution.cdf">cdf</a></code></li>
<li><code><a title="riskmodels.bivariate.BaseDistribution.pdf" href="#riskmodels.bivariate.BaseDistribution.pdf">pdf</a></code></li>
<li><code><a title="riskmodels.bivariate.BaseDistribution.plot" href="#riskmodels.bivariate.BaseDistribution.plot">plot</a></code></li>
<li><code><a title="riskmodels.bivariate.BaseDistribution.simulate" href="#riskmodels.bivariate.BaseDistribution.simulate">simulate</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="riskmodels.bivariate.ExceedanceDistribution"><code class="flex name class">
<span>class <span class="ident">ExceedanceDistribution</span></span>
<span>(</span><span>**data:Â Any)</span>
</code></dt>
<dd>
<div class="desc"><p>Main interface for exceedance distributions, which are defined on a region of the form <span><span class="MathJax_Preview"> U \nleq u </span><script type="math/tex"> U \nleq u </script></span>, or equivalently <span><span class="MathJax_Preview"> \max\{U_1,U_2\} &gt; u </span><script type="math/tex"> \max\{U_1,U_2\} > u </script></span>.</p>
<p>Create a new model by parsing and validating input data from keyword arguments.</p>
<p>Raises ValidationError if the input data cannot be parsed to form a valid model.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ExceedanceDistribution(BaseDistribution):

    &#34;&#34;&#34;Main interface for exceedance distributions, which are defined on a region of the form \\( U \\nleq u \\), or equivalently \\( \\max\\{U_1,U_2\\} &gt; u \\).&#34;&#34;&#34;


    quantile_threshold: float

    margin1: t.Union[univar.BaseDistribution, continuous_dist]
    margin2: t.Union[univar.BaseDistribution, continuous_dist]

    # default method variables below are the same as for the logistic model
    # this does not matter as this class should not be instantiated directly
    params: t.Optional[np.ndarray] = Field(default_factory=lambda: np.zeros((1,), dtype=np.float32))
    _param_names = {0:&#34;alpha&#34;} #mapping from params array indices to names for diagnostic plots
    _model_marginal_dist = gumbel
    _plotting_dist_name = &#34;Gumbel&#34;
    _default_x0 = np.array([0.0])

    @validator(&#34;data&#34;, allow_reuse=True)
    def validate_data(cls, data):
        if data is not None and (len(data.shape) != 2 or data.shape[1] != 2 or np.any(np.isnan(data))):
            raise ValueError(&#34;Data is not an n x 2 numpy array&#34;)
        else:
            return data

    @classmethod
    @abstractmethod
    def unconditioned_cdf(cls, params: np.ndarray, data: np.ndarray):
        &#34;&#34;&#34;Computes the raw model cdf, without conditioning to the exceedance region.
        
        Args:
            params (np.ndarray): array with dependence parameter as only element
            data (t.Union[np.ndarray, t.Iterable]): Observed data in model scale
        &#34;&#34;&#34;
        pass

    @classmethod
    @abstractmethod
    def logpdf(
        cls, params: np.ndarray, threshold: float, data: t.Union[np.ndarray, t.Iterable]
    ):
        &#34;&#34;&#34;Calculates logpdf function for exceedance distribution
        
        
        Args:
            params (np.ndarray): array with model parameters
            threshold (float): Exceedance threshold in model scale
            data (t.Union[np.ndarray, t.Iterable]): Observed data in model scale
        &#34;&#34;&#34;
        pass

    @property
    def model_scale_threshold(self):
        return self._model_marginal_dist.ppf(self.quantile_threshold)

    @property
    def data_scale_threshold(self):
        return self.model_to_data_dist(
            self.bundle(self.model_scale_threshold, self.model_scale_threshold)
        )

    @property
    def mle_cov(self):
        return -np.linalg.inv(
            self.hessian(
                self.params, 
                self.model_scale_threshold, 
                self.data_to_model_dist(self.data)))
    
    @classmethod
    def unbundle(
        cls, data: t.Union[np.ndarray, t.Iterable]
    ) -&gt; t.Tuple[t.Union[np.ndarray, float], t.Union[np.ndarray, float]]:
        &#34;&#34;&#34;Unbundles matrix or iterables into separate components

        Args:
            data (t.Union[np.ndarray, t.Iterable]): dara

        &#34;&#34;&#34;
        if isinstance(data, np.ndarray) and len(data.shape) == 2 and data.shape[1] == 2:
            x = data[:, 0]
            y = data[:, 1]
        elif isinstance(data, Iterable):
            # if iterable, unroll
            x, y = data
        else:
            raise TypeError(
                &#34;data must be an n x 2 numpy array or an iterable of length 2.&#34;
            )
        return x, y

    @classmethod
    def bundle(
        cls, x: t.Union[np.ndarray, float, int], y: t.Union[np.ndarray, float, int]
    ) -&gt; t.Tuple[t.Union[np.ndarray, float], t.Union[np.ndarray, float]]:
        &#34;&#34;&#34;bundle a pair of arrays or primitives into n x 2 matrix

        Args:
            data (t.Union[np.ndarray, t.Iterable])

        &#34;&#34;&#34;
        if isinstance(x, np.ndarray) and isinstance(y, np.ndarray) and len(x) == len(y):
            z = np.concatenate([x.reshape((-1, 1)), y.reshape((-1, 1))], axis=1)
        elif issubclass(type(x), (float, int)) and issubclass(type(y), (float, int)):
            z = np.array([x, y]).reshape((1, 2))
        else:
            raise TypeError(
                &#34;x, y must be 1-dimensional arrays or inherit from float or int.&#34;
            )
        return z

    def data_to_model_dist(self, data: t.Union[np.ndarray, t.Iterable]) -&gt; np.ndarray:
        &#34;&#34;&#34;Transforms original data scale to standard Gumbel scale

        Args:
            data (t.Union[np.ndarray, t.Iterable]): observations in original scale

        &#34;&#34;&#34;
        x, y = self.unbundle(data)

        ## to copula scale
        x = self.margin1.cdf(x)
        y = self.margin2.cdf(y)

        # pass to Gumbel scale
        x = self._model_marginal_dist.ppf(x)
        y = self._model_marginal_dist.ppf(y)

        return self.bundle(x, y)

    def model_to_data_dist(self, data: t.Union[np.ndarray, t.Iterable]) -&gt; np.ndarray:
        &#34;&#34;&#34;Transforms data in standard Gumbel scale to original data scale

        Args:
            x (np.ndarray): data from first component
            y (np.ndarray): data from second component

        Returns:
            np.ndarray
        &#34;&#34;&#34;

        # copula scale
        x, y = self.unbundle(data)

        u = self._model_marginal_dist.cdf(x)
        w = self._model_marginal_dist.cdf(y)

        # data scale
        u = self.margin1.ppf(u)
        w = self.margin2.ppf(w)

        return self.bundle(u, w)

    def simulate(self, size: int):

        return self.model_to_data_dist(self.simulate_model(size, self.params, self.quantile_threshold))

    @classmethod
    def loglik(
        cls, params: np.ndarray, threshold: float, data: t.Union[np.ndarray, t.Iterable]
    ):
        &#34;&#34;&#34;Computes log-likelihood for threshold exceedances&#34;&#34;&#34;
        return np.sum(cls.logpdf(params, threshold, data))

    def cdf(self, x: np.ndarray):
        mapped_data = self.data_to_model_dist(x)
        model_threshold = self.model_scale_threshold
        u = np.minimum(mapped_data, model_threshold)
        norm_factor = float(
            1
            - self.unconditioned_cdf(
                self.params, self.bundle(model_threshold, model_threshold)
            )
        )

        return (
            self.unconditioned_cdf(self.params, mapped_data)
            - self.unconditioned_cdf(self.params, u)
        ) / norm_factor

    def pdf(self, x: np.ndarray, eps = 1e-5) -&gt; np.ndarray:
        &#34;&#34;&#34;Numerical approximation to the model&#39;s pdf in the original data scale. This is only non-zero when both marginal distributions are continuous on x
        
        Args:
            x (np.ndarray): Points to evaluate
            eps (float, optional): Numeric delta
        
        Returns:
            np.ndarray: pdf approximation
        &#34;&#34;&#34;
        x1, x2 = self.unbundle(x)
        model_scale_data = self.data_to_model_dist(x)
        n = len(model_scale_data)

        e1, e2 = (np.stack([np.ones((n,), dtype=np.float32), np.zeros((n,), dtype=np.float32)], axis=1),
            np.stack([np.zeros((n,), dtype=np.float32), np.ones((n,), dtype=np.float32)], axis=1))

        dz1_dx1 = (self.data_to_model_dist(x + eps*e1)[:,0] - self.data_to_model_dist(x - eps*e1)[:,0])/(2*eps)
        dz2_dx2 = (self.data_to_model_dist(x + eps*e2)[:,1] - self.data_to_model_dist(x - eps*e2)[:,1])/(2*eps)

        return (
            np.exp(self.logpdf(self.params, self.quantile_threshold, model_scale_data))
            * dz1_dx1 * dz2_dx2
        )

    @classmethod
    def fit(
        cls,
        data: t.Union[np.ndarray, t.Iterable],
        quantile_threshold: float,
        margin1: univar.BaseDistribution = None,
        margin2: univar.BaseDistribution = None,
        return_opt_results=False,
        x0: float = None,
    ) -&gt; Logistic:
        &#34;&#34;&#34;Fits the model from provided data, threshold and marginal distributons

        Args:
            data (t.Union[np.ndarray, t.Iterable]): input data
            quantile_threshold (float): Description: quantile threshold over which observations are classified as extreme
            margin1 (univar.BaseDistribution, optional): Marginal distribution for first component
            margin2 (univar.BaseDistribution, optional): Marginal distribution for second component
            return_opt_results (bool, optional): If True, the object from the optimization result is returned
            x0 (float, optional): Initial point for the optimisation algorithm. Defaults to 0.5

        Returns:
            Logistic: Fitted model


        &#34;&#34;&#34;
        if margin1 is None:
            margin1 = univar.empirical.from_data(data[:, 0])
            warnings.warn(
                &#34;margin1 is None; using an empirical distribution&#34;, stacklevel=2
            )

        if margin2 is None:
            margin1 = univar.empirical.from_data(data[:, 1])
            warnings.warn(
                &#34;margin1 is None; using an empirical distribution&#34;, stacklevel=2
            )

        if (
            not isinstance(quantile_threshold, float)
            or quantile_threshold &lt;= 0
            or quantile_threshold &gt;= 1
        ):
            raise ValueError(&#34;quantile_threshold must be in the open interval (0,1)&#34;)

        mapped_data = cls(
            margin1=margin1,
            margin2=margin2,
            quantile_threshold=quantile_threshold,
        ).data_to_model_dist(data)

        x, y = cls.unbundle(mapped_data)

        # get threshold exceedances
        model_scale_threshold = cls._model_marginal_dist.ppf(quantile_threshold)

        exs_idx = np.logical_or(x &gt; model_scale_threshold, y &gt; model_scale_threshold)
        x = x[exs_idx]
        y = y[exs_idx]

        x0 = cls._default_x0 if x0 is None else x0

        mapped_exceedances = cls.bundle(x, y)
        n = len(mapped_exceedances)

        def logistic(x):
            return 1.0 / (1 + np.exp(-x))

        def loss(x, data):
            params = logistic(x)
            # optimise normalised log-likelihood
            return -cls.loglik(params, model_scale_threshold, data)/n

        res = minimize(fun=loss, x0=x0, method=&#34;BFGS&#34;, args=(mapped_exceedances,))

        if return_opt_results:
            return res

        return cls(
            quantile_threshold=quantile_threshold,
            params=logistic(res.x),
            data=data[exs_idx, :],
            margin1=margin1,
            margin2=margin2,
        )

    @classmethod
    def hessian(
        cls, 
        params: np.ndarray, 
        threshold: float, 
        data: t.Union[np.ndarray, t.Iterable],
        eps = 1e-6) -&gt;np.ndarray:
        &#34;&#34;&#34;Numerical approximation for the loglikelihood function&#39;s Hessian
        
        Args:
            params (np.ndarray): parameter array
            threshold (float): Threshold in standard scale (i.e. Gumbel or Gaussian)
            data (t.Union[np.ndarray, t.Iterable]): Data in standard scale (i.e. Gumbel or Gaussian)
            eps (float, optional): Numerical delta in each component
        
        Returns:
            np.ndarray: Hessian matrix
        
        &#34;&#34;&#34;
        x0 = params
        grad0 = approx_fprime(x0, cls.loglik, eps, threshold, data) 
        n = len(x0)
        hessian = np.zeros((n,n), dtype=np.float32)
        # The next loop fill in the matrix
        xx = x0
        for j in range( n ):
            xx0 = xx[j] # Store old value
            xx[j] = xx0 + eps # Perturb with finite difference
            # Recalculate the partial derivatives for this new point
            current_grad = approx_fprime(xx, cls.loglik, eps, threshold, data) 
            hessian[:, j] = (current_grad - grad0)/eps # scale...
            xx[j] = xx0 # Restore initial value of x0        
        return hessian

    def plot_diagnostics(self, eps=1e-6):
        &#34;&#34;&#34;Returns a figure with the fitted exceedance model&#39;s profile log-likelihoods and fitted density in model scale.
        
        Args:
            eps (float, optional): numeric delta when calculating the Hessian matrix numerically; this is used to plot profile loglikelihoods.
        
        &#34;&#34;&#34;
        # unbundle data
        if self.data is None:
            raise ValueError(&#34;Diagnostics cannot be shown for models with no underlying data.&#34;)

        x, y = self.unbundle(self.data)
        z1, z2 = self.unbundle(self.data_to_model_dist(self.data))
        n = len(z1)
        params = self.params

        # create plot mosaic
        n_params = len(params)
        r, c = int(np.ceil(n_params/2 + 1/2)), 2
        # this function is needed to handle mosaics with a varying number of rows
        def get_subplot(k):
            if r == 1:
                return axs[k]
            else:
                return axs[k//2,k%2]
        fig, axs = plt.subplots(r,c)

        # compute mle sdev
        model_scale_threshold = self.model_scale_threshold
        try:
            hessian = self.hessian(params, model_scale_threshold, self.bundle(z1, z2), eps)
            sdevs = np.sqrt(-np.linalg.inv(hessian))
        except Exception as e:
            raise Exception(f&#34;Error when estimating fitted parameter variances; if fitted parameters are at the edge of their support, passing a lower eps value might help. Full trace: {traceback.format_exc()}&#34;)
        # create profile log-likelihood plots
        for k in range(n_params):
            param = params[k]
            sdev = sdevs[k,k]
            grid = np.linspace(param - 1.96*sdev, param + 1.96*sdev, 100)
            feasible_grid = grid[np.logical_and(grid &gt; 0, grid &lt; 1)]
            perturbed_params = np.copy(params)
            profile_ll = []
            for x in feasible_grid:
                perturbed_params[k] = x
                profile_ll.append(self.loglik(perturbed_params, model_scale_threshold, self.bundle(z1, z2)))
            # filter to almost optimal values
            profile_ll = np.array(profile_ll)
            max_ll = max(profile_ll)
            almost_optimal = np.abs(profile_ll - max_ll) &lt; np.abs(2 * max_ll)
            profile_ll = profile_ll[almost_optimal]
            feasible_grid = feasible_grid[almost_optimal]
            get_subplot(k).plot(feasible_grid, profile_ll, color=self._figure_color_palette[0])
            get_subplot(k).vlines(
                x=param,
                ymin=min(profile_ll),
                ymax=max(profile_ll),
                linestyle=&#34;dashed&#34;,
                colors=self._figure_color_palette[1],
            )
            get_subplot(k).title.set_text(&#34;Profile log-likelihood&#34;)
            get_subplot(k).set_xlabel(self._param_names.get(k, f&#34;params[{k}]&#34;))
            get_subplot(k).set_ylabel(&#34;&#34;)
            get_subplot(k).grid()

        ### last subplot contains the fitted density in model scale
        # avoid plotting in Frechet scale as it makes it difficult to see anything
        if isinstance(self._model_marginal_dist, Frechet):
            # convert Frechet data to Gumbel
            z1 = np.log(z1)
            z2 = np.log(z2)
            dist_name = &#34;Gumbel&#34;
            logpdf = Logistic.logpdf
            contour_dist_threshold = np.log(model_scale_threshold)
        else:
            dist_name = self._plotting_dist_name
            logpdf = self.logpdf
            contour_dist_threshold = model_scale_threshold

        x_range = np.linspace(np.min(z1), np.max(z1), 50)
        y_range = np.linspace(np.min(z2), np.max(z2), 50)

        X, Y = np.meshgrid(x_range, y_range)
        bundled_grid = self.bundle(X.reshape((-1, 1)), Y.reshape((-1, 1)))
        Z = logpdf(
            data=bundled_grid, threshold=contour_dist_threshold, params=params
        ).reshape(X.shape)
        get_subplot(-1).contourf(X, Y, Z)
        get_subplot(-1).scatter(z1, z2, color=self._figure_color_palette[1], s=0.9)
        get_subplot(-1).title.set_text(f&#34;Model density ({dist_name} scale)&#34;)
        get_subplot(-1).set_xlabel(&#34;x&#34;)
        get_subplot(-1).set_ylabel(&#34;y&#34;)

        plt.tight_layout()
        return fig</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="riskmodels.bivariate.BaseDistribution" href="#riskmodels.bivariate.BaseDistribution">BaseDistribution</a></li>
<li>pydantic.main.BaseModel</li>
<li>pydantic.utils.Representation</li>
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="riskmodels.bivariate.AsymmetricLogistic" href="#riskmodels.bivariate.AsymmetricLogistic">AsymmetricLogistic</a></li>
<li><a title="riskmodels.bivariate.Gaussian" href="#riskmodels.bivariate.Gaussian">Gaussian</a></li>
<li><a title="riskmodels.bivariate.Logistic" href="#riskmodels.bivariate.Logistic">Logistic</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="riskmodels.bivariate.ExceedanceDistribution.margin1"><code class="name">var <span class="ident">margin1</span> :Â Union[<a title="riskmodels.univariate.BaseDistribution" href="univariate.html#riskmodels.univariate.BaseDistribution">BaseDistribution</a>,Â scipy.stats._distn_infrastructure.rv_continuous]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="riskmodels.bivariate.ExceedanceDistribution.margin2"><code class="name">var <span class="ident">margin2</span> :Â Union[<a title="riskmodels.univariate.BaseDistribution" href="univariate.html#riskmodels.univariate.BaseDistribution">BaseDistribution</a>,Â scipy.stats._distn_infrastructure.rv_continuous]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="riskmodels.bivariate.ExceedanceDistribution.params"><code class="name">var <span class="ident">params</span> :Â Optional[numpy.ndarray]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="riskmodels.bivariate.ExceedanceDistribution.quantile_threshold"><code class="name">var <span class="ident">quantile_threshold</span> :Â float</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="riskmodels.bivariate.ExceedanceDistribution.bundle"><code class="name flex">
<span>def <span class="ident">bundle</span></span>(<span>x:Â t.Union[np.ndarray,Â float,Â int], y:Â t.Union[np.ndarray,Â float,Â int]) â€‘>Â Tuple[Union[numpy.ndarray,Â float],Â Union[numpy.ndarray,Â float]]</span>
</code></dt>
<dd>
<div class="desc"><p>bundle a pair of arrays or primitives into n x 2 matrix</p>
<h2 id="args">Args</h2>
<p>data (t.Union[np.ndarray, t.Iterable])</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def bundle(
    cls, x: t.Union[np.ndarray, float, int], y: t.Union[np.ndarray, float, int]
) -&gt; t.Tuple[t.Union[np.ndarray, float], t.Union[np.ndarray, float]]:
    &#34;&#34;&#34;bundle a pair of arrays or primitives into n x 2 matrix

    Args:
        data (t.Union[np.ndarray, t.Iterable])

    &#34;&#34;&#34;
    if isinstance(x, np.ndarray) and isinstance(y, np.ndarray) and len(x) == len(y):
        z = np.concatenate([x.reshape((-1, 1)), y.reshape((-1, 1))], axis=1)
    elif issubclass(type(x), (float, int)) and issubclass(type(y), (float, int)):
        z = np.array([x, y]).reshape((1, 2))
    else:
        raise TypeError(
            &#34;x, y must be 1-dimensional arrays or inherit from float or int.&#34;
        )
    return z</code></pre>
</details>
</dd>
<dt id="riskmodels.bivariate.ExceedanceDistribution.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>data:Â t.Union[np.ndarray,Â t.Iterable], quantile_threshold:Â float, margin1:Â univar.BaseDistributionÂ =Â None, margin2:Â univar.BaseDistributionÂ =Â None, return_opt_results=False, x0:Â floatÂ =Â None) â€‘>Â <a title="riskmodels.bivariate.Logistic" href="#riskmodels.bivariate.Logistic">Logistic</a></span>
</code></dt>
<dd>
<div class="desc"><p>Fits the model from provided data, threshold and marginal distributons</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>t.Union[np.ndarray, t.Iterable]</code></dt>
<dd>input data</dd>
<dt><strong><code>quantile_threshold</code></strong> :&ensp;<code>float</code></dt>
<dd>Description: quantile threshold over which observations are classified as extreme</dd>
<dt><strong><code>margin1</code></strong> :&ensp;<code>univar.BaseDistribution</code>, optional</dt>
<dd>Marginal distribution for first component</dd>
<dt><strong><code>margin2</code></strong> :&ensp;<code>univar.BaseDistribution</code>, optional</dt>
<dd>Marginal distribution for second component</dd>
<dt><strong><code>return_opt_results</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If True, the object from the optimization result is returned</dd>
<dt><strong><code>x0</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Initial point for the optimisation algorithm. Defaults to 0.5</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="riskmodels.bivariate.Logistic" href="#riskmodels.bivariate.Logistic">Logistic</a></code></dt>
<dd>Fitted model</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def fit(
    cls,
    data: t.Union[np.ndarray, t.Iterable],
    quantile_threshold: float,
    margin1: univar.BaseDistribution = None,
    margin2: univar.BaseDistribution = None,
    return_opt_results=False,
    x0: float = None,
) -&gt; Logistic:
    &#34;&#34;&#34;Fits the model from provided data, threshold and marginal distributons

    Args:
        data (t.Union[np.ndarray, t.Iterable]): input data
        quantile_threshold (float): Description: quantile threshold over which observations are classified as extreme
        margin1 (univar.BaseDistribution, optional): Marginal distribution for first component
        margin2 (univar.BaseDistribution, optional): Marginal distribution for second component
        return_opt_results (bool, optional): If True, the object from the optimization result is returned
        x0 (float, optional): Initial point for the optimisation algorithm. Defaults to 0.5

    Returns:
        Logistic: Fitted model


    &#34;&#34;&#34;
    if margin1 is None:
        margin1 = univar.empirical.from_data(data[:, 0])
        warnings.warn(
            &#34;margin1 is None; using an empirical distribution&#34;, stacklevel=2
        )

    if margin2 is None:
        margin1 = univar.empirical.from_data(data[:, 1])
        warnings.warn(
            &#34;margin1 is None; using an empirical distribution&#34;, stacklevel=2
        )

    if (
        not isinstance(quantile_threshold, float)
        or quantile_threshold &lt;= 0
        or quantile_threshold &gt;= 1
    ):
        raise ValueError(&#34;quantile_threshold must be in the open interval (0,1)&#34;)

    mapped_data = cls(
        margin1=margin1,
        margin2=margin2,
        quantile_threshold=quantile_threshold,
    ).data_to_model_dist(data)

    x, y = cls.unbundle(mapped_data)

    # get threshold exceedances
    model_scale_threshold = cls._model_marginal_dist.ppf(quantile_threshold)

    exs_idx = np.logical_or(x &gt; model_scale_threshold, y &gt; model_scale_threshold)
    x = x[exs_idx]
    y = y[exs_idx]

    x0 = cls._default_x0 if x0 is None else x0

    mapped_exceedances = cls.bundle(x, y)
    n = len(mapped_exceedances)

    def logistic(x):
        return 1.0 / (1 + np.exp(-x))

    def loss(x, data):
        params = logistic(x)
        # optimise normalised log-likelihood
        return -cls.loglik(params, model_scale_threshold, data)/n

    res = minimize(fun=loss, x0=x0, method=&#34;BFGS&#34;, args=(mapped_exceedances,))

    if return_opt_results:
        return res

    return cls(
        quantile_threshold=quantile_threshold,
        params=logistic(res.x),
        data=data[exs_idx, :],
        margin1=margin1,
        margin2=margin2,
    )</code></pre>
</details>
</dd>
<dt id="riskmodels.bivariate.ExceedanceDistribution.hessian"><code class="name flex">
<span>def <span class="ident">hessian</span></span>(<span>params:Â np.ndarray, threshold:Â float, data:Â t.Union[np.ndarray,Â t.Iterable], eps=1e-06) â€‘>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Numerical approximation for the loglikelihood function's Hessian</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>params</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>parameter array</dd>
<dt><strong><code>threshold</code></strong> :&ensp;<code>float</code></dt>
<dd>Threshold in standard scale (i.e. Gumbel or Gaussian)</dd>
<dt><strong><code>data</code></strong> :&ensp;<code>t.Union[np.ndarray, t.Iterable]</code></dt>
<dd>Data in standard scale (i.e. Gumbel or Gaussian)</dd>
<dt><strong><code>eps</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Numerical delta in each component</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>Hessian matrix</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def hessian(
    cls, 
    params: np.ndarray, 
    threshold: float, 
    data: t.Union[np.ndarray, t.Iterable],
    eps = 1e-6) -&gt;np.ndarray:
    &#34;&#34;&#34;Numerical approximation for the loglikelihood function&#39;s Hessian
    
    Args:
        params (np.ndarray): parameter array
        threshold (float): Threshold in standard scale (i.e. Gumbel or Gaussian)
        data (t.Union[np.ndarray, t.Iterable]): Data in standard scale (i.e. Gumbel or Gaussian)
        eps (float, optional): Numerical delta in each component
    
    Returns:
        np.ndarray: Hessian matrix
    
    &#34;&#34;&#34;
    x0 = params
    grad0 = approx_fprime(x0, cls.loglik, eps, threshold, data) 
    n = len(x0)
    hessian = np.zeros((n,n), dtype=np.float32)
    # The next loop fill in the matrix
    xx = x0
    for j in range( n ):
        xx0 = xx[j] # Store old value
        xx[j] = xx0 + eps # Perturb with finite difference
        # Recalculate the partial derivatives for this new point
        current_grad = approx_fprime(xx, cls.loglik, eps, threshold, data) 
        hessian[:, j] = (current_grad - grad0)/eps # scale...
        xx[j] = xx0 # Restore initial value of x0        
    return hessian</code></pre>
</details>
</dd>
<dt id="riskmodels.bivariate.ExceedanceDistribution.loglik"><code class="name flex">
<span>def <span class="ident">loglik</span></span>(<span>params:Â np.ndarray, threshold:Â float, data:Â t.Union[np.ndarray,Â t.Iterable])</span>
</code></dt>
<dd>
<div class="desc"><p>Computes log-likelihood for threshold exceedances</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def loglik(
    cls, params: np.ndarray, threshold: float, data: t.Union[np.ndarray, t.Iterable]
):
    &#34;&#34;&#34;Computes log-likelihood for threshold exceedances&#34;&#34;&#34;
    return np.sum(cls.logpdf(params, threshold, data))</code></pre>
</details>
</dd>
<dt id="riskmodels.bivariate.ExceedanceDistribution.logpdf"><code class="name flex">
<span>def <span class="ident">logpdf</span></span>(<span>params:Â np.ndarray, threshold:Â float, data:Â t.Union[np.ndarray,Â t.Iterable])</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates logpdf function for exceedance distribution</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>params</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>array with model parameters</dd>
<dt><strong><code>threshold</code></strong> :&ensp;<code>float</code></dt>
<dd>Exceedance threshold in model scale</dd>
<dt><strong><code>data</code></strong> :&ensp;<code>t.Union[np.ndarray, t.Iterable]</code></dt>
<dd>Observed data in model scale</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
@abstractmethod
def logpdf(
    cls, params: np.ndarray, threshold: float, data: t.Union[np.ndarray, t.Iterable]
):
    &#34;&#34;&#34;Calculates logpdf function for exceedance distribution
    
    
    Args:
        params (np.ndarray): array with model parameters
        threshold (float): Exceedance threshold in model scale
        data (t.Union[np.ndarray, t.Iterable]): Observed data in model scale
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="riskmodels.bivariate.ExceedanceDistribution.unbundle"><code class="name flex">
<span>def <span class="ident">unbundle</span></span>(<span>data:Â t.Union[np.ndarray,Â t.Iterable]) â€‘>Â Tuple[Union[numpy.ndarray,Â float],Â Union[numpy.ndarray,Â float]]</span>
</code></dt>
<dd>
<div class="desc"><p>Unbundles matrix or iterables into separate components</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>t.Union[np.ndarray, t.Iterable]</code></dt>
<dd>dara</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def unbundle(
    cls, data: t.Union[np.ndarray, t.Iterable]
) -&gt; t.Tuple[t.Union[np.ndarray, float], t.Union[np.ndarray, float]]:
    &#34;&#34;&#34;Unbundles matrix or iterables into separate components

    Args:
        data (t.Union[np.ndarray, t.Iterable]): dara

    &#34;&#34;&#34;
    if isinstance(data, np.ndarray) and len(data.shape) == 2 and data.shape[1] == 2:
        x = data[:, 0]
        y = data[:, 1]
    elif isinstance(data, Iterable):
        # if iterable, unroll
        x, y = data
    else:
        raise TypeError(
            &#34;data must be an n x 2 numpy array or an iterable of length 2.&#34;
        )
    return x, y</code></pre>
</details>
</dd>
<dt id="riskmodels.bivariate.ExceedanceDistribution.unconditioned_cdf"><code class="name flex">
<span>def <span class="ident">unconditioned_cdf</span></span>(<span>params:Â np.ndarray, data:Â np.ndarray)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the raw model cdf, without conditioning to the exceedance region.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>params</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>array with dependence parameter as only element</dd>
<dt><strong><code>data</code></strong> :&ensp;<code>t.Union[np.ndarray, t.Iterable]</code></dt>
<dd>Observed data in model scale</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
@abstractmethod
def unconditioned_cdf(cls, params: np.ndarray, data: np.ndarray):
    &#34;&#34;&#34;Computes the raw model cdf, without conditioning to the exceedance region.
    
    Args:
        params (np.ndarray): array with dependence parameter as only element
        data (t.Union[np.ndarray, t.Iterable]): Observed data in model scale
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="riskmodels.bivariate.ExceedanceDistribution.validate_data"><code class="name flex">
<span>def <span class="ident">validate_data</span></span>(<span>data)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@validator(&#34;data&#34;, allow_reuse=True)
def validate_data(cls, data):
    if data is not None and (len(data.shape) != 2 or data.shape[1] != 2 or np.any(np.isnan(data))):
        raise ValueError(&#34;Data is not an n x 2 numpy array&#34;)
    else:
        return data</code></pre>
</details>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="riskmodels.bivariate.ExceedanceDistribution.data_scale_threshold"><code class="name">var <span class="ident">data_scale_threshold</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def data_scale_threshold(self):
    return self.model_to_data_dist(
        self.bundle(self.model_scale_threshold, self.model_scale_threshold)
    )</code></pre>
</details>
</dd>
<dt id="riskmodels.bivariate.ExceedanceDistribution.mle_cov"><code class="name">var <span class="ident">mle_cov</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def mle_cov(self):
    return -np.linalg.inv(
        self.hessian(
            self.params, 
            self.model_scale_threshold, 
            self.data_to_model_dist(self.data)))</code></pre>
</details>
</dd>
<dt id="riskmodels.bivariate.ExceedanceDistribution.model_scale_threshold"><code class="name">var <span class="ident">model_scale_threshold</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def model_scale_threshold(self):
    return self._model_marginal_dist.ppf(self.quantile_threshold)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="riskmodels.bivariate.ExceedanceDistribution.data_to_model_dist"><code class="name flex">
<span>def <span class="ident">data_to_model_dist</span></span>(<span>self, data:Â t.Union[np.ndarray,Â t.Iterable]) â€‘>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Transforms original data scale to standard Gumbel scale</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>t.Union[np.ndarray, t.Iterable]</code></dt>
<dd>observations in original scale</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def data_to_model_dist(self, data: t.Union[np.ndarray, t.Iterable]) -&gt; np.ndarray:
    &#34;&#34;&#34;Transforms original data scale to standard Gumbel scale

    Args:
        data (t.Union[np.ndarray, t.Iterable]): observations in original scale

    &#34;&#34;&#34;
    x, y = self.unbundle(data)

    ## to copula scale
    x = self.margin1.cdf(x)
    y = self.margin2.cdf(y)

    # pass to Gumbel scale
    x = self._model_marginal_dist.ppf(x)
    y = self._model_marginal_dist.ppf(y)

    return self.bundle(x, y)</code></pre>
</details>
</dd>
<dt id="riskmodels.bivariate.ExceedanceDistribution.model_to_data_dist"><code class="name flex">
<span>def <span class="ident">model_to_data_dist</span></span>(<span>self, data:Â t.Union[np.ndarray,Â t.Iterable]) â€‘>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Transforms data in standard Gumbel scale to original data scale</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>data from first component</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>data from second component</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>np.ndarray</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def model_to_data_dist(self, data: t.Union[np.ndarray, t.Iterable]) -&gt; np.ndarray:
    &#34;&#34;&#34;Transforms data in standard Gumbel scale to original data scale

    Args:
        x (np.ndarray): data from first component
        y (np.ndarray): data from second component

    Returns:
        np.ndarray
    &#34;&#34;&#34;

    # copula scale
    x, y = self.unbundle(data)

    u = self._model_marginal_dist.cdf(x)
    w = self._model_marginal_dist.cdf(y)

    # data scale
    u = self.margin1.ppf(u)
    w = self.margin2.ppf(w)

    return self.bundle(u, w)</code></pre>
</details>
</dd>
<dt id="riskmodels.bivariate.ExceedanceDistribution.pdf"><code class="name flex">
<span>def <span class="ident">pdf</span></span>(<span>self, x:Â np.ndarray, eps=1e-05) â€‘>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Numerical approximation to the model's pdf in the original data scale. This is only non-zero when both marginal distributions are continuous on x</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Points to evaluate</dd>
<dt><strong><code>eps</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Numeric delta</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>pdf approximation</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pdf(self, x: np.ndarray, eps = 1e-5) -&gt; np.ndarray:
    &#34;&#34;&#34;Numerical approximation to the model&#39;s pdf in the original data scale. This is only non-zero when both marginal distributions are continuous on x
    
    Args:
        x (np.ndarray): Points to evaluate
        eps (float, optional): Numeric delta
    
    Returns:
        np.ndarray: pdf approximation
    &#34;&#34;&#34;
    x1, x2 = self.unbundle(x)
    model_scale_data = self.data_to_model_dist(x)
    n = len(model_scale_data)

    e1, e2 = (np.stack([np.ones((n,), dtype=np.float32), np.zeros((n,), dtype=np.float32)], axis=1),
        np.stack([np.zeros((n,), dtype=np.float32), np.ones((n,), dtype=np.float32)], axis=1))

    dz1_dx1 = (self.data_to_model_dist(x + eps*e1)[:,0] - self.data_to_model_dist(x - eps*e1)[:,0])/(2*eps)
    dz2_dx2 = (self.data_to_model_dist(x + eps*e2)[:,1] - self.data_to_model_dist(x - eps*e2)[:,1])/(2*eps)

    return (
        np.exp(self.logpdf(self.params, self.quantile_threshold, model_scale_data))
        * dz1_dx1 * dz2_dx2
    )</code></pre>
</details>
</dd>
<dt id="riskmodels.bivariate.ExceedanceDistribution.plot_diagnostics"><code class="name flex">
<span>def <span class="ident">plot_diagnostics</span></span>(<span>self, eps=1e-06)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a figure with the fitted exceedance model's profile log-likelihoods and fitted density in model scale.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>eps</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>numeric delta when calculating the Hessian matrix numerically; this is used to plot profile loglikelihoods.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_diagnostics(self, eps=1e-6):
    &#34;&#34;&#34;Returns a figure with the fitted exceedance model&#39;s profile log-likelihoods and fitted density in model scale.
    
    Args:
        eps (float, optional): numeric delta when calculating the Hessian matrix numerically; this is used to plot profile loglikelihoods.
    
    &#34;&#34;&#34;
    # unbundle data
    if self.data is None:
        raise ValueError(&#34;Diagnostics cannot be shown for models with no underlying data.&#34;)

    x, y = self.unbundle(self.data)
    z1, z2 = self.unbundle(self.data_to_model_dist(self.data))
    n = len(z1)
    params = self.params

    # create plot mosaic
    n_params = len(params)
    r, c = int(np.ceil(n_params/2 + 1/2)), 2
    # this function is needed to handle mosaics with a varying number of rows
    def get_subplot(k):
        if r == 1:
            return axs[k]
        else:
            return axs[k//2,k%2]
    fig, axs = plt.subplots(r,c)

    # compute mle sdev
    model_scale_threshold = self.model_scale_threshold
    try:
        hessian = self.hessian(params, model_scale_threshold, self.bundle(z1, z2), eps)
        sdevs = np.sqrt(-np.linalg.inv(hessian))
    except Exception as e:
        raise Exception(f&#34;Error when estimating fitted parameter variances; if fitted parameters are at the edge of their support, passing a lower eps value might help. Full trace: {traceback.format_exc()}&#34;)
    # create profile log-likelihood plots
    for k in range(n_params):
        param = params[k]
        sdev = sdevs[k,k]
        grid = np.linspace(param - 1.96*sdev, param + 1.96*sdev, 100)
        feasible_grid = grid[np.logical_and(grid &gt; 0, grid &lt; 1)]
        perturbed_params = np.copy(params)
        profile_ll = []
        for x in feasible_grid:
            perturbed_params[k] = x
            profile_ll.append(self.loglik(perturbed_params, model_scale_threshold, self.bundle(z1, z2)))
        # filter to almost optimal values
        profile_ll = np.array(profile_ll)
        max_ll = max(profile_ll)
        almost_optimal = np.abs(profile_ll - max_ll) &lt; np.abs(2 * max_ll)
        profile_ll = profile_ll[almost_optimal]
        feasible_grid = feasible_grid[almost_optimal]
        get_subplot(k).plot(feasible_grid, profile_ll, color=self._figure_color_palette[0])
        get_subplot(k).vlines(
            x=param,
            ymin=min(profile_ll),
            ymax=max(profile_ll),
            linestyle=&#34;dashed&#34;,
            colors=self._figure_color_palette[1],
        )
        get_subplot(k).title.set_text(&#34;Profile log-likelihood&#34;)
        get_subplot(k).set_xlabel(self._param_names.get(k, f&#34;params[{k}]&#34;))
        get_subplot(k).set_ylabel(&#34;&#34;)
        get_subplot(k).grid()

    ### last subplot contains the fitted density in model scale
    # avoid plotting in Frechet scale as it makes it difficult to see anything
    if isinstance(self._model_marginal_dist, Frechet):
        # convert Frechet data to Gumbel
        z1 = np.log(z1)
        z2 = np.log(z2)
        dist_name = &#34;Gumbel&#34;
        logpdf = Logistic.logpdf
        contour_dist_threshold = np.log(model_scale_threshold)
    else:
        dist_name = self._plotting_dist_name
        logpdf = self.logpdf
        contour_dist_threshold = model_scale_threshold

    x_range = np.linspace(np.min(z1), np.max(z1), 50)
    y_range = np.linspace(np.min(z2), np.max(z2), 50)

    X, Y = np.meshgrid(x_range, y_range)
    bundled_grid = self.bundle(X.reshape((-1, 1)), Y.reshape((-1, 1)))
    Z = logpdf(
        data=bundled_grid, threshold=contour_dist_threshold, params=params
    ).reshape(X.shape)
    get_subplot(-1).contourf(X, Y, Z)
    get_subplot(-1).scatter(z1, z2, color=self._figure_color_palette[1], s=0.9)
    get_subplot(-1).title.set_text(f&#34;Model density ({dist_name} scale)&#34;)
    get_subplot(-1).set_xlabel(&#34;x&#34;)
    get_subplot(-1).set_ylabel(&#34;y&#34;)

    plt.tight_layout()
    return fig</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="riskmodels.bivariate.BaseDistribution" href="#riskmodels.bivariate.BaseDistribution">BaseDistribution</a></b></code>:
<ul class="hlist">
<li><code><a title="riskmodels.bivariate.BaseDistribution.cdf" href="#riskmodels.bivariate.BaseDistribution.cdf">cdf</a></code></li>
<li><code><a title="riskmodels.bivariate.BaseDistribution.plot" href="#riskmodels.bivariate.BaseDistribution.plot">plot</a></code></li>
<li><code><a title="riskmodels.bivariate.BaseDistribution.simulate" href="#riskmodels.bivariate.BaseDistribution.simulate">simulate</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="riskmodels.bivariate.ExceedanceModel"><code class="flex name class">
<span>class <span class="ident">ExceedanceModel</span></span>
<span>(</span><span>**data:Â Any)</span>
</code></dt>
<dd>
<div class="desc"><p>Interface for exceedance models. This is a mixture of an empirical distribution with support below the exceedance threshold and an exceedance distribution (see <code><a title="riskmodels.bivariate.ExceedanceDistribution" href="#riskmodels.bivariate.ExceedanceDistribution">ExceedanceDistribution</a></code>) above it.</p>
<p>Create a new model by parsing and validating input data from keyword arguments.</p>
<p>Raises ValidationError if the input data cannot be parsed to form a valid model.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ExceedanceModel(Mixture):

    &#34;&#34;&#34;Interface for exceedance models. This is a mixture of an empirical distribution with support below the exceedance threshold and an exceedance distribution (see `ExceedanceDistribution`) above it.&#34;&#34;&#34;

    def __repr__(self):
        return f&#34;Sempirametric model with {self.tail.__class__.__name__} exceedance dependence&#34;

    def plot_diagnostics(self):

        return self.distributions[1].plot_diagnostics()

    @property
    def tail(self):
        return self.distributions[1]

    @property
    def empirical(self):
        return self.distributions[0]</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="riskmodels.bivariate.Mixture" href="#riskmodels.bivariate.Mixture">Mixture</a></li>
<li><a title="riskmodels.bivariate.BaseDistribution" href="#riskmodels.bivariate.BaseDistribution">BaseDistribution</a></li>
<li>pydantic.main.BaseModel</li>
<li>pydantic.utils.Representation</li>
<li>abc.ABC</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="riskmodels.bivariate.ExceedanceModel.distributions"><code class="name">var <span class="ident">distributions</span> :Â List[<a title="riskmodels.bivariate.BaseDistribution" href="#riskmodels.bivariate.BaseDistribution">BaseDistribution</a>]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="riskmodels.bivariate.ExceedanceModel.weights"><code class="name">var <span class="ident">weights</span> :Â numpy.ndarray</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="riskmodels.bivariate.ExceedanceModel.empirical"><code class="name">var <span class="ident">empirical</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def empirical(self):
    return self.distributions[0]</code></pre>
</details>
</dd>
<dt id="riskmodels.bivariate.ExceedanceModel.tail"><code class="name">var <span class="ident">tail</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def tail(self):
    return self.distributions[1]</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="riskmodels.bivariate.ExceedanceModel.plot_diagnostics"><code class="name flex">
<span>def <span class="ident">plot_diagnostics</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_diagnostics(self):

    return self.distributions[1].plot_diagnostics()</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="riskmodels.bivariate.Mixture" href="#riskmodels.bivariate.Mixture">Mixture</a></b></code>:
<ul class="hlist">
<li><code><a title="riskmodels.bivariate.Mixture.cdf" href="#riskmodels.bivariate.BaseDistribution.cdf">cdf</a></code></li>
<li><code><a title="riskmodels.bivariate.Mixture.pdf" href="#riskmodels.bivariate.BaseDistribution.pdf">pdf</a></code></li>
<li><code><a title="riskmodels.bivariate.Mixture.plot" href="#riskmodels.bivariate.BaseDistribution.plot">plot</a></code></li>
<li><code><a title="riskmodels.bivariate.Mixture.simulate" href="#riskmodels.bivariate.BaseDistribution.simulate">simulate</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="riskmodels.bivariate.Frechet"><code class="flex name class">
<span>class <span class="ident">Frechet</span></span>
</code></dt>
<dd>
<div class="desc"><p>Minimal implementation of a unit Frechet distribution</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Frechet(object):
    &#34;&#34;&#34;Minimal implementation of a unit Frechet distribution&#34;&#34;&#34;
    @classmethod
    def cdf(cls, x: np.ndarray):
        return np.exp(-1/x)

    @classmethod
    def ppf(cls, p: np.ndarray):
        return -1.0/np.log(p)</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="riskmodels.bivariate.Frechet.cdf"><code class="name flex">
<span>def <span class="ident">cdf</span></span>(<span>x:Â np.ndarray)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def cdf(cls, x: np.ndarray):
    return np.exp(-1/x)</code></pre>
</details>
</dd>
<dt id="riskmodels.bivariate.Frechet.ppf"><code class="name flex">
<span>def <span class="ident">ppf</span></span>(<span>p:Â np.ndarray)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def ppf(cls, p: np.ndarray):
    return -1.0/np.log(p)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="riskmodels.bivariate.Gaussian"><code class="flex name class">
<span>class <span class="ident">Gaussian</span></span>
<span>(</span><span>**data:Â Any)</span>
</code></dt>
<dd>
<div class="desc"><p>This model assumes association between exceedances at different components follow a Gaussian copula. Exceedances in each component are defined as observations above a fixed quantile threshold <span><span class="MathJax_Preview"> \textbf{q}</span><script type="math/tex"> \textbf{q}</script></span> for a high probability level <span><span class="MathJax_Preview">p \approx 1</span><script type="math/tex">p \approx 1</script></span>, and so bivariate exceedances <span><span class="MathJax_Preview">\textbf{Z}</span><script type="math/tex">\textbf{Z}</script></span> are defined in an inverted-L-shaped region of space, <span><span class="MathJax_Preview"> \textbf{Z} \nleq \mathbf{q} </span><script type="math/tex"> \textbf{Z} \nleq \mathbf{q} </script></span>: that in which there is an exceedance in at least one component. Consequently this copula model is only defined in the corresponding inverted-L-shaped region in <span><span class="MathJax_Preview"> [\textbf{0}, \textbf{1}]</span><script type="math/tex"> [\textbf{0}, \textbf{1}]</script></span>; the functional form is the same as a Gaussian copula, but the normalisation constant is different.</p>
<p>If the underlying marginal distributions follow a generalised Pareto above the quantile thresholds, this model can be seen as a pre-limit version of a bivariate generalised Pareto distribution (see Rootzen and Tajvidi, 2006) with a Gaussian dependence model. Because Gaussian copulas are asymptotically independent (this is, dependence
weakens at progressively more extreme levels regardless of the correlation parameter, and disappears in the limit), said limiting model is degenerate, with probability mass at <span><span class="MathJax_Preview">-\infty</span><script type="math/tex">-\infty</script></span>. This pre-limit model on the other hand is non-degenerate and can be used to model asymptotically independent data.</p>
<p>Create a new model by parsing and validating input data from keyword arguments.</p>
<p>Raises ValidationError if the input data cannot be parsed to form a valid model.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Gaussian(ExceedanceDistribution):

    &#34;&#34;&#34;This model assumes association between exceedances at different components follow a Gaussian copula. Exceedances in each component are defined as observations above a fixed quantile threshold \\( \\textbf{q}\\) for a high probability level \\(p \\approx 1\\), and so bivariate exceedances \\(\\textbf{Z}\\) are defined in an inverted-L-shaped region of space, \\( \\textbf{Z} \\nleq \\mathbf{q} \\): that in which there is an exceedance in at least one component. Consequently this copula model is only defined in the corresponding inverted-L-shaped region in \\( [\\textbf{0}, \\textbf{1}]\\); the functional form is the same as a Gaussian copula, but the normalisation constant is different.

    If the underlying marginal distributions follow a generalised Pareto above the quantile thresholds, this model can be seen as a pre-limit version of a bivariate generalised Pareto distribution (see Rootzen and Tajvidi, 2006) with a Gaussian dependence model. Because Gaussian copulas are asymptotically independent (this is, dependence  weakens at progressively more extreme levels regardless of the correlation parameter, and disappears in the limit), said limiting model is degenerate, with probability mass at \\(-\\infty\\). This pre-limit model on the other hand is non-degenerate and can be used to model asymptotically independent data.
    &#34;&#34;&#34;

    _model_marginal_dist = gaussian
    #_plotting_dist = gaussian
    _plotting_dist_name = &#34;Gaussian&#34;
    _param_names = {0:&#34;rho&#34;} #mapping from params array indices to names for diagnostic plots

    def __repr__(self):
        return f&#34;{self.__class__.__name__} exceedance dependence model with rho = {self.params} and quantile threshold {self.quantile_threshold}&#34;

    @validator(&#34;params&#34;)
    def validate_params(cls, params):
        rho = params[0]
        if rho &lt; 0 or rho &gt;= 1:
            raise TypeError(f&#34;rho must be in the interval [0,1)&#34;)
        else:
            return params

    @property
    def cov(self):
        rho = self.params[0]
        return np.array([[1, rho], [rho, 1]])

    @property
    def rho(self):
        return self.params[0]
    
    @classmethod
    def unconditioned_cdf(cls, params: np.ndarray, data: np.ndarray):
        rho = params[0]
        &#34;&#34;&#34;Calculates unconstrained standard Gaussian CDF&#34;&#34;&#34;
        return mv_gaussian.cdf(data, cov=np.array([[1, rho], [rho, 1]]))

    @classmethod
    def logpdf(
        cls, params: np.ndarray, threshold: float, data: t.Union[np.ndarray, t.Iterable]
    ):
        &#34;&#34;&#34;Calculates logpdf for Gaussian exceedances&#34;&#34;&#34;
        rho = params[0]
        x, y = cls.unbundle(data)
        if isinstance(rho, (list, np.ndarray)):
            rho = rho[0]
        norm_factor = 1 - mv_gaussian.cdf(
            cls.bundle(threshold, threshold), cov=np.array([[1, rho], [rho, 1]])
        )
        density = mv_gaussian.logpdf(
            data, cov=np.array([[1, rho], [rho, 1]])
        ) - np.log(norm_factor)

        # density is 0 when both coordinates are below the threshold
        nil_density_idx = np.logical_and(x &lt;= threshold, y &lt;= threshold)
        density[nil_density_idx] = -np.Inf

        return density

    @classmethod
    def simulate_model(cls, size: int, params: np.ndarray, quantile_threshold: float) -&gt; np.ndarray:
        &#34;&#34;&#34;Simulate Gaussian exceedance model in Gaussian scale
        
        Args:
            size (int): Simulated sample size
            params (np.ndarray): Array with dependence parameter
            threshold (float): Exceedance threshold in both components
        
        Returns:
            np.ndarray: Simulated sample
        &#34;&#34;&#34;

        # exceedance subregions:
        # r1 =&gt; exceedance in second component only, r2 =&gt; exceedance in both components, r3 =&gt; exceedance in first component only
        rho = params[0]
        cov = np.array([[1, rho], [rho, 1]])

        if quantile_threshold == 0:
            samples = mv_gaussian.rvs(size=size, cov=cov)
        else:
            threshold = gaussian.ppf(quantile_threshold)

            th = cls.bundle(threshold, threshold)
            th_cdf = mv_gaussian.cdf(th, cov=cov) 

            p1 = quantile_threshold - th_cdf
            p2 = 1 - 2 * quantile_threshold + th_cdf
            p3 = 1 - th_cdf - (p1 + p2)

            p = np.array([p1, p2, p3])
            p = p / np.sum(p)

            # compute number of samples per subregion
            n1, n2, n3 = np.random.multinomial(n=size, pvals=p, size=1)[0].astype(np.int32)
            n1, n2, n3 = int(n1), int(n2), int(n3)

            r1_samples = (
                tmvn(
                    mu=np.zeros((2,)),
                    cov=cov,
                    lb=np.array([-np.Inf, threshold]),
                    ub=np.array([threshold, np.Inf]),
                )
                .sample(n1)
                .T
            )

            r2_samples = (
                tmvn(
                    mu=np.zeros((2,)),
                    cov=cov,
                    lb=np.array([threshold, threshold]),
                    ub=np.array([np.Inf, np.Inf]),
                )
                .sample(n2)
                .T
            )

            r3_samples = (
                tmvn(
                    mu=np.zeros((2,)),
                    cov=cov,
                    lb=np.array([threshold, -np.Inf]),
                    ub=np.array([np.Inf, threshold]),
                )
                .sample(n3)
                .T
            )

            samples = np.concatenate([r1_samples, r2_samples, r3_samples], axis=0)

        return samples</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="riskmodels.bivariate.ExceedanceDistribution" href="#riskmodels.bivariate.ExceedanceDistribution">ExceedanceDistribution</a></li>
<li><a title="riskmodels.bivariate.BaseDistribution" href="#riskmodels.bivariate.BaseDistribution">BaseDistribution</a></li>
<li>pydantic.main.BaseModel</li>
<li>pydantic.utils.Representation</li>
<li>abc.ABC</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="riskmodels.bivariate.Gaussian.margin1"><code class="name">var <span class="ident">margin1</span> :Â Union[<a title="riskmodels.univariate.BaseDistribution" href="univariate.html#riskmodels.univariate.BaseDistribution">BaseDistribution</a>,Â scipy.stats._distn_infrastructure.rv_continuous]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="riskmodels.bivariate.Gaussian.margin2"><code class="name">var <span class="ident">margin2</span> :Â Union[<a title="riskmodels.univariate.BaseDistribution" href="univariate.html#riskmodels.univariate.BaseDistribution">BaseDistribution</a>,Â scipy.stats._distn_infrastructure.rv_continuous]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="riskmodels.bivariate.Gaussian.params"><code class="name">var <span class="ident">params</span> :Â Optional[numpy.ndarray]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="riskmodels.bivariate.Gaussian.quantile_threshold"><code class="name">var <span class="ident">quantile_threshold</span> :Â float</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="riskmodels.bivariate.Gaussian.logpdf"><code class="name flex">
<span>def <span class="ident">logpdf</span></span>(<span>params:Â np.ndarray, threshold:Â float, data:Â t.Union[np.ndarray,Â t.Iterable])</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates logpdf for Gaussian exceedances</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def logpdf(
    cls, params: np.ndarray, threshold: float, data: t.Union[np.ndarray, t.Iterable]
):
    &#34;&#34;&#34;Calculates logpdf for Gaussian exceedances&#34;&#34;&#34;
    rho = params[0]
    x, y = cls.unbundle(data)
    if isinstance(rho, (list, np.ndarray)):
        rho = rho[0]
    norm_factor = 1 - mv_gaussian.cdf(
        cls.bundle(threshold, threshold), cov=np.array([[1, rho], [rho, 1]])
    )
    density = mv_gaussian.logpdf(
        data, cov=np.array([[1, rho], [rho, 1]])
    ) - np.log(norm_factor)

    # density is 0 when both coordinates are below the threshold
    nil_density_idx = np.logical_and(x &lt;= threshold, y &lt;= threshold)
    density[nil_density_idx] = -np.Inf

    return density</code></pre>
</details>
</dd>
<dt id="riskmodels.bivariate.Gaussian.simulate_model"><code class="name flex">
<span>def <span class="ident">simulate_model</span></span>(<span>size:Â int, params:Â np.ndarray, quantile_threshold:Â float) â€‘>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Simulate Gaussian exceedance model in Gaussian scale</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code></dt>
<dd>Simulated sample size</dd>
<dt><strong><code>params</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Array with dependence parameter</dd>
<dt><strong><code>threshold</code></strong> :&ensp;<code>float</code></dt>
<dd>Exceedance threshold in both components</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>Simulated sample</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def simulate_model(cls, size: int, params: np.ndarray, quantile_threshold: float) -&gt; np.ndarray:
    &#34;&#34;&#34;Simulate Gaussian exceedance model in Gaussian scale
    
    Args:
        size (int): Simulated sample size
        params (np.ndarray): Array with dependence parameter
        threshold (float): Exceedance threshold in both components
    
    Returns:
        np.ndarray: Simulated sample
    &#34;&#34;&#34;

    # exceedance subregions:
    # r1 =&gt; exceedance in second component only, r2 =&gt; exceedance in both components, r3 =&gt; exceedance in first component only
    rho = params[0]
    cov = np.array([[1, rho], [rho, 1]])

    if quantile_threshold == 0:
        samples = mv_gaussian.rvs(size=size, cov=cov)
    else:
        threshold = gaussian.ppf(quantile_threshold)

        th = cls.bundle(threshold, threshold)
        th_cdf = mv_gaussian.cdf(th, cov=cov) 

        p1 = quantile_threshold - th_cdf
        p2 = 1 - 2 * quantile_threshold + th_cdf
        p3 = 1 - th_cdf - (p1 + p2)

        p = np.array([p1, p2, p3])
        p = p / np.sum(p)

        # compute number of samples per subregion
        n1, n2, n3 = np.random.multinomial(n=size, pvals=p, size=1)[0].astype(np.int32)
        n1, n2, n3 = int(n1), int(n2), int(n3)

        r1_samples = (
            tmvn(
                mu=np.zeros((2,)),
                cov=cov,
                lb=np.array([-np.Inf, threshold]),
                ub=np.array([threshold, np.Inf]),
            )
            .sample(n1)
            .T
        )

        r2_samples = (
            tmvn(
                mu=np.zeros((2,)),
                cov=cov,
                lb=np.array([threshold, threshold]),
                ub=np.array([np.Inf, np.Inf]),
            )
            .sample(n2)
            .T
        )

        r3_samples = (
            tmvn(
                mu=np.zeros((2,)),
                cov=cov,
                lb=np.array([threshold, -np.Inf]),
                ub=np.array([np.Inf, threshold]),
            )
            .sample(n3)
            .T
        )

        samples = np.concatenate([r1_samples, r2_samples, r3_samples], axis=0)

    return samples</code></pre>
</details>
</dd>
<dt id="riskmodels.bivariate.Gaussian.validate_params"><code class="name flex">
<span>def <span class="ident">validate_params</span></span>(<span>params)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@validator(&#34;params&#34;)
def validate_params(cls, params):
    rho = params[0]
    if rho &lt; 0 or rho &gt;= 1:
        raise TypeError(f&#34;rho must be in the interval [0,1)&#34;)
    else:
        return params</code></pre>
</details>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="riskmodels.bivariate.Gaussian.cov"><code class="name">var <span class="ident">cov</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def cov(self):
    rho = self.params[0]
    return np.array([[1, rho], [rho, 1]])</code></pre>
</details>
</dd>
<dt id="riskmodels.bivariate.Gaussian.rho"><code class="name">var <span class="ident">rho</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def rho(self):
    return self.params[0]</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="riskmodels.bivariate.ExceedanceDistribution" href="#riskmodels.bivariate.ExceedanceDistribution">ExceedanceDistribution</a></b></code>:
<ul class="hlist">
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.bundle" href="#riskmodels.bivariate.ExceedanceDistribution.bundle">bundle</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.cdf" href="#riskmodels.bivariate.BaseDistribution.cdf">cdf</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.data_to_model_dist" href="#riskmodels.bivariate.ExceedanceDistribution.data_to_model_dist">data_to_model_dist</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.fit" href="#riskmodels.bivariate.ExceedanceDistribution.fit">fit</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.hessian" href="#riskmodels.bivariate.ExceedanceDistribution.hessian">hessian</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.loglik" href="#riskmodels.bivariate.ExceedanceDistribution.loglik">loglik</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.model_to_data_dist" href="#riskmodels.bivariate.ExceedanceDistribution.model_to_data_dist">model_to_data_dist</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.pdf" href="#riskmodels.bivariate.ExceedanceDistribution.pdf">pdf</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.plot" href="#riskmodels.bivariate.BaseDistribution.plot">plot</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.plot_diagnostics" href="#riskmodels.bivariate.ExceedanceDistribution.plot_diagnostics">plot_diagnostics</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.simulate" href="#riskmodels.bivariate.BaseDistribution.simulate">simulate</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.unbundle" href="#riskmodels.bivariate.ExceedanceDistribution.unbundle">unbundle</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.unconditioned_cdf" href="#riskmodels.bivariate.ExceedanceDistribution.unconditioned_cdf">unconditioned_cdf</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="riskmodels.bivariate.Independent"><code class="flex name class">
<span>class <span class="ident">Independent</span></span>
<span>(</span><span>**data:Â Any)</span>
</code></dt>
<dd>
<div class="desc"><p>Bivariate distribution with independent components</p>
<p>Create a new model by parsing and validating input data from keyword arguments.</p>
<p>Raises ValidationError if the input data cannot be parsed to form a valid model.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Independent(BaseDistribution):

    &#34;&#34;&#34;Bivariate distribution with independent components&#34;&#34;&#34;

    x: univar.BaseDistribution
    y: univar.BaseDistribution

    def __repr__(self):
        return f&#34;Independent bivariate distribution with marginals:\nx:{self.x.__repr__()}\ny:{self.y.__repr__()}&#34;

    def pdf(self, x: np.ndarray):
        x1, x2 = x
        return self.x.pdf(x1) * self.y.pdf(x2)

    def cdf(self, x: np.ndarray):
        x1, x2 = x
        return self.x.cdf(x1) * self.y.cdf(x2)

    def simulate(self, size: int):
        return np.concatenate(
            [
                self.x.simulate(size).reshape((-1, 1)),
                self.y.simulate(size).reshape((-1, 1)),
            ],
            axis=1,
        )</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="riskmodels.bivariate.BaseDistribution" href="#riskmodels.bivariate.BaseDistribution">BaseDistribution</a></li>
<li>pydantic.main.BaseModel</li>
<li>pydantic.utils.Representation</li>
<li>abc.ABC</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="riskmodels.bivariate.Independent.x"><code class="name">var <span class="ident">x</span> :Â <a title="riskmodels.univariate.BaseDistribution" href="univariate.html#riskmodels.univariate.BaseDistribution">BaseDistribution</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="riskmodels.bivariate.Independent.y"><code class="name">var <span class="ident">y</span> :Â <a title="riskmodels.univariate.BaseDistribution" href="univariate.html#riskmodels.univariate.BaseDistribution">BaseDistribution</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="riskmodels.bivariate.BaseDistribution" href="#riskmodels.bivariate.BaseDistribution">BaseDistribution</a></b></code>:
<ul class="hlist">
<li><code><a title="riskmodels.bivariate.BaseDistribution.cdf" href="#riskmodels.bivariate.BaseDistribution.cdf">cdf</a></code></li>
<li><code><a title="riskmodels.bivariate.BaseDistribution.pdf" href="#riskmodels.bivariate.BaseDistribution.pdf">pdf</a></code></li>
<li><code><a title="riskmodels.bivariate.BaseDistribution.plot" href="#riskmodels.bivariate.BaseDistribution.plot">plot</a></code></li>
<li><code><a title="riskmodels.bivariate.BaseDistribution.simulate" href="#riskmodels.bivariate.BaseDistribution.simulate">simulate</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="riskmodels.bivariate.Logistic"><code class="flex name class">
<span>class <span class="ident">Logistic</span></span>
<span>(</span><span>**data:Â Any)</span>
</code></dt>
<dd>
<div class="desc"><p>This model assumes association between exceedances at different components follow a Gumbel-Hougaard copula; in Gumbel scale, this is given by</p>
<p><span><span class="MathJax_Preview"> \mathbb{P}(\textbf{X} \leq \mathbf{x}) = \exp \left(- \left(
\exp \left( - \frac{\mathbf{x}_1}{\alpha} \right)+ \left(
- \frac{\mathbf{x}_2}{\alpha}
\right) \right)^\alpha \right), \, 0 \leq \alpha\leq 1, \, \mathbf{X} \in \mathbb{R}^2</span><script type="math/tex; mode=display"> \mathbb{P}(\textbf{X} \leq \mathbf{x}) = \exp \left(- \left(
\exp \left( - \frac{\mathbf{x}_1}{\alpha} \right)+ \left(
- \frac{\mathbf{x}_2}{\alpha}
\right) \right)^\alpha \right), \, 0 \leq \alpha\leq 1, \, \mathbf{X} \in \mathbb{R}^2</script></span></p>
<p>Exceedances in each component are defined as observations above a fixed quantile threshold <span><span class="MathJax_Preview"> \textbf{q}</span><script type="math/tex"> \textbf{q}</script></span> for a high probability level <span><span class="MathJax_Preview">p \approx 1</span><script type="math/tex">p \approx 1</script></span>, and so bivariate exceedances <span><span class="MathJax_Preview">\textbf{Z}</span><script type="math/tex">\textbf{Z}</script></span> are defined in an inverted-L-shaped region of space, <span><span class="MathJax_Preview"> \textbf{Z} \nleq \mathbf{q} </span><script type="math/tex"> \textbf{Z} \nleq \mathbf{q} </script></span>: that in which there is an exceedance in at least one component. Consequently the model implemented here is only defined in the corresponding inverted-L-shaped region; the functional form of the dependence is the same as a Gumbel-Hougaard copula, but the normalisation constant is different because of this constraint.</p>
<p>If the underlying marginal distributions follow a generalised Pareto above the quantile thresholds, this model can be seen as a pre-limit version of a bivariate generalised Pareto distribution with a logistic dependence model (see Rootzen and Tajvidi, 2006), to which this model converges as the quantile thresholds grow.</p>
<p>Create a new model by parsing and validating input data from keyword arguments.</p>
<p>Raises ValidationError if the input data cannot be parsed to form a valid model.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Logistic(ExceedanceDistribution):

    &#34;&#34;&#34;This model assumes association between exceedances at different components follow a Gumbel-Hougaard copula; in Gumbel scale, this is given by
    
    $$ \\mathbb{P}(\\textbf{X} \\leq \\mathbf{x}) = \\exp \\left(- \\left(  \\exp \\left( - \\frac{\\mathbf{x}_1}{\\alpha} \\right)+ \\left(  - \\frac{\\mathbf{x}_2}{\\alpha}  \\right) \\right)^\\alpha \\right), \\, 0 \\leq \\alpha\\leq 1, \\, \\mathbf{X} \\in \\mathbb{R}^2$$

    Exceedances in each component are defined as observations above a fixed quantile threshold \\( \\textbf{q}\\) for a high probability level \\(p \\approx 1\\), and so bivariate exceedances \\(\\textbf{Z}\\) are defined in an inverted-L-shaped region of space, \\( \\textbf{Z} \\nleq \\mathbf{q} \\): that in which there is an exceedance in at least one component. Consequently the model implemented here is only defined in the corresponding inverted-L-shaped region; the functional form of the dependence is the same as a Gumbel-Hougaard copula, but the normalisation constant is different because of this constraint.

    If the underlying marginal distributions follow a generalised Pareto above the quantile thresholds, this model can be seen as a pre-limit version of a bivariate generalised Pareto distribution with a logistic dependence model (see Rootzen and Tajvidi, 2006), to which this model converges as the quantile thresholds grow.
    &#34;&#34;&#34;
    params: t.Optional[np.ndarray] = Field(default_factory=lambda: np.zeros((1,), dtype=np.float32))

    _param_names = {0:&#34;alpha&#34;} #mapping from params array indices to names for diagnostic plots

    _model_marginal_dist = gumbel
    #_plotting_dist = gumbel
    _plotting_dist_name = &#34;Gumbel&#34;
    _default_x0 = np.array([0.0])

    def __repr__(self):
        return f&#34;{self.__class__.__name__} exceedance dependence model with alpha = {self.alpha} and quantile threshold {self.quantile_threshold}&#34;

    @validator(&#34;params&#34;)
    def validate_params(cls, params):
        alpha = params[0]
        if alpha &lt;= 0 or alpha &gt; 1:
            raise TypeError(f&#34;alpha must be in the interval (0,1]&#34;)
        else:
            return params

    @property
    def alpha(self):
        return self.params[0]


    @classmethod
    def logpdf(
        cls, params: np.ndarray, threshold: float, data: t.Union[np.ndarray, t.Iterable]
    ):
        &#34;&#34;&#34;Calculates logpdf function for Gumbel exceedances
        
        
        Args:
            params (np.ndarray): array with dependence parameter as only element
            threshold (float): Exceedance threshold in Gumbel scale
            data (t.Union[np.ndarray, t.Iterable]): Observed data in Gumbel scale
        
        &#34;&#34;&#34;
        alpha = params[0]

        x, y = cls.unbundle(data)

        nlogp = (np.exp(-x / alpha) + np.exp(-y / alpha)) ** alpha
        lognlogp = alpha * np.log(np.exp(-x / alpha) + np.exp(-y / alpha))
        rescaler = 1 - cls.unconditioned_cdf(params, cls.bundle(threshold, threshold))

        # a = np.exp((x + y - nlogp*alpha)/alpha)
        log_a = (x + y) / alpha - nlogp

        # b = nlogp
        log_b = lognlogp

        # c = 1 + alpha*(nlogp - 1)
        log_c = np.log(1 + alpha * (nlogp - 1))

        # d = 1.0/(alpha*(np.exp(x/alpha) + np.exp(y/alpha))**2)
        log_d = -(np.log(alpha) + 2 * np.log(np.exp(x / alpha) + np.exp(y / alpha)))

        log_density = log_a + log_b + log_c + log_d - np.log(rescaler)

        # density is 0 when both coordinates are below the threshold
        nil_density_idx = np.logical_and(x &lt;= threshold, y &lt;= threshold)
        log_density[nil_density_idx] = -np.Inf

        return log_density

    @classmethod
    def unconditioned_cdf(cls, params: np.ndarray, data: t.Union[np.ndarray, t.Iterable]):
        &#34;&#34;&#34;Calculates unconstrained standard Gumbel CDF&#34;&#34;&#34;
        alpha = params[0]
        x, y = cls.unbundle(data)
        return np.exp(-((np.exp(-x / alpha) + np.exp(-y / alpha)) ** (alpha)))           

    @classmethod
    def simulate_model(cls, size: int, params: np.ndarray, quantile_threshold: float) -&gt; np.ndarray:
        &#34;&#34;&#34;Simulates logistic exceedance model in Gumbel scale
        
        Args:
            size (int): Simulated sample size
            params (np.ndarray): Array with dependence parameter
            threshold (float): Exceedance threshold in both components
        
        Returns:
            np.ndarray: Simulated sample
        &#34;&#34;&#34;
        ### simulate in Gumbel scale maximum component: z = max(x1, x2) ~ Gumbel(loc=alpha*np.log(2)) using inverse function method
        threshold = gumbel.ppf(quantile_threshold)
        alpha = params[0]

        q0 = gumbel.cdf(threshold, loc=alpha * np.log(2))  # quantile of model&#39;s threshold in the maximum&#39;s distribution
        u = np.random.uniform(size=size, low=q0)
        maxima = gumbel.ppf(q=u, loc=alpha * np.log(2))

        if alpha == 0:
            r = 0
        elif alpha == 1:
            r = np.log(exponential.rvs(size=size, loc=1, scale = np.exp(maxima)))
        else:
            ###simulate difference between maxima and minima r = max(x,y) - min(x,y) using inverse function method
            u = np.random.uniform(size=size)
            r = (
                alpha
                * np.log(
                    (
                        -(
                            (alpha - 1)
                            * np.exp(maxima)
                            * lambertw(
                                -(
                                    np.exp(
                                        -maxima
                                        - (2 ** alpha * np.exp(-maxima) * alpha)
                                        / (alpha - 1)
                                    )
                                    * (-(2 ** (alpha - 1)) * (u - 1))
                                    ** (alpha / (alpha - 1))
                                    * alpha
                                )
                                / (alpha - 1)
                            )
                        )
                        / alpha
                    )
                    ** (1 / alpha)
                    - 1
                )
            ).real

        minima = maxima - r

        # allocate maxima randomly between components
        max_indices = np.random.binomial(1, 0.5, size)

        x = np.concatenate(
            [
                maxima[max_indices == 0].reshape((-1, 1)),
                minima[max_indices == 1].reshape((-1, 1)),
            ],
            axis=0,
        )

        y = np.concatenate(
            [
                minima[max_indices == 0].reshape((-1, 1)),
                maxima[max_indices == 1].reshape((-1, 1)),
            ],
            axis=0,
        )

        return cls.bundle(x,y)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="riskmodels.bivariate.ExceedanceDistribution" href="#riskmodels.bivariate.ExceedanceDistribution">ExceedanceDistribution</a></li>
<li><a title="riskmodels.bivariate.BaseDistribution" href="#riskmodels.bivariate.BaseDistribution">BaseDistribution</a></li>
<li>pydantic.main.BaseModel</li>
<li>pydantic.utils.Representation</li>
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="riskmodels.bivariate.LogisticGP" href="#riskmodels.bivariate.LogisticGP">LogisticGP</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="riskmodels.bivariate.Logistic.params"><code class="name">var <span class="ident">params</span> :Â Optional[numpy.ndarray]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="riskmodels.bivariate.Logistic.logpdf"><code class="name flex">
<span>def <span class="ident">logpdf</span></span>(<span>params:Â np.ndarray, threshold:Â float, data:Â t.Union[np.ndarray,Â t.Iterable])</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates logpdf function for Gumbel exceedances</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>params</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>array with dependence parameter as only element</dd>
<dt><strong><code>threshold</code></strong> :&ensp;<code>float</code></dt>
<dd>Exceedance threshold in Gumbel scale</dd>
<dt><strong><code>data</code></strong> :&ensp;<code>t.Union[np.ndarray, t.Iterable]</code></dt>
<dd>Observed data in Gumbel scale</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def logpdf(
    cls, params: np.ndarray, threshold: float, data: t.Union[np.ndarray, t.Iterable]
):
    &#34;&#34;&#34;Calculates logpdf function for Gumbel exceedances
    
    
    Args:
        params (np.ndarray): array with dependence parameter as only element
        threshold (float): Exceedance threshold in Gumbel scale
        data (t.Union[np.ndarray, t.Iterable]): Observed data in Gumbel scale
    
    &#34;&#34;&#34;
    alpha = params[0]

    x, y = cls.unbundle(data)

    nlogp = (np.exp(-x / alpha) + np.exp(-y / alpha)) ** alpha
    lognlogp = alpha * np.log(np.exp(-x / alpha) + np.exp(-y / alpha))
    rescaler = 1 - cls.unconditioned_cdf(params, cls.bundle(threshold, threshold))

    # a = np.exp((x + y - nlogp*alpha)/alpha)
    log_a = (x + y) / alpha - nlogp

    # b = nlogp
    log_b = lognlogp

    # c = 1 + alpha*(nlogp - 1)
    log_c = np.log(1 + alpha * (nlogp - 1))

    # d = 1.0/(alpha*(np.exp(x/alpha) + np.exp(y/alpha))**2)
    log_d = -(np.log(alpha) + 2 * np.log(np.exp(x / alpha) + np.exp(y / alpha)))

    log_density = log_a + log_b + log_c + log_d - np.log(rescaler)

    # density is 0 when both coordinates are below the threshold
    nil_density_idx = np.logical_and(x &lt;= threshold, y &lt;= threshold)
    log_density[nil_density_idx] = -np.Inf

    return log_density</code></pre>
</details>
</dd>
<dt id="riskmodels.bivariate.Logistic.simulate_model"><code class="name flex">
<span>def <span class="ident">simulate_model</span></span>(<span>size:Â int, params:Â np.ndarray, quantile_threshold:Â float) â€‘>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Simulates logistic exceedance model in Gumbel scale</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code></dt>
<dd>Simulated sample size</dd>
<dt><strong><code>params</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Array with dependence parameter</dd>
<dt><strong><code>threshold</code></strong> :&ensp;<code>float</code></dt>
<dd>Exceedance threshold in both components</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>Simulated sample</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def simulate_model(cls, size: int, params: np.ndarray, quantile_threshold: float) -&gt; np.ndarray:
    &#34;&#34;&#34;Simulates logistic exceedance model in Gumbel scale
    
    Args:
        size (int): Simulated sample size
        params (np.ndarray): Array with dependence parameter
        threshold (float): Exceedance threshold in both components
    
    Returns:
        np.ndarray: Simulated sample
    &#34;&#34;&#34;
    ### simulate in Gumbel scale maximum component: z = max(x1, x2) ~ Gumbel(loc=alpha*np.log(2)) using inverse function method
    threshold = gumbel.ppf(quantile_threshold)
    alpha = params[0]

    q0 = gumbel.cdf(threshold, loc=alpha * np.log(2))  # quantile of model&#39;s threshold in the maximum&#39;s distribution
    u = np.random.uniform(size=size, low=q0)
    maxima = gumbel.ppf(q=u, loc=alpha * np.log(2))

    if alpha == 0:
        r = 0
    elif alpha == 1:
        r = np.log(exponential.rvs(size=size, loc=1, scale = np.exp(maxima)))
    else:
        ###simulate difference between maxima and minima r = max(x,y) - min(x,y) using inverse function method
        u = np.random.uniform(size=size)
        r = (
            alpha
            * np.log(
                (
                    -(
                        (alpha - 1)
                        * np.exp(maxima)
                        * lambertw(
                            -(
                                np.exp(
                                    -maxima
                                    - (2 ** alpha * np.exp(-maxima) * alpha)
                                    / (alpha - 1)
                                )
                                * (-(2 ** (alpha - 1)) * (u - 1))
                                ** (alpha / (alpha - 1))
                                * alpha
                            )
                            / (alpha - 1)
                        )
                    )
                    / alpha
                )
                ** (1 / alpha)
                - 1
            )
        ).real

    minima = maxima - r

    # allocate maxima randomly between components
    max_indices = np.random.binomial(1, 0.5, size)

    x = np.concatenate(
        [
            maxima[max_indices == 0].reshape((-1, 1)),
            minima[max_indices == 1].reshape((-1, 1)),
        ],
        axis=0,
    )

    y = np.concatenate(
        [
            minima[max_indices == 0].reshape((-1, 1)),
            maxima[max_indices == 1].reshape((-1, 1)),
        ],
        axis=0,
    )

    return cls.bundle(x,y)</code></pre>
</details>
</dd>
<dt id="riskmodels.bivariate.Logistic.unconditioned_cdf"><code class="name flex">
<span>def <span class="ident">unconditioned_cdf</span></span>(<span>params:Â np.ndarray, data:Â t.Union[np.ndarray,Â t.Iterable])</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates unconstrained standard Gumbel CDF</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def unconditioned_cdf(cls, params: np.ndarray, data: t.Union[np.ndarray, t.Iterable]):
    &#34;&#34;&#34;Calculates unconstrained standard Gumbel CDF&#34;&#34;&#34;
    alpha = params[0]
    x, y = cls.unbundle(data)
    return np.exp(-((np.exp(-x / alpha) + np.exp(-y / alpha)) ** (alpha)))           </code></pre>
</details>
</dd>
<dt id="riskmodels.bivariate.Logistic.validate_params"><code class="name flex">
<span>def <span class="ident">validate_params</span></span>(<span>params)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@validator(&#34;params&#34;)
def validate_params(cls, params):
    alpha = params[0]
    if alpha &lt;= 0 or alpha &gt; 1:
        raise TypeError(f&#34;alpha must be in the interval (0,1]&#34;)
    else:
        return params</code></pre>
</details>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="riskmodels.bivariate.Logistic.alpha"><code class="name">var <span class="ident">alpha</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def alpha(self):
    return self.params[0]</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="riskmodels.bivariate.ExceedanceDistribution" href="#riskmodels.bivariate.ExceedanceDistribution">ExceedanceDistribution</a></b></code>:
<ul class="hlist">
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.bundle" href="#riskmodels.bivariate.ExceedanceDistribution.bundle">bundle</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.cdf" href="#riskmodels.bivariate.BaseDistribution.cdf">cdf</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.data_to_model_dist" href="#riskmodels.bivariate.ExceedanceDistribution.data_to_model_dist">data_to_model_dist</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.fit" href="#riskmodels.bivariate.ExceedanceDistribution.fit">fit</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.hessian" href="#riskmodels.bivariate.ExceedanceDistribution.hessian">hessian</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.loglik" href="#riskmodels.bivariate.ExceedanceDistribution.loglik">loglik</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.model_to_data_dist" href="#riskmodels.bivariate.ExceedanceDistribution.model_to_data_dist">model_to_data_dist</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.pdf" href="#riskmodels.bivariate.ExceedanceDistribution.pdf">pdf</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.plot" href="#riskmodels.bivariate.BaseDistribution.plot">plot</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.plot_diagnostics" href="#riskmodels.bivariate.ExceedanceDistribution.plot_diagnostics">plot_diagnostics</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.simulate" href="#riskmodels.bivariate.BaseDistribution.simulate">simulate</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.unbundle" href="#riskmodels.bivariate.ExceedanceDistribution.unbundle">unbundle</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="riskmodels.bivariate.LogisticGP"><code class="flex name class">
<span>class <span class="ident">LogisticGP</span></span>
<span>(</span><span>**data:Â Any)</span>
</code></dt>
<dd>
<div class="desc"><p>This model is an implementation of a bivariate generalised Pareto distribution with logistic dependence. In Gumbel scale this is given by </p>
<p><span><span class="MathJax_Preview"> \mathbb{P}(\textbf{X} \leq \mathbf{x}) = \frac{\Psi(\mathbf{x}) - \Psi(\min \{\mathbf{x},\mathbf{0}\})}{-\Psi(\mathbf{0})}, \mathbf{X} \nleq \mathbf{0}</span><script type="math/tex; mode=display"> \mathbb{P}(\textbf{X} \leq \mathbf{x}) = \frac{\Psi(\mathbf{x}) - \Psi(\min \{\mathbf{x},\mathbf{0}\})}{-\Psi(\mathbf{0})}, \mathbf{X} \nleq \mathbf{0}</script></span></p>
<p>where</p>
<p><span><span class="MathJax_Preview"> \Psi(\mathbf{x}) = - \left(
\exp \left( - \frac{\mathbf{x}_1}{\alpha} \right)+ \left(
- \frac{\mathbf{x}_2}{\alpha}
\right) \right)^\alpha, \, 0 \leq \alpha\leq 1, \</span><script type="math/tex; mode=display"> \Psi(\mathbf{x}) = - \left(
\exp \left( - \frac{\mathbf{x}_1}{\alpha} \right)+ \left(
- \frac{\mathbf{x}_2}{\alpha}
\right) \right)^\alpha, \, 0 \leq \alpha\leq 1, \</script></span></p>
<p>Create a new model by parsing and validating input data from keyword arguments.</p>
<p>Raises ValidationError if the input data cannot be parsed to form a valid model.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LogisticGP(Logistic):

    &#34;&#34;&#34;This model is an implementation of a bivariate generalised Pareto distribution with logistic dependence. In Gumbel scale this is given by 
    
    $$ \\mathbb{P}(\\textbf{X} \\leq \\mathbf{x}) = \\frac{\\Psi(\\mathbf{x}) - \\Psi(\\min \\{\\mathbf{x},\\mathbf{0}\\})}{-\\Psi(\\mathbf{0})}, \\mathbf{X} \\nleq \\mathbf{0}$$

    where

    $$ \\Psi(\\mathbf{x}) = - \\left(  \\exp \\left( - \\frac{\\mathbf{x}_1}{\\alpha} \\right)+ \\left(  - \\frac{\\mathbf{x}_2}{\\alpha}  \\right) \\right)^\\alpha, \\, 0 \\leq \\alpha\\leq 1, \\$$

    &#34;&#34;&#34;

    @validator(&#34;quantile_threshold&#34;)
    def val_qt(cls, quantile_threshold):
        q = Logistic._model_marginal_dist.cdf(0)
        if quantile_threshold &lt;= q:
            raise ValueError(f&#34;This model does not support quantile thresholds lower than {np.round(q,2)}&#34;)
        else:
            return quantile_threshold

    @classmethod
    def logpdf(
        cls, params: np.ndarray, threshold: float, data: t.Union[np.ndarray, t.Iterable]
    ):
        &#34;&#34;&#34;Calculates the logpdf function
        
        
        Args:
            params (np.ndarray): array with dependence parameter as only element
            threshold (float): Exceedance threshold in Gumbel scale
            data (t.Union[np.ndarray, t.Iterable]): Observed data in Gumbel scale
        
        &#34;&#34;&#34;
        alpha = params[0]

        x, y = cls.unbundle(data)

        rescaler = 1 - Logistic.unconditioned_cdf(params, cls.bundle(threshold, threshold))

        log_a = (-x/alpha-y/alpha)
        log_b = (-2+alpha)*np.log(np.exp(-x/alpha)+np.exp(-y/alpha))
        log_c = np.log(1-alpha)
        log_d = -np.log(alpha)

        log_density = log_a + log_b + log_c + log_d - np.log(rescaler)

        # density is 0 when both coordinates are below the threshold
        nil_density_idx = np.logical_and(x &lt;= threshold, y &lt;= threshold)
        log_density[nil_density_idx] = -np.Inf

        return log_density

    @classmethod
    def unconditioned_cdf(cls, params: np.ndarray, data: t.Union[np.ndarray, t.Iterable]):
        &#34;&#34;&#34;Calculates cdf function with an exceedance threshold of zero&#34;&#34;&#34;
        alpha = params[0]
        x, y = cls.unbundle(data)
        G0 = Logistic.unconditioned_cdf(params, cls.bundle(0,0))
        Ga = Logistic.unconditioned_cdf(params, data)
        Gb = Logistic.unconditioned_cdf(params, np.minimum(data,0))
        return -1/np.log(G0)*(np.log(Ga)-np.log(Gb))

    @classmethod
    def simulate_model(cls, size: int, params: np.ndarray, quantile_threshold: float) -&gt; np.ndarray:
        &#34;&#34;&#34;Simulates exceedances from bivariate GP model
        
        Args:
            size (int): Simulated sample size
            params (np.ndarray): Array with dependence parameter
            threshold (float): Exceedance threshold in both components
        
        Returns:
            np.ndarray: Simulated sample
        &#34;&#34;&#34;
        threshold = gumbel.ppf(quantile_threshold)
        alpha = params[0]

        maxima = exponential.rvs(size=size, loc = threshold)

        if alpha == 0:
            r = 0
        elif alpha == 1:
            r = np.Inf
        else:
            ###simulate difference between maxima and minima r = max(x,y) - min(x,y) using inverse function method
            u = np.random.uniform(size=size)
            r = alpha * np.log(((1-u)/2**(1-alpha))**(1/(alpha-1))-1)

        minima = maxima - r

        # allocate maxima randomly between components
        max_indices = np.random.binomial(1, 0.5, size)

        x = np.concatenate(
            [
                maxima[max_indices == 0].reshape((-1, 1)),
                minima[max_indices == 1].reshape((-1, 1)),
            ],
            axis=0,
        )

        y = np.concatenate(
            [
                minima[max_indices == 0].reshape((-1, 1)),
                maxima[max_indices == 1].reshape((-1, 1)),
            ],
            axis=0,
        )

        return cls.bundle(x,y)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="riskmodels.bivariate.Logistic" href="#riskmodels.bivariate.Logistic">Logistic</a></li>
<li><a title="riskmodels.bivariate.ExceedanceDistribution" href="#riskmodels.bivariate.ExceedanceDistribution">ExceedanceDistribution</a></li>
<li><a title="riskmodels.bivariate.BaseDistribution" href="#riskmodels.bivariate.BaseDistribution">BaseDistribution</a></li>
<li>pydantic.main.BaseModel</li>
<li>pydantic.utils.Representation</li>
<li>abc.ABC</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="riskmodels.bivariate.LogisticGP.params"><code class="name">var <span class="ident">params</span> :Â Optional[numpy.ndarray]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="riskmodels.bivariate.LogisticGP.logpdf"><code class="name flex">
<span>def <span class="ident">logpdf</span></span>(<span>params:Â np.ndarray, threshold:Â float, data:Â t.Union[np.ndarray,Â t.Iterable])</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the logpdf function</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>params</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>array with dependence parameter as only element</dd>
<dt><strong><code>threshold</code></strong> :&ensp;<code>float</code></dt>
<dd>Exceedance threshold in Gumbel scale</dd>
<dt><strong><code>data</code></strong> :&ensp;<code>t.Union[np.ndarray, t.Iterable]</code></dt>
<dd>Observed data in Gumbel scale</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def logpdf(
    cls, params: np.ndarray, threshold: float, data: t.Union[np.ndarray, t.Iterable]
):
    &#34;&#34;&#34;Calculates the logpdf function
    
    
    Args:
        params (np.ndarray): array with dependence parameter as only element
        threshold (float): Exceedance threshold in Gumbel scale
        data (t.Union[np.ndarray, t.Iterable]): Observed data in Gumbel scale
    
    &#34;&#34;&#34;
    alpha = params[0]

    x, y = cls.unbundle(data)

    rescaler = 1 - Logistic.unconditioned_cdf(params, cls.bundle(threshold, threshold))

    log_a = (-x/alpha-y/alpha)
    log_b = (-2+alpha)*np.log(np.exp(-x/alpha)+np.exp(-y/alpha))
    log_c = np.log(1-alpha)
    log_d = -np.log(alpha)

    log_density = log_a + log_b + log_c + log_d - np.log(rescaler)

    # density is 0 when both coordinates are below the threshold
    nil_density_idx = np.logical_and(x &lt;= threshold, y &lt;= threshold)
    log_density[nil_density_idx] = -np.Inf

    return log_density</code></pre>
</details>
</dd>
<dt id="riskmodels.bivariate.LogisticGP.simulate_model"><code class="name flex">
<span>def <span class="ident">simulate_model</span></span>(<span>size:Â int, params:Â np.ndarray, quantile_threshold:Â float) â€‘>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Simulates exceedances from bivariate GP model</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code></dt>
<dd>Simulated sample size</dd>
<dt><strong><code>params</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Array with dependence parameter</dd>
<dt><strong><code>threshold</code></strong> :&ensp;<code>float</code></dt>
<dd>Exceedance threshold in both components</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>Simulated sample</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def simulate_model(cls, size: int, params: np.ndarray, quantile_threshold: float) -&gt; np.ndarray:
    &#34;&#34;&#34;Simulates exceedances from bivariate GP model
    
    Args:
        size (int): Simulated sample size
        params (np.ndarray): Array with dependence parameter
        threshold (float): Exceedance threshold in both components
    
    Returns:
        np.ndarray: Simulated sample
    &#34;&#34;&#34;
    threshold = gumbel.ppf(quantile_threshold)
    alpha = params[0]

    maxima = exponential.rvs(size=size, loc = threshold)

    if alpha == 0:
        r = 0
    elif alpha == 1:
        r = np.Inf
    else:
        ###simulate difference between maxima and minima r = max(x,y) - min(x,y) using inverse function method
        u = np.random.uniform(size=size)
        r = alpha * np.log(((1-u)/2**(1-alpha))**(1/(alpha-1))-1)

    minima = maxima - r

    # allocate maxima randomly between components
    max_indices = np.random.binomial(1, 0.5, size)

    x = np.concatenate(
        [
            maxima[max_indices == 0].reshape((-1, 1)),
            minima[max_indices == 1].reshape((-1, 1)),
        ],
        axis=0,
    )

    y = np.concatenate(
        [
            minima[max_indices == 0].reshape((-1, 1)),
            maxima[max_indices == 1].reshape((-1, 1)),
        ],
        axis=0,
    )

    return cls.bundle(x,y)</code></pre>
</details>
</dd>
<dt id="riskmodels.bivariate.LogisticGP.unconditioned_cdf"><code class="name flex">
<span>def <span class="ident">unconditioned_cdf</span></span>(<span>params:Â np.ndarray, data:Â t.Union[np.ndarray,Â t.Iterable])</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates cdf function with an exceedance threshold of zero</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def unconditioned_cdf(cls, params: np.ndarray, data: t.Union[np.ndarray, t.Iterable]):
    &#34;&#34;&#34;Calculates cdf function with an exceedance threshold of zero&#34;&#34;&#34;
    alpha = params[0]
    x, y = cls.unbundle(data)
    G0 = Logistic.unconditioned_cdf(params, cls.bundle(0,0))
    Ga = Logistic.unconditioned_cdf(params, data)
    Gb = Logistic.unconditioned_cdf(params, np.minimum(data,0))
    return -1/np.log(G0)*(np.log(Ga)-np.log(Gb))</code></pre>
</details>
</dd>
<dt id="riskmodels.bivariate.LogisticGP.val_qt"><code class="name flex">
<span>def <span class="ident">val_qt</span></span>(<span>quantile_threshold)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@validator(&#34;quantile_threshold&#34;)
def val_qt(cls, quantile_threshold):
    q = Logistic._model_marginal_dist.cdf(0)
    if quantile_threshold &lt;= q:
        raise ValueError(f&#34;This model does not support quantile thresholds lower than {np.round(q,2)}&#34;)
    else:
        return quantile_threshold</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="riskmodels.bivariate.Logistic" href="#riskmodels.bivariate.Logistic">Logistic</a></b></code>:
<ul class="hlist">
<li><code><a title="riskmodels.bivariate.Logistic.bundle" href="#riskmodels.bivariate.ExceedanceDistribution.bundle">bundle</a></code></li>
<li><code><a title="riskmodels.bivariate.Logistic.cdf" href="#riskmodels.bivariate.BaseDistribution.cdf">cdf</a></code></li>
<li><code><a title="riskmodels.bivariate.Logistic.data_to_model_dist" href="#riskmodels.bivariate.ExceedanceDistribution.data_to_model_dist">data_to_model_dist</a></code></li>
<li><code><a title="riskmodels.bivariate.Logistic.fit" href="#riskmodels.bivariate.ExceedanceDistribution.fit">fit</a></code></li>
<li><code><a title="riskmodels.bivariate.Logistic.hessian" href="#riskmodels.bivariate.ExceedanceDistribution.hessian">hessian</a></code></li>
<li><code><a title="riskmodels.bivariate.Logistic.loglik" href="#riskmodels.bivariate.ExceedanceDistribution.loglik">loglik</a></code></li>
<li><code><a title="riskmodels.bivariate.Logistic.model_to_data_dist" href="#riskmodels.bivariate.ExceedanceDistribution.model_to_data_dist">model_to_data_dist</a></code></li>
<li><code><a title="riskmodels.bivariate.Logistic.pdf" href="#riskmodels.bivariate.ExceedanceDistribution.pdf">pdf</a></code></li>
<li><code><a title="riskmodels.bivariate.Logistic.plot" href="#riskmodels.bivariate.BaseDistribution.plot">plot</a></code></li>
<li><code><a title="riskmodels.bivariate.Logistic.plot_diagnostics" href="#riskmodels.bivariate.ExceedanceDistribution.plot_diagnostics">plot_diagnostics</a></code></li>
<li><code><a title="riskmodels.bivariate.Logistic.simulate" href="#riskmodels.bivariate.BaseDistribution.simulate">simulate</a></code></li>
<li><code><a title="riskmodels.bivariate.Logistic.unbundle" href="#riskmodels.bivariate.ExceedanceDistribution.unbundle">unbundle</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="riskmodels.bivariate.Mixture"><code class="flex name class">
<span>class <span class="ident">Mixture</span></span>
<span>(</span><span>**data:Â Any)</span>
</code></dt>
<dd>
<div class="desc"><p>Base interface for a bivariate mixture distribution</p>
<p>Create a new model by parsing and validating input data from keyword arguments.</p>
<p>Raises ValidationError if the input data cannot be parsed to form a valid model.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Mixture(BaseDistribution):

    &#34;&#34;&#34;Base interface for a bivariate mixture distribution&#34;&#34;&#34;

    distributions: t.List[BaseDistribution]
    weights: np.ndarray

    def __repr__(self):
        return f&#34;Mixture with {len(self.weights)} components&#34;

    def simulate(self, size: int) -&gt; np.ndarray:

        n_samples = np.random.multinomial(n=size, pvals=self.weights, size=1)[0]
        indices = (n_samples &gt; 0).nonzero()[0]
        samples = [
            dist.simulate(size=k)
            for dist, k in zip(
                [self.distributions[k] for k in indices], n_samples[indices]
            )
        ]
        samples = np.concatenate(samples, axis=0)
        np.random.shuffle(samples)
        return samples

    def cdf(self, x: np.ndarray, **kwargs) -&gt; float:
        vals = [
            w * dist.cdf(x, **kwargs)
            for w, dist in zip(self.weights, self.distributions)
        ]
        return reduce(lambda x, y: x + y, vals)

    def pdf(self, x: np.ndarray, **kwargs) -&gt; float:

        vals = [
            w * dist.pdf(x, **kwargs)
            for w, dist in zip(self.weights, self.distributions)
        ]
        return reduce(lambda x, y: x + y, vals)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="riskmodels.bivariate.BaseDistribution" href="#riskmodels.bivariate.BaseDistribution">BaseDistribution</a></li>
<li>pydantic.main.BaseModel</li>
<li>pydantic.utils.Representation</li>
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="riskmodels.bivariate.ExceedanceModel" href="#riskmodels.bivariate.ExceedanceModel">ExceedanceModel</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="riskmodels.bivariate.Mixture.distributions"><code class="name">var <span class="ident">distributions</span> :Â List[<a title="riskmodels.bivariate.BaseDistribution" href="#riskmodels.bivariate.BaseDistribution">BaseDistribution</a>]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="riskmodels.bivariate.Mixture.weights"><code class="name">var <span class="ident">weights</span> :Â numpy.ndarray</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="riskmodels.bivariate.BaseDistribution" href="#riskmodels.bivariate.BaseDistribution">BaseDistribution</a></b></code>:
<ul class="hlist">
<li><code><a title="riskmodels.bivariate.BaseDistribution.cdf" href="#riskmodels.bivariate.BaseDistribution.cdf">cdf</a></code></li>
<li><code><a title="riskmodels.bivariate.BaseDistribution.pdf" href="#riskmodels.bivariate.BaseDistribution.pdf">pdf</a></code></li>
<li><code><a title="riskmodels.bivariate.BaseDistribution.plot" href="#riskmodels.bivariate.BaseDistribution.plot">plot</a></code></li>
<li><code><a title="riskmodels.bivariate.BaseDistribution.simulate" href="#riskmodels.bivariate.BaseDistribution.simulate">simulate</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="riskmodels" href="index.html">riskmodels</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="riskmodels.bivariate.AsymmetricLogistic" href="#riskmodels.bivariate.AsymmetricLogistic">AsymmetricLogistic</a></code></h4>
<ul class="">
<li><code><a title="riskmodels.bivariate.AsymmetricLogistic.logpdf" href="#riskmodels.bivariate.AsymmetricLogistic.logpdf">logpdf</a></code></li>
<li><code><a title="riskmodels.bivariate.AsymmetricLogistic.params" href="#riskmodels.bivariate.AsymmetricLogistic.params">params</a></code></li>
<li><code><a title="riskmodels.bivariate.AsymmetricLogistic.simulate_model" href="#riskmodels.bivariate.AsymmetricLogistic.simulate_model">simulate_model</a></code></li>
<li><code><a title="riskmodels.bivariate.AsymmetricLogistic.unconditioned_cdf" href="#riskmodels.bivariate.AsymmetricLogistic.unconditioned_cdf">unconditioned_cdf</a></code></li>
<li><code><a title="riskmodels.bivariate.AsymmetricLogistic.validate_params" href="#riskmodels.bivariate.AsymmetricLogistic.validate_params">validate_params</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="riskmodels.bivariate.BaseDistribution" href="#riskmodels.bivariate.BaseDistribution">BaseDistribution</a></code></h4>
<ul class="two-column">
<li><code><a title="riskmodels.bivariate.BaseDistribution.Config" href="#riskmodels.bivariate.BaseDistribution.Config">Config</a></code></li>
<li><code><a title="riskmodels.bivariate.BaseDistribution.cdf" href="#riskmodels.bivariate.BaseDistribution.cdf">cdf</a></code></li>
<li><code><a title="riskmodels.bivariate.BaseDistribution.data" href="#riskmodels.bivariate.BaseDistribution.data">data</a></code></li>
<li><code><a title="riskmodels.bivariate.BaseDistribution.pdf" href="#riskmodels.bivariate.BaseDistribution.pdf">pdf</a></code></li>
<li><code><a title="riskmodels.bivariate.BaseDistribution.plot" href="#riskmodels.bivariate.BaseDistribution.plot">plot</a></code></li>
<li><code><a title="riskmodels.bivariate.BaseDistribution.simulate" href="#riskmodels.bivariate.BaseDistribution.simulate">simulate</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="riskmodels.bivariate.Empirical" href="#riskmodels.bivariate.Empirical">Empirical</a></code></h4>
<ul class="">
<li><code><a title="riskmodels.bivariate.Empirical.check_pdf_values" href="#riskmodels.bivariate.Empirical.check_pdf_values">check_pdf_values</a></code></li>
<li><code><a title="riskmodels.bivariate.Empirical.data" href="#riskmodels.bivariate.Empirical.data">data</a></code></li>
<li><code><a title="riskmodels.bivariate.Empirical.fit_tail_model" href="#riskmodels.bivariate.Empirical.fit_tail_model">fit_tail_model</a></code></li>
<li><code><a title="riskmodels.bivariate.Empirical.from_data" href="#riskmodels.bivariate.Empirical.from_data">from_data</a></code></li>
<li><code><a title="riskmodels.bivariate.Empirical.get_marginals" href="#riskmodels.bivariate.Empirical.get_marginals">get_marginals</a></code></li>
<li><code><a title="riskmodels.bivariate.Empirical.pdf_values" href="#riskmodels.bivariate.Empirical.pdf_values">pdf_values</a></code></li>
<li><code><a title="riskmodels.bivariate.Empirical.pickands" href="#riskmodels.bivariate.Empirical.pickands">pickands</a></code></li>
<li><code><a title="riskmodels.bivariate.Empirical.plot_chi" href="#riskmodels.bivariate.Empirical.plot_chi">plot_chi</a></code></li>
<li><code><a title="riskmodels.bivariate.Empirical.plot_pickands" href="#riskmodels.bivariate.Empirical.plot_pickands">plot_pickands</a></code></li>
<li><code><a title="riskmodels.bivariate.Empirical.test_asymptotic_dependence" href="#riskmodels.bivariate.Empirical.test_asymptotic_dependence">test_asymptotic_dependence</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="riskmodels.bivariate.ExceedanceDistribution" href="#riskmodels.bivariate.ExceedanceDistribution">ExceedanceDistribution</a></code></h4>
<ul class="">
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.bundle" href="#riskmodels.bivariate.ExceedanceDistribution.bundle">bundle</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.data_scale_threshold" href="#riskmodels.bivariate.ExceedanceDistribution.data_scale_threshold">data_scale_threshold</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.data_to_model_dist" href="#riskmodels.bivariate.ExceedanceDistribution.data_to_model_dist">data_to_model_dist</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.fit" href="#riskmodels.bivariate.ExceedanceDistribution.fit">fit</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.hessian" href="#riskmodels.bivariate.ExceedanceDistribution.hessian">hessian</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.loglik" href="#riskmodels.bivariate.ExceedanceDistribution.loglik">loglik</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.logpdf" href="#riskmodels.bivariate.ExceedanceDistribution.logpdf">logpdf</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.margin1" href="#riskmodels.bivariate.ExceedanceDistribution.margin1">margin1</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.margin2" href="#riskmodels.bivariate.ExceedanceDistribution.margin2">margin2</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.mle_cov" href="#riskmodels.bivariate.ExceedanceDistribution.mle_cov">mle_cov</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.model_scale_threshold" href="#riskmodels.bivariate.ExceedanceDistribution.model_scale_threshold">model_scale_threshold</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.model_to_data_dist" href="#riskmodels.bivariate.ExceedanceDistribution.model_to_data_dist">model_to_data_dist</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.params" href="#riskmodels.bivariate.ExceedanceDistribution.params">params</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.pdf" href="#riskmodels.bivariate.ExceedanceDistribution.pdf">pdf</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.plot_diagnostics" href="#riskmodels.bivariate.ExceedanceDistribution.plot_diagnostics">plot_diagnostics</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.quantile_threshold" href="#riskmodels.bivariate.ExceedanceDistribution.quantile_threshold">quantile_threshold</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.unbundle" href="#riskmodels.bivariate.ExceedanceDistribution.unbundle">unbundle</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.unconditioned_cdf" href="#riskmodels.bivariate.ExceedanceDistribution.unconditioned_cdf">unconditioned_cdf</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceDistribution.validate_data" href="#riskmodels.bivariate.ExceedanceDistribution.validate_data">validate_data</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="riskmodels.bivariate.ExceedanceModel" href="#riskmodels.bivariate.ExceedanceModel">ExceedanceModel</a></code></h4>
<ul class="">
<li><code><a title="riskmodels.bivariate.ExceedanceModel.distributions" href="#riskmodels.bivariate.ExceedanceModel.distributions">distributions</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceModel.empirical" href="#riskmodels.bivariate.ExceedanceModel.empirical">empirical</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceModel.plot_diagnostics" href="#riskmodels.bivariate.ExceedanceModel.plot_diagnostics">plot_diagnostics</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceModel.tail" href="#riskmodels.bivariate.ExceedanceModel.tail">tail</a></code></li>
<li><code><a title="riskmodels.bivariate.ExceedanceModel.weights" href="#riskmodels.bivariate.ExceedanceModel.weights">weights</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="riskmodels.bivariate.Frechet" href="#riskmodels.bivariate.Frechet">Frechet</a></code></h4>
<ul class="">
<li><code><a title="riskmodels.bivariate.Frechet.cdf" href="#riskmodels.bivariate.Frechet.cdf">cdf</a></code></li>
<li><code><a title="riskmodels.bivariate.Frechet.ppf" href="#riskmodels.bivariate.Frechet.ppf">ppf</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="riskmodels.bivariate.Gaussian" href="#riskmodels.bivariate.Gaussian">Gaussian</a></code></h4>
<ul class="two-column">
<li><code><a title="riskmodels.bivariate.Gaussian.cov" href="#riskmodels.bivariate.Gaussian.cov">cov</a></code></li>
<li><code><a title="riskmodels.bivariate.Gaussian.logpdf" href="#riskmodels.bivariate.Gaussian.logpdf">logpdf</a></code></li>
<li><code><a title="riskmodels.bivariate.Gaussian.margin1" href="#riskmodels.bivariate.Gaussian.margin1">margin1</a></code></li>
<li><code><a title="riskmodels.bivariate.Gaussian.margin2" href="#riskmodels.bivariate.Gaussian.margin2">margin2</a></code></li>
<li><code><a title="riskmodels.bivariate.Gaussian.params" href="#riskmodels.bivariate.Gaussian.params">params</a></code></li>
<li><code><a title="riskmodels.bivariate.Gaussian.quantile_threshold" href="#riskmodels.bivariate.Gaussian.quantile_threshold">quantile_threshold</a></code></li>
<li><code><a title="riskmodels.bivariate.Gaussian.rho" href="#riskmodels.bivariate.Gaussian.rho">rho</a></code></li>
<li><code><a title="riskmodels.bivariate.Gaussian.simulate_model" href="#riskmodels.bivariate.Gaussian.simulate_model">simulate_model</a></code></li>
<li><code><a title="riskmodels.bivariate.Gaussian.validate_params" href="#riskmodels.bivariate.Gaussian.validate_params">validate_params</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="riskmodels.bivariate.Independent" href="#riskmodels.bivariate.Independent">Independent</a></code></h4>
<ul class="">
<li><code><a title="riskmodels.bivariate.Independent.x" href="#riskmodels.bivariate.Independent.x">x</a></code></li>
<li><code><a title="riskmodels.bivariate.Independent.y" href="#riskmodels.bivariate.Independent.y">y</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="riskmodels.bivariate.Logistic" href="#riskmodels.bivariate.Logistic">Logistic</a></code></h4>
<ul class="two-column">
<li><code><a title="riskmodels.bivariate.Logistic.alpha" href="#riskmodels.bivariate.Logistic.alpha">alpha</a></code></li>
<li><code><a title="riskmodels.bivariate.Logistic.logpdf" href="#riskmodels.bivariate.Logistic.logpdf">logpdf</a></code></li>
<li><code><a title="riskmodels.bivariate.Logistic.params" href="#riskmodels.bivariate.Logistic.params">params</a></code></li>
<li><code><a title="riskmodels.bivariate.Logistic.simulate_model" href="#riskmodels.bivariate.Logistic.simulate_model">simulate_model</a></code></li>
<li><code><a title="riskmodels.bivariate.Logistic.unconditioned_cdf" href="#riskmodels.bivariate.Logistic.unconditioned_cdf">unconditioned_cdf</a></code></li>
<li><code><a title="riskmodels.bivariate.Logistic.validate_params" href="#riskmodels.bivariate.Logistic.validate_params">validate_params</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="riskmodels.bivariate.LogisticGP" href="#riskmodels.bivariate.LogisticGP">LogisticGP</a></code></h4>
<ul class="">
<li><code><a title="riskmodels.bivariate.LogisticGP.logpdf" href="#riskmodels.bivariate.LogisticGP.logpdf">logpdf</a></code></li>
<li><code><a title="riskmodels.bivariate.LogisticGP.params" href="#riskmodels.bivariate.LogisticGP.params">params</a></code></li>
<li><code><a title="riskmodels.bivariate.LogisticGP.simulate_model" href="#riskmodels.bivariate.LogisticGP.simulate_model">simulate_model</a></code></li>
<li><code><a title="riskmodels.bivariate.LogisticGP.unconditioned_cdf" href="#riskmodels.bivariate.LogisticGP.unconditioned_cdf">unconditioned_cdf</a></code></li>
<li><code><a title="riskmodels.bivariate.LogisticGP.val_qt" href="#riskmodels.bivariate.LogisticGP.val_qt">val_qt</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="riskmodels.bivariate.Mixture" href="#riskmodels.bivariate.Mixture">Mixture</a></code></h4>
<ul class="">
<li><code><a title="riskmodels.bivariate.Mixture.distributions" href="#riskmodels.bivariate.Mixture.distributions">distributions</a></code></li>
<li><code><a title="riskmodels.bivariate.Mixture.weights" href="#riskmodels.bivariate.Mixture.weights">weights</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>